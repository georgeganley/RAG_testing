>> Hello. In this lecture we're going to look at n-grams. This is a technique that you can use for agents to learn what a human player is going to do based on their previous behavior, especially in terms of analyzing sequences of behaviors and looking for patterns. And so it's all about action prediction. And so this particular technique with n-grams is especially popular with melee combat game. So these are like you're fighting games such as street fighter, and you have lots and lots of high frequency of actions being performed by the game player, pressing like the punch and the kick buttons. In fact, in many cases, the action prediction is directly related to user inputs. So just those are the sample points. Now you might evaluate some higher-level actions performed by the player. So for instance, you could, instead of the raw key presses that you're trying to predict, you might use something else such as say, like a street fighter to throwing a fireball. So that actually involves a combination of key presses. But you could treat the action selection as being just the firewall rather than the arrow key manipulations and then the punch button that is used in combination. So this can also be used for any other types of decisions that players might use. So where does a player choose to go, maybe like down a particular hallway. In a multiplayer game, if there is an intersection, you could apply this to weapon selection, so maybe the agent trying to guess what weapon they're going to be attacked with next, some tactics. There's lots of possibilities. Ellington mentions in his book that this particular technique is so heavily associated with melee combat that a lot of game developers don't even consider using it for anything else. But in fact that it can be very useful for other scenarios. And being a fairly straightforward thing to implement, that might be missing out by not using it. So the whole approach of action prediction as applied to humans is something that really relies on the fact that humans are bad at being random. So if you ask an individual to intentionally select random choices, they will tend to make choices that focus on change rather than correctly understanding a random distribution. So for instance, if you have two choices to make, an individual will have a preference for changing whatever they did before. So if they hit the A button, they're very likely the next time to hit the B button. Even though true random distribution there's equal likelihood of A or B being selected. So you will tend to see an absence of repeated values in a sequence when you analyze when humans are performing it. They also forget over a short period of time what patterns they have followed before. And because of that, we'll repeat those same patterns over and over. Furthermore, a single individual you might think, well it'd be easy to learn one individual, but that can actually have carryover for applying that same knowledge against other human opponents. I have a quote from Ellington here. We have shared characteristics that run so deep that learning to anticipate one player's actions can often lead to better play against a completely different player. So that means that you can have offline training. You can train on say, some small number of human opponents, and then you can have that agent with the training data applied to create its decision-making algorithm. So applying that same understanding of human behavior for some small number of human players, and it'll carry over to successes against other human players that were not trained on just because of this tendency to all humans to make the same flaws in their decision-making. So let's start off by looking at a naive algorithms. So this is not going to be the n-gram algorithm, but we will start to get us thinking in the right direction here. So we are going to try to predict what the human player is going to do just by tracking the frequency of their choices. And so we could just keep a running number of whatever the choices are. So we know there's a total count of all choices made, but then some portion of those is going to be Choice A and some portion will be Choice B. Of course, if there's further choices that we would all be divvied up. But these ratios relative to the total number of A choices over a total choices plus number of B choices over total choices, and so on, that the sum will add up to one. So once you have that information, the idea is that the agent could just say, well, I'm going to guess the player is going to do the thing that they've previously done the most. So if one key press is more common than another key press, in say a fighting game, so that maybe the human player throws a punch a lot than the agent can make a selection, that says, well, they punch more than anything else. So the agent will make a selection of their own action with that assumption. So it might be that they block high or that they counterattack with a low kick, for instance. Now, the problem with this approach is that it gives a lot of feedback to the player. So the human player immediately knows what the agent is doing, and they'll realize that if they punch a lot more than anything else, that they start to see the agent performing clear counters to that. So that is knowledge that the player is gaining. And they will begin to experiment and learn and they'll realize that they can alternate, for instance, wait until the agent catches on and then switch, or just generally alternating can be beneficial or otherwise balancing out their strategies, but not necessarily avoiding patterns in the strategy otherwise. Now, you can implement this. Probably the most naive would just be to keep an indefinite tally going. So you're just counting all occurrences of punches and kicks. Now you could add the ability for the agent to be able to more quickly change the observed statistics for determining these probabilities. So you can imagine that if you keep an indefinite tally, the longer you observe the player, that you're just going to take that much more new observations before you can shift the ratio because you're going to have to have counted so many observations to shift the ratio one way or the other. Instead of counting from the very beginning, the very first observation on forever, you could instead have a sliding window. And this would typically be implemented with a circular buffer. You could also use an exponential moving average, which doesn't involve any memory storage other than a single value. But with the sliding window, you have a circular buffer, and so at first you just treat it like a regular array with empty slots and so you keep account of how many slots are used. Then once the circular buffer is full, then you have to go to a replacement strategy. So anytime you want to add new data to the circular buffer, you remove the oldest and replace the oldest value with the new value. >> At that point, anytime you're adding new data, you're removing old data and you typically couple this with a running total of what's in the array. When it's time to add something to an already full circular buffer, you subtract out the oldest value from the running total, and then you add in the new running total and of course the count values in the array. From that you can quickly compute the mean or the average. This improves the Naive algorithm a little bit in that it can more quickly adapt it, the learned probability can change more quickly, so it's not going to get slower and slower at learning. It'll at least be able to learn. But this still doesn't solve the issue of the human player figuring out an exploit to the agent implementation. We can imagine for these scenarios that we're talking about trying to learn the player choices. We imagine just a simple game. We use the left or right hand game. This is where you show your opponent an object that you are going to conceal, on either your left or right hand. So you put your two hands behind your back, you pick one of the hands to put the object in, squeeze your hands tight and then bring them to the front of you and then you let the opponent pick, and they try to pick which hand holds the object and you can repeat this over and over, which is what we're proposing here. The agent is either picking left or right, and then the result is the hand opens that they choose and you either see the object and there you got it right. In which case you can clearly know which hand has the object or it's an empty hand, but you also know what the answer is. So in either case it's being revealed to you which hand actually had the object. The agent can observe this information now. Again, though we could use the Naive algorithm either with a running total from the beginning or with a sliding window or what we might do is just keep a history. So we can think of this history as a string of symbols. The symbols being either an L for left or an R for right. So in this string we have a history and one thing we might consider doing is to make a prediction based on the recent history that we're observing. For instance, if the player that's hiding the object in their hand, the last two times has hidden the object in their right hand. Well, that gives us a window size of two that we're considering. We might then go through our history and find an occurrence of right, right, and then we could just pick well. Once we found the position of right, right, well, what comes next? We could just look in the history and pick one and then that third character in the sequence or whatever comes next after RR, that will be our guess. This is rarely used in video games as a way to track the human behavior and usually this is instead done in a table form which we'll look at. But you might imagine, well, it would probably be more reliable rather than just searching for the first occurrence of right, right. We might actually want to find all the occurrences of right, right, and what comes after it, and then determine a probability based on the possibilities for what we've seen before. If we see right, right, left, a very high percentage, let's say, 80% of the time you see right, right, left, then that's what you would want to pick as left. But you might have some other probability as well. Either in this case, the scenario we're considering you're going to have a left or a right. Maybe the human does a good job of evenly spreading out the left and right occurrences for that particular sequence but maybe not for other sequences. This gives you the basic idea of how the N-gram works at least from a high level. But rather than keep this history as this long string and constantly analyzing it through the search operations, it would probably be better if we can make a table. If we're sticking with this window size of two choices and then what's considering what's the third? We can build a table like what's shown here on the screen. This will allow us to keep track of probabilities. We can update our probabilities based on what we've observed in the history. We need to consider all the combinations of two. If our window size is two, then this is called a three-gram in this case. That's where the N-gram comes from. Whatever your window size is, plus 1 is what you call the gram, three-gram in this case. We want to just make a lookup table of these probabilities. If we see about an equal split with left, left, right versus left, left, left, then you would just have 50% probability stored for each of those. But maybe for say, right, left, right and right, left, left, there's a heavy bias. That's clearly something that the agent could take advantage of and do better than chance in terms of guessing what the human player will do. The table, of course, it's going to need to sum to one because it needs to cover all possibilities and you want to make sure that you've properly generated the combinations of choices, whatever your window size is. >> So this would be great if we could analyze, say, our string and then populate these tables. But we could actually use a table structure like this to tabulate on the fly what the probabilities should be. So we can make a little modification rather than storing probabilities as the cells in our table. We can instead store the count for each intersection of the rows and columns. So we know that say, for occurrences of the previous two choices by the player is left, left, then we find an intersection with either right or left. Once we observe that third key or third choice made, then we just update a tally. So if we get a left, left, right, well, do you see my mouse cursor here in the video? We are going to just add to this value here. If we see a right, right, left, we'll find right, right in the table and then in the row, and then we'll look for the column left, so right, right, left is here and so we'll add one there. Simultaneously, we're also keeping track for each row one value. So it doesn't matter how many columns there are, there's still going to be this one value and that's the running total. So this running total value allows us to quickly determine our probabilities. We simply need to just perform a division. Anytime there's a need to make a prediction, we do a table lookup, so we know what the previous history is. We don't know yet what the player is going to do, and so the idea is we would make the prediction right before the human player does something. And so we'll look at the recent history, we'd see, well they just hit right, right, or made the choice, right, right, so what's going to come next? So we look in the row and then we find the biggest running tally. And that's our selection. And if we actually needed the probability for some reason we could perform the division. So that is basically how n-grams work. Of course, the thing that you need to decide when you're designing the solution yourself is, well, what should the window size be? So how do you get to where you have a window size because you can imagine you could pick two or three or you keep going maybe 10. And so you could test empirically just have a number of people play the game and analyze and see well what is occurring the most. And furthermore, look for the accuracy of these choices. So you could aggregate all the data and figure out, well, what if the choice was random? If the human player is truly random, then with two choices, you would expect that if you're making a guess as to what comes next, then you would have a 50% chance of getting it right because the human player is doing such a good job. So you could actually just test out all of these different window sizes and then pick which one does the best job for accurately predicting the player. With two choices you're looking for better than 50%, and you want to pick the best of all the possibilities. So you probably will find that the window size will be around four or five, and that is the sweet spot for a human player typically. Once you start getting beyond that four or five range, you're going to start having a quick drop-off of performance. And that's because human, when they're just trying to make up choices in their head intentionally trying to be random, they're going to change what they do. Because of the short causal processes. So there's a lot of things that can play. And into what the player is doing, but these tend to occur over a relatively short ranges of time. So no one will normally tend to plan out something and then follow through with that plan for a very long period. There'll be some interruption most likely, or they're just not thinking that far ahead. And therefore, they're going to be driven more by the environment in the game based on what their next choices are going to be. So there's again, a sweet spot for where the most predictable patterns of behavior will occur. So that's what your agent should shoot for. And probably you'll get your best luck by doing something exactly like what's shown in this table. You'll just test it out and see how well the different window sizes work in terms of your selection, and then you, of course, pick the best one. But one thing you can do to improve on the n-gram concept is you can just use a whole bunch of windows sizes all at once. So that's called the hierarchical n-gram. This can work really well for online learning. >> The way it works is that you can have small windows and large windows that you're populating all of them in terms of your running tally. The question is, well, which one do you pick? So you've got, say with a hierarchical 3-gram, you have 1, 2, 3 levels so you could pick something else as well, and you would have all the different possible n-grams sizes below that. So again, which one do you pick? And so you might imagine with the larger n-grams, they're going to be more accurate up to a point. So you probably wouldn't go much more than say, five or six as your size. But it's going to take a lot more data before you can really make an informed decision with the larger n-gram sizes. The smaller n-gram sizes, you're going to get your data very quickly just because there's less possible combinations that can occur. So the idea is that you might start off with your hierarchical n-gram with no information and you begin to observe the opponent human player. So in that case, what you're doing is, in order to decide which of the n-grams to use for the prediction, well, which one has sufficient examples? So and you might basically be looking at the rows, not necessarily the whole table which you need the information, but say per row you look to see, well, is there enough information there? And then these might be just empirically defined thresholds to be sufficient. And once you have reached that threshold, then it becomes a candidate for consideration. And so you look up in each table row that does have sufficient examples, you look them up and then you say a particular sequence that you're looking at. So you might be looking for say, left, left, left. So in your one gram, you'd be just looking at one back, so it'd be left, it would be the row you look at. But 2-gram you'd be looking at left, left and 3-gram you'd be looking at left, left, left. So you might find that say, you have entries for the 1-gram and the 2-gram, but the 3-gram doesn't have sufficient examples. So you don't even considered the 3-gram, so it would just be between the one and the two. Then what you do is you look at for that row and the possible choices, is there a clear winner? So whichever entry has the highest value. And so if say the 2-gram has a higher value prediction like guessing, say right and it thinks that there's an 80% chance that it's going to be a right. But the 1-gram only has say, the max is a 60% chance of left. In this case, you would want to go with the 2-gram that has the higher likelihood predictive accuracy. And so you're still going to be updating the tallies for all of your tables. And so when you're learning very early on, you're going to rely more on the smaller n-grams. Then you're going to start to specialize or observe specialization in your tables. So that for maybe some certain sequences, it'll be highly predictive for any of the different n-grams that you are tracking and that is going to probably be related to the strategy of the human player. Maybe there are a very highly repeated sequence that maybe is a longer sequence. This just related to some activity that the player is performing whereas other cases they're making more shorter decisions. And so therefore, you would want to use the smaller size n-grams. And again, you just wait for it to show up in the table and then of course, you will have to take longer to learn and not get it sufficient examples to use those larger size n-grams. So the algorithm smoothly improves over time. Just once you have the data, it can start making more informed decisions. So it's a pretty powerful technique. In fact, it ends up being so effective that in many cases, AI designers actually have to dumb down the implementation and it works just that well, especially in fighting games. So you can make the AI where it is basically unbeatable for a human player, they don't realize they're that predictable and the agent can just really take advantage of them. So you might intentionally avoid picking the best choice always maybe intentionally throwing in a poor choice. Maybe control with difficulty settings, which n-gram settings or errors in responses are made. So this is certainly very useful. It again, can be applied to different types of gameplay decisions, like we mentioned before, weapon selection, locations that the human player might visit. You also see this technique used outside of game AI and it has other applications that is useful in as well, such as language learning where different phonemes fall in the likelihood. And then that can inform other parts of say, natural language processing. So if you're trying to figure out some ambiguous syllable sound well, you can make a guess because you know well, what's the most likely sequence of, say, three syllables in a row, for instance. So definitely this is an algorithm that you should keep in mind if you're developing an agent that is working against the human player and it would be useful to know what's the next action that the human player is going to pick.
