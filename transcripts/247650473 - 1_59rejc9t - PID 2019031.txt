>> Hello. In this lecture, we're going to continue looking at goal-oriented behavior. So we're now going to consider a more complicated scenario where we're going to need A star planning similar to our use of A star for path planning. So if we think back to when we were doing path planning, we needed to represent our continuous game world with some discretized form. So we broke up our continuous space into these different distinct regions that we could associate with the graph structure, with nodes that have adjacencies in form of edges that connect the nodes and each of these nodes represents some state information about where the agent is located and we could use a search algorithm such as A star to plan moving the agent around in an abstract way through operators that define the action of moving from one of these discretized space locations to the next. So we're going to have a very similar thing for action or behavior planning where we do still need to discretize the space or the game world rather as whatever is involved in the planning. So this might be, say, an inventory or agent, like objects, or artifacts that the agent possesses. It could be other state information in the game that is important to the gameplay. So maybe if light switches are on or off, or if certain resources are in certain places. But we still need this discretized representation that is often defined in terms of some relationship between the different elements of the game. And so this discretized representation we have a world state and we can manipulate it through applying operators that change or augment a given state to a new state so for instance, the agent might pick up an object and now possesses it so it's in their inventory or that they flip a light switch and then switch changes. These operators have preconditions and effects or post-conditions. So these are similar to the operators in path planning where I have the up, down, and left, right movement with the grid lattice type of movement. But these operators, there can be many more of them because you have complicated behavior in games. These will often be maybe associated with animations that the character plays. So it appears as if the character is performing some behavior. So there's a lot of similarities and really it is the same problems just the way in which the space or the world is discretized and the specific operators will change, perhaps there's many more as compared to path planning. But again at a high level, it's still the same planning. There are some challenges that pop up because of the differences in representation, and we will address those as we move forward. So one thing I wanted to mention before we get too far into this is that Jeff Orkin, who worked at monolith studios worked on an A star behavior planning system for the game fear. In this class, you should check out his presentation that he gave at a game developer conference. And he went into detail on his motivation and implementation details related to designing the agents for the enemies in the game fear. So be sure and check that out in the canvas learning module. Watch his talk. I definitely recommend checking out. It's very interesting, very informative. We will still touch on some of his ideas in what he presented in this talk. But again, I recommend you watch the whole thing. So one thing I want to mention now is that his motivation was not primarily to develop advanced AI capabilities for his agents. In fact, that was probably not a significant motivation at all. Instead, Jeff was looking to make it easier to author the behavior for his agents. And he saw benefit in decoupling goals and actions and allowing for more declarative authoring and allowing for a lot more reuse in the code and implementation. So we've seen that as being a beneficial feature of other reactive decision-making approaches in some of the previous lectures. So for instance, behavior trees have some very similar aspects as compared to finite state machines. But they are supporting reuse. Just the nature of the behavior tree structure being able to break off parts subtrees from the overall tree and reusing parts across agents. So there's similar sorts of benefits from this goal-oriented behavior planning system. Now when we're considering these systems, it is worthwhile to consider whether we're using a general-purpose solution or a domain-specific solution. In academia, you'll generally find papers and books that discuss general-purpose approaches, and that makes sense because it's broadly applicable. But from the context of video games or other high-performance situations maybe control of real-time systems, you might want the benefits of domain-specific implementation just largely because of performance. But the general-purpose solutions are often very abstract. They work with symbolic representations, so from the era of Lisp where knowledge is represented symbolically and because of that, you have a performance penalty because you're working with a lot of dynamic data structures. >> And as compared to say, having fixed length arrays compressed representation rather than general purpose symbols having very specific datatypes that certainly restricts how you might apply such a system to other problems. You're going to be locked in with a domain-specific implementation. But that's the thing that you'll probably need to do for a video game to get the real-time performance. For instance, with Jeff Hawkins work in fear, that's one thing he gives into a lot of detail about is how he had to give up some of this general purpose ability in order to streamline the system for real-time performance. Jeff Hawkins solution was actually based on strips which is out of Stanford from Fikes and Nielsen. This was a planning system that was designed for a demonstration robot. And this robot could solve problems that involved manipulating stacks of blocks and so you could give it a particular goal of maybe configuring blocks or getting the robot to a certain location, perhaps by building a staircase out of box and then climbing. And so it represented the discretize, the state or the world of the robot, which was confined to a room like a square area I believe. It had a finite number of blocks in the room and they could be configured in different ways so you might have, say, one block on top of another so that could be represented symbolically in their planning system with a relationship of like, on, with that meaning one block on another, and it's parameterized as the symbols parameterize. So you can say which block was on top of another. And so the planning system would work by having, there'd be some number of states. And that would be like the initial state. And then the goal would be to have certain configuration of states be achieved. Action is just a small number that the robot was capable of. This might involve stacking or unstacking blocks or driving around to different locations in the room that it was confined in. So each of these actions has preconditions and effects or post-conditions. The preconditions would be some requirement for the action to be allowed to be performed. And then there would be effects that provided this preconditions are met, that the robot could reliably perform the action, and then you would get the certain effects as a result. So for instance, you might have an initial state where you've got, say, some blocks in a random configuration, maybe two stacked one by itself but your goal is to get them stacked in order or A is on top, B is in the middle and C is on the bottom. So this requires several different manipulations of the blocks of picking them up and moving them. And the robot has to be able to figure out the relationship between these different operations so that for instance, it doesn't get further from the solution or block itself from whatever the next appropriate actions should be. This is a demonstration of why the planning is needed. An operator might be unstack and we can see the preconditions. It requires that there be two blocks that are stacked and that the top block, the one that we're attempting to unstack through the operator being performed it can't have anything on top of it. So we can say maybe the robot is not strong enough or not able to balance, lifting two blocks at once or whatever the limitation is, and then the effect is going to be manipulations of the world state. And this is performed by these add delete operations. So we're adding a new symbol that defines a relationship and we're deleting some old state information that no longer applies because it's been manipulated and changed. So we can think of the search space as being these connected world states so we're representing the entirety of the world in each of these little state representations. And we've talked before in part one of this goal-oriented behavior series about optimization. So you could do here, you could just represent these worlds states as differences from the current state. So we don't have to store as much information. But we can still think of this as being this graph like structure where we have these different connected states. There are a few little substantial issues that we need to consider about the nature of the states and how the states can change, which we'll talk about in a bit so keep that in mind. So the strips actions can of course, we'd have all sorts of different actions. Here is an example. A problem involving travel. So we have different airports that say the agent might be at and we're trying to get from one location to another. So we might have flying as one of the possibilities. You might have a system like this to help someone planning on how to travel as well. This wouldn't necessarily have to be an agent. It could be like a recommendation system for how to get somewhere, which tickets to buy and building an itinerary for travel. >> Applying actions will change the world state in the context of the planning. The world state just not being the actual world state, but a representation of the world as it changes incrementally according to these different operators or actions that are applied. And there's different sequences of actions that can reach the same world state. So this is an interesting issue. And we have the same thing in path planning. There's more than one route to a location. And so you can have more than one route to a world state as well. But there is no limit to how many actions in a sequence can be applied. Typically maybe in some narrowly defined problem domains we might have limits. But generally your agent if it has some number of actions available to it it's going to be able to just keep completing things, say like stacking a block and, and unstacking, and stacking, and unstacking. It's not an easy way to limit that behavior without adding some constraints to how our search is going to be performed. And so you can have actions that reverse these previous changes that have been made to world state. Yeah, so this is different than path planning because typically at least for a video game we'll have a finite map and you can just keep searching until you exhaust all of the different nodes. But we can't do that with behavior planning because you can just keep going back and forth. So because of that it's important that we have some effective strategy to deal with. There's different things that are necessary. So since we have these worlds states that can be repeated according to the operators that are applied, is worthwhile to recognize this and stop planning when there's a repeated state encountered. So this means that our state representations we want to build to easily compare them as we generate new ones. And one way to efficiently clone them and apply incremental changes. Because generally as we apply operators just small changes are going to occur typically. And then we want to be able to hash them efficiently and effectively. And so if we can do that and then we can support transposition tables. And so these are hash tables, but ones that have only one item per hash key. And so if we have that what we can do is as we're considering the application of different operators as we're performing our search, we're going to be generating these new world states and we can preserve them in the transposition table. And then every time we apply a new operator, like some new step in our search we can check the transposition table first to see if there's already something at the hash of the state we're trying to save. If there is then rather than storing it redundantly we will either replace the entry in the transposition table. So each of these worlds is states also going to be associated with the plan so far that led up to it. And so we're either going to replace the entry in the table with the updated partial plan or we might decide that what's there is not worth replacing and therefore, we're not going to be storing what we currently have and we'll just abandon searching any further down that path. So this creates some improved efficiency in terms of the search algorithm. Now, we could possibly have states that are different that hash to the same value, but we can still leverage the transposition table for this. And so the fact that we're only going to replace entries with a certain condition. And so this can be done with a heuristic that if the action sequence like this partial sequence of actions. We haven't figured out the goal yet. This is just like an intermediate that's part of the algorithm. If we have a shorter action sequence then that will replace the entry in the transposition table. So the same hash value, but now we have a new sequence. And otherwise, we stopped searching that branch. But as in Millington's AI for Video Games book, he points out that this heuristic allows identifying the shorter action sequences in such a way that it ends up pruning off many of these possibilities of sequences of operators being applied in such a way that you avoid mini-possible redundant states. So this is a worthwhile optimization to the search, especially again considering that we can have unlimited sequences of operators applied. There's nothing that restricts, at least from the possibilities of what the agent could do so we have to restrict the search itself. Now, in addition to that optimization we still don't want to search to some arbitrary depth. We will limit the depth. And an ideal way to do this is to use the iterative deepening implementation of A star. We did not cover this in detail, but it is inappropriate variation of A star for this purpose. And you also need a heuristic. And so in this context, the heuristic estimates the distance from one world state to the goal world state. And so we'll revisit the topic of heuristic, but keep in mind that that's what the heuristic is going to do. Now, what we will end up with in terms of a plan is going to be a sequence of different actions. And we will have preconditions that are equivalent to the transitions in say like a finite state machine and the effects are the same like the results that you have from performing the action or behavior associated with a finite state machine. So if you perform A star planning with similar sorts of actions you're going to end up with the same resulting observed behavior of your agent. So it's really pretty much the same thing except we are able to remove many of the different contingency transitions that you might otherwise have to build into a finite state machine. So you get some decoupling of all of these different contingencies. So you have a cleaner representation, but overall you still have the same individual behaviors. It's just the relationship between these behaviors is going to change a bit. So in many cases you still get the same outcome. >> We're getting more flexibility because we don't have to bake in all of these different contingency transitions. Instead, we can have goals that can change. An agent might have many goals in mind, but the priorities might change based on the contingency. When goals change, that means that the plan changes. So rather than representing that like in a finite state machine, we're going to be able to just allow, sorry, the behavior planning algorithm to do all of that management for us. And so this is really where the big benefit came for Jeff Orkin in his system. Finite state machines, they're going to be telling the agent how to behave in every situation, every contingency. But in the planning systems, the agents have goals and they have sets of actions. And the agent gets to decide how to apply the actions to achieve the different goals. Because that planning is automated, it doesn't have to be prebaked in. It's really easy to add additional goals, play around with prioritization of goals. It's also very easy to add additional actions. That was a big payoff for F.E.A.R or the game F.E.A.R. So Jeff Orkin was able to leverage that. So there's a lot of agent interaction with the environment like strategically using objects for cover, hiding behind things while fighting with the player. The agents, they have to have a goal in order to be able to accomplish anything. But the agents themselves are not especially complicated in terms of what they do. They're all basically attacking the player. It's a combat-oriented game. The system is basically the same planning system with many common goals across all the different agents. In the game, there's different types of soldiers that might have different types of attacks and abilities. But a lot of them, they'll share the same goals for the most part, but they'll have different actions available to them. These actions are largely associated with animations that they play, maybe some special cases for certain types of weapons and that sort of thing, and they will have some path planning as well to perform to get around the environment and to track down the player. But the overall system is very flexible and Jeff Orkin did not need to build complicated contingencies into state machines like he had with previous games that made the state machine very susceptible to bugs, especially when trying to add new capabilities with very little time for design and debugging, but it is much easier to do so in F.E.A.R. The planning in F.E.A.R results in some different benefits. So for instance, the decoupled goals and actions. Each character has their own action set and therefore, when the search is performed, there's different unique preconditions and effects still motivated by the same goals of attacking the player, self-preservation, and that sort of thing. But this allows for late additions to character types. It automatically addresses, [NOISE] excuse me, the connections between different sequences of behaviors. And as long as you get the preconditions and effects correct, you get generally the expected behavior from the agent that you want. You also can more easily share information between goals whereas in a state machine, this requires a lot of hard coding that's very fragile. You can layer behaviors easily through just having different actions or behaviors and also goals with different priorities. You can also realize dynamic problem-solving so the system can re-plan when there are failures. If the plan is generated and the plan cannot be completed. So there's an example shown in the Jeff Orkin's presentation where there is a soldier trying to track down the player, so the first try is going through a door but the door is locked. So then the agent will go through the windows, so it breaks down the window and hops in. So in the case of F.E.A.R, there's a very flexible system. Again, it's doing things that all could have theoretically been designed as part of a finite state machine but it's a much easier system to maintain, to extend, and modify. Even the initial authoring is all much easier to handle. So benefits of using A* is that the agents can find creative or emergent solutions that weren't specifically designed, but that as individual, I guess, actions performed is all possible. It all could have been implemented with some other reactive planning. It's also can be less predictable because, as goals and situation changes, it can shuffle around or mix and match different actions to achieve different results. Downsides though, you can have optimization of actions to a fault. You can end up with, for instance, these absurd mall cop-type solutions. So if you think back to the example of the guard trying to break down the door to get to the player to attack them, if the guard already knows where the player is, it might decide that the shortest path spatially might be real rather than walk all the way around to the door, just go through the window right from the start. And that would be silly in many situations, you want to try the easiest, lowest energy effort. And that's really just a problem of defining action cost. In terms of searching, you want to have certain actions that have relative different cost. So this is tuning of issue. You could also have other crazy possibilities like maybe if the agent is aware that an object can say flip a switch, you could end up with the agent throwing a grenade to flip a light switch. So that would be the thing you'd probably want to avoid. And so the nature of having this planning system is that all of these actions are available to consider. There might need to be some restriction to make that less likely or not possible to have returned as a result. It can also be more difficult to debug. When you have a finite state machine, you at least know all the states and transitions. So when you see some behavior occur, that was the dynamic result from the planning system. It can also be much more computation-involved. The finite state machine has a fixed data structure whereas with the planning system, this has to be rebuilt anytime there is a change in goals or maybe a broken plan because a certain action could not successfully be performed. So you're having a lot of opportunities for computation. If there are lots of actions that are possible, then you can have a really huge search space as well. So F.E.A.R is not the only game to have used a planning system like this. There's also some other examples you can find if you Google and look into the implementation details, Empire: Total War, Fallout 3, Killzone, which, I believe, used hierarchical task networks, which we'll briefly mention in a bit. But yes, this is a fairly popular approach that has, especially after Jeff Orkins' presentation, generated a lot of interest. Here's another example of a scenario where we have some hypothetical characters, got this initial state, has gunForSale, ammoForSale, possumAlive. So the little tildes negate. It doesn't have a gun loaded, doesn't have food, doesn't have a gun, is not a criminal, doesn't have ammo, and so on. That's the initial state for the character, but the goal of the character is to get rich and have food. So there's all these different actions that can be performed like RobBank, ShootPossum, LoadGun, and you can see each has pre-conditions and different effects. >> If we visualise all of these different possibilities that could lead to the goal. You end up with fairly elaborate set of actions that can be performed in different ways, different combinations. It does look a little bit like a finite state machine in terms of this structure generated, but it can continue on forever if we don't have some limitation. Now the way in which these sequences are created. There's two fundamental ways of approaching this and one is to do so in a forward direction. So you're performing a state-space search where you start with the initial state, whatever the current conditions are. And then applicable actions that with the preconditions are currently satisfied, whatever the current at the very beginning, it's just whatever the current world state is. But then as we apply operators or actions, then the world state is going to change and it'll open up new possibilities, either new actions will be available or actions will be no longer available depending on the precondition requirement. So we can expand all of the possible actions from the current state, then we can test each for the goal, as we select the one with the best score, we can check for a goal. And then basically perform the A-star search, but again, most likely a depth limited search. And that seems familiar and expected way of approaching the problem. But we can also perform a State-space search that goes backwards. So we would start with the goal and assumed that the goal is met. And so what action could have led to that? And then of those actions that could have led to the goal state, what are the preconditions for that action? And then that defines what would be the action that would go before it. So we'd look for actions that have goals that address the preconditions of the other action, and so you're basically working backwards. And so there is this reasoning, this is a little example that was on Wikipedia with Fritz the frog. But it's showing how you might go about reasoning in either with forward reasoning or backward reasoning. So we may have certain rules that define characteristics of, say different creatures, and we have some knowledge. And so based on the knowledge you go forward through the rules, chaining them together until you reach the information that you're looking for. So we have rules like if X croaks and X eats flies then X is a frog. So we have knowledge that Fritz croaks and Fritz eats flies. And so we can just look through and see if any of the rules match, of course, the first rule matches very quickly and we're done. We're able to figure out our goal which was to determine what colors is Fritz. So we actually have to evaluate rule three is well. So we plug 1 and 2, it only matches with rule one. So now we have new knowledge that Fritz is a frog, then we can plug that into three and then we get green. The backward reasoning, we start with what would be valid for color. So we've looked at all the rules, and we see that there were only two the rules address color. So one is that green and the other is yellow, so we can recognize that those are the only two colors. So then we can say well, if it's green, then it's going to be a frog. So then we could find, what is a rule that matches with the result of being a frog? And so that identifies that whatever it is croaks and eats flies. And so you can see how this backward chaining can work, so this is a really simple constraint example. But let's consider this other scenario. Let's say we have a needle in a haystack so we've got this array of sticks and all of the sticks are either hay or will also say a stick can be a needle. So either it's a hay or a needle. And a goal is to stick a needle in a Voodoo Dolls, so maybe we have a video game character and wants to stick a needle in the Voodoo Doll. So there's lots of sticks around and we need the needle. So forward chaining, it's going to require checking each stick with GetStick operator or action. And then seeing if we can chain that with PokeVoodooDoll with needle, so that the prerequisite being that the agent has a needle. But with backward chaining, we can start with sticking the Voodoo Doll because the result is that action has been performed. Or sorry the goal is to stick a needle on a voodoo doll. So if that has been performed then that was the result of that particular action. So we're work backwards from that, and so we know that needs to be a needle. And so the symbol for needle is identified and then GetStick needle can be called completing the search. So you have these scenarios that can pop up. Another popular example that you'll find if you look at planning systems is say, buying a book and buy is ISBN, is the unique code for the book. So if you need to buy a book before, it's part of a sequence of actions, you may buy a book and then read it and write a book report on the book being said, the goal. In terms of the planning system, you'd have to have the agent as part of the planning algorithm you'd have to consider buying every possible ISBN in the database, which could be massive. So to us it might seem obvious, well, you just pick the correct book. Or you'd have to have a heuristic, this aware of that which might be possible in a very narrowly constrained problem. But if you have a more general purpose planning system, that would be challenging to have a heuristic that would work in a general case. So it's likely that in terms of the search, you would need to forward planning. You would need to search all of these different possible ISBN until you finally find the correct one. So this demonstrates the problem. Now of course, you could have for the needle in the haystack, you might actually want your game to have this challenge of finding the needle. But this isn't reflective of a puzzle for the agent, just searching out of the number of possible actions that are available. And it could be a result of the combination of parameters for the action could result in this explosion of possibilities and that would make forward planning not desirable. So backward planning could work a lot better in this case. So with forward chaining, you have irrelevant actions, the potential for irrelevant actions to cause a high branching factor. And in planning systems you can have agents that they're capable of lots of things to do. But with backward branching, you're branching factor can be much smaller, because by working backwards, you're often in a scenario where you've excluded possibilities because you already know particular bits of information that are desired. And so they similar parameterized possibilities like the needle in the haystack, you exclude all of the irrelevant actions by going the other way. But this can be susceptible to long backtracks where affects negate earlier decisions. So you can have trouble not being able to connect with the whatever the initial state is easily. And it can also be more challenging in terms of heuristic design with backward chaining. But I believe, at least in academia, descriptions of planning systems tend to focus on backward chaining. For these planning systems. >> A heuristic. Again, with a star, we have the sum of the action cost and then plus the heuristic. So how do you estimate the distance to the goal? And it's going to depend on whether you're going forward or backward chaining whether you're actually the goal that you're trying to reach or the initial states and distance to the initial state for backward. So our heuristic is going to be informing the search algorithm about which node to expand. And if you recall from path planning discussions, there's a good bit of attention given to the admissibility of the heuristic. So if we have an admissible heuristic, that means that we will be able to get the shortest path for path planning. But we also saw the inadmissible heuristics and the benefits that that can have. Because it can give us a more informed estimate. So we might be able to sacrifice some possibility of getting optimal solution, sacrifice a bit of that so that we spend a lot less time, a lot less effort in the search expanding nodes. If you want an optimal solution use admissible. If you don't have a heuristic at all, this is going to be the same as Dykstra's because the H is zero. So all we have is the cost is we expand. But if you have a heuristic that more accurately reflects costs, then the search becomes more informed. So in this general planning, inadmissible heuristic can actually be beneficial in similar ways that we saw with the path planning. So we might be willing to sacrifice this optimality for the purpose of the real-time performance of our game, for instance. So in terms of a heuristic for a star path planning, it can get pretty challenging to design a heuristic. So it depends on if you're going with forward chaining or backward chaining. But it'll generally be based on the cost of what are the differences in the world state representation that you currently have that you're considering and what is the desired world state. So whether you're going forward or backwards, we're comparing world states and differences in the world states. Now you can potentially do some pre-computation or analysis to come up with maybe some fixed or perhaps parameterized function that would give you a heuristic cost for different, say actions that can be performed. And the way in which these are compared, you probably want some common unit of comparison. So if some say a particular symbol in the world state might be associated with one action. So that would be straightforward, like if it's always only that one action, you could just have whatever the cost of the action that you know manipulates that symbol in the world state. That would be your heuristic value and it would be a very accurate one. But you can't have things that involve multiple steps and they can be variables steps. In terms of comparing these, again, want some common unit of comparison, it could just be that an inaction is like counts as one. So you might say normalize it just all actions count as one. But some elements of the world state might say involve several of these actions. So maybe you'd have an estimate of three for a particular symbol that could change. Or you might have maybe rather than each action has just one unit, you may be as based on time. In a video game, you might have actions that are based on animations that play and so maybe you normalize this heuristic scoring based on the amount of time, but there could be other game resources as well that maybe makes more sense. This is something where it's going to be very domain-specific in how you go about doing things and how much work you want to put into heuristic design. Now from academia, there's some suggestions on how to deal with a heuristic if you want it to be admissible. In order to be admissible, if you have say you're trying to estimate again differences in world state and you say you have a handful of things that are different would suggest if got like actions associated with each of those differences. Like you've got to get, say, collect a number of things. The character has to have collected like four or five things in their inventory. Then you're probably going to have like four or five actions at least to realize those changes. So the safest thing to do is that you could just pick, well, addressing those preconditions, which is the one that is going to cost the most. And so just use that as the estimate. That one thing. Now realistically, you would think, well, the agent still has to do all these other things. So surely it would cost more than the maximum, but you can't guarantee it because you might have more side effects from whatever actions are performed. You can't guarantee that they're independent of one another. So the safest thing is to just only take the maximum. But if you take the sum, again, if they're not independent, that might overestimate. So if we take the sum of estimates of whatever changes need to be made between these two different worlds, say it's the current and the goal world state that some might be overestimating if the action is being performed are not independent of one another. But you could probably argue that the sum in most cases is a more realistic heuristic. So in fact, that's what was used in fear. If you listen to the entire presentation, there's a question at the very end of the presentation someone in the audience asked. >> Each action, there's an assumption that for each difference in the world state that there is one action associated with changing that and it costs one point. So all the actions are just weighted equally with this value of one. So if there's four things different between the current state and the goal state, then the heuristic is that it costs four units. And Jeff Oregon can said it worked fine in his situation. So you probably had a lot of independence between the actions that were performed may be entirely guaranteed independent, perhaps. Which and I think in a simple game that might be a reasonable thing. So the benefits of planning, decouple goals and actions and because of that, you can do things like what Jeff Oregon was able to do and fear creating these new character types with minimal effort. The effort fell more on the side of the artists and animators rather than him as the AI behavior designer. And overcomes issues that make state machines that become unmanageable because of dealing with all those contingency type transitions. And also allows for dynamic problem solving, ability to replan when failures occur. So basically capturing the ability that you would have in a finite state machine that truly correctly captured all contingencies, but without having to put in that development effort and debugging. So where do you go from A star behavior planning. So there's clearly limitations. If you have lots and lots of actions, things are just not going to scale very well, you're going to have a heavy burden for any search defined possibilities. One way to address that is with hierarchy. So you can have a hierarchical task network. And this is at least at a high-level, works much as you would probably expect, we're going to encapsulate more primitive actions within higher level descriptions. So for instance, going on a trip might say at a high-level you're just going from location A to location B. But that could consist of lots of smaller steps like getting a cab, taxi to take you in the airport, you take one airplane flight to one city and then to another or if you have a direct flight and there's all these different possibilities. So here's other things as well, I maybe build a house in a game or perform some strategic maneuver and a military game. And so there's an assumption that all the lower-level preconditions are met when this higher level plan is made. So once a high-level plan is made, it needs to be broken down into the smaller levels and then check to make sure all the correct preconditions were met at the lower level, and if they're not, then that's going to bubble back up to the higher level and require changes to the plan to fix it. But assuming that the high-level plan is feasible, then you can decompose it down into all of those base low-level actions. And those are what the agent is actually able to interpret and perform. So this has benefits because it limits the number of operators that would be performed at any given point in the A-star search. So you considerably decreased the number of actions that need to be analyze but then you still have this opportunity to double check and make sure that it's truly possible. So you're given a task and then you pick a method which conditions match the current world state or I pick randomly. Then you work your way down to the lower level in the hierarchy. And then consider the application of those lower-level operators. And keep continuing the process. And then the agent can execute the full plan once it's realized. Another interesting thing, is that you can create a partial plan maybe by only decomposing early parts of the high-level plan. and assuming that the high-level plan will continue to work, but if not worst case is you just re-plan and so there's these performance benefits again, because you're able to do a lot of the searching aspects of the planning at this higher level and therefore less actions and state information to be considered. So we could think about how this might work better than that. Let's say an A star behavior planning approach with a hierachichal task network. So such time. Very likely we're going to see these improvements because we're representing lots of primitive actions with these higher-level actions. Authoring time maybe be a bit more work with hierarchical task network. But there also tends to often be natural structure to organizing and the fact that you are organizing could be beneficial just in terms of managing and thinking about the problem. Now, optimality is a situation where the hierarchical task network can lose out, the abstractions that are taking place. Might mean that some sequences have lower-level actions or behaviors, will never even be attempted by the search algorithm because you've got these boundaries between the higher-level. When you decompose down from the high level to the low-, there's these boundaries imposed. And so there might be optimizations that if the entire A-star search as performed, you can work out a lot better. And also this similarly with the optimality, not noverlty or predictability, and be a problem, just like if you had everything A-star, you might have more novelty because more possibilities are explored. And lastly, partial solutions you can more easily do that with hierarchical task networks because you can just work only with the high-level parks that you're interested in the short-term and then later decompose as needed. So that concludes our look at goal-oriented behavior and ASR planning.
