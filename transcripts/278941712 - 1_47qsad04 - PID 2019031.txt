>> Hello. In this lecture, we are going to look at exploration versus exploitation in the context of learning. We're going to look at the one-armed bandit problem. If you've taken a machine learning class before, you're probably already familiar with this, but if not, the basic idea is that we're considering a scenario where an agent is playing a game. So we'll start off just by considering the simple problem of observing the payout of a game like a slot machine, so where also called a one-armed bandit. So if we're playing a game, there is no choice to make other than to continue playing, but we might want to know what the payout is. Each example where we pull the lever or put her in money and pull the lever and then see what the payout is. So we can track this as a probability of winning. We'll keep it simple and there is not going to be varying amounts of winning or losing, there is just you either win or you don't. And so if you win, that is a positive example and if you lose, that is a negative example. So you can just tally up the number of wins over the total number of times you've played. So you might have, say, one win for one attempt and that would give you a probability of one or you might have one win out of two attempts. So that would mean like one win and one loss, so you'd have 0.5. And if you keep track of these multiple times pulling the lever, then you can track and as you get more and more samples if you have some fixed payout rate for the game, then you're going to converge on an accurate measure of the payout for the machine. So when you first start off measuring, you're going to be susceptible to chance. Just the random nature of the game as to what your estimate is, what your belief is in the payout. So it'll move around a lot at the beginning, but then it'll converge over time to something that accurately reflects the random distribution that the machine is based on. So in the case of the one-armed bandit, there's no decision to be made. It is all just you have to play if you want to win, but what if we have multiple machines? So we may have a whole number of different one-armed bandits that we can pick from and these one-armed bandits might have different payouts. So if there are different payouts, that means that there is going to be this dilemma. We are going to want to play all of the machines to try to figure out, well which one is best. But during that process of figuring out which one is best, that would also mean that we're not optimizing our winnings. So we might be spending money unnecessarily, just so that we can try to get a better estimate of the payout for a particular machine until we finally are satisfied. This dilemma is exploration versus exploitation. So again, our goal is we want to start making money as quickly as possible, but long term we want to make sure we're making the most money possible. So if we consider a strategy of exploration, we might have a situation where we have three machines, we don't know what their payout is. We can figure it out if we just keep randomly selecting between the machines. So if you have three machines and you sum random process like a pseudo-random number generator and pick which machine we are going to try next. So regardless of what information we've collected. So if we are tracking a table like described before, where we know a tally of how many times we play each machine based on the history of wins and losses, then we would expect that we will converge on an accurate estimate. What we believe to be the payout for each machine. If we just stick with this strategy, there is never a point where we can now take advantage of this information that we've learned. On the other hand, we could utilize an exploitation strategy. So this is where we'd use a greedy strategy where we're always going to pick the machine we think will give us the best reward. And this is not really well defined what it means to use a greedy strategy, separate and independent from exploration. The simplest thing you could probably come up with is just don't bother with tracking any stats on the machines, just pick your favorite machine and you stick with it. And so whatever the payout is for that machine, that's what you're stuck with forever, as long as you're running through this decision-making strategy. Now we might alternatively keep track of stats and then maybe actually have the potential to change machines. So any possibility of changing machines might introduce some element of exploration. It does allow us to update our belief based on the tally of the machine. So we may choose as a first possibility, we just assume the beliefs for all the machines are zero. I guess you could also say there's an uninitialized, so either zero or uninitialized, and you could pick the machine with the highest belief. And if there is no machine with the highest belief, like they're all uninitialized, maybe you pick randomly. And then the first machine that you test that has a success, well, suddenly that machine has the highest success. In terms of decision-making, that value will always be higher than all the others so you will never abandon it, even if your first early success is followed by losses and the information you collect maybe converges on a low payout level. So another thing that you could do is, you could say, well, rather than assuming that we're going to start with an assumption that the machines have a zero belief, we can instead assume they have 100% belief. So that'd be at a value of one for every machine. Now that value by itself isn't very helpful, at least in terms of our tallying, because we also initialize not just the win rate at one, but we also need to have some value in for our tally. So we might assume that we're initializing every machine with a belief of one and pretend like we've had some number n of trials where they've achieved that level 1. >> Now, if you do that, what will happen for all the machines that their payout is less than 1, that number will shrink over time as you play. Say you try the first machine, the first loss as you update the tally, you're going to be knocked down from 100% down to something less than 100% depending on what n is. And then you would pick a new machine. So the greedy strategy of always picking the best machine, we would engage that logic and be able to switch between machines. Now if that n value though is very small, we might suddenly have our value of 1 drop down substantially. So say you initialize with n equals 1. So the first time you attempt to play one of the machines, say it's a loss, you would go from 1 out of 1 to1 out of 2. So you'd immediately drop by 50% and then another loss would drop you down to a 1/3. And so some bad luck at the beginning might knock the value well below what the machine's actual long-term payout is. And so bad luck early on could result in the wrong machine being identified in this greedy strategy. So even doing that, it's not perfect. We could make n very large. But then that means that it takes a very long time to converge where we know accurately what is the right machine. So there would be a long period of figuring it out and there's still always that chance that the n was not big enough. There was just such a long, unlucky streak, again priming with I'm a belief of one or 100%, pay off and then a very large n estimated or we're pretending how many attempts had already been made, even though that's not accurate. Another approach you might take is anytime a machine loses, regardless of what its running tally is, we might just say switch anytime there is a loss to the next best machine. So if you're playing a particular machine because you thought it was the best, but it has a loss, you then switch to the next best. So that helps to get the agent or the process switching machines to do a little bit of exploration. But the problem is you might still have a scenario where a bad luck streak would push one machine to a low estimate and then you get to the point where maybe you're just bouncing back and forth between two machines. So you pick what you think is currently the best machine and you play it until there's a loss, and then you switch to the next best machine. You play it till there's a loss and then likely the next best machine that you switch will be the one you started with. So you just oscillate back and forth between those two. And if any good machines were unlucky to get bad short-term stats, then they will never be revisited again. So that's not really a reliable strategy either. So one other possibility that we could consider is that we can just visit all the machines and visit each n times. So there's some number of times that we will determine that we want to sample. And that will allow us to control how much effort we put into getting some statistical measure of the performance of the machine and get a good estimate of the probability for the payout. Once we have collected all that information, then we can switch to the greedy strategy and simply use the best one. And so that could actually work pretty well. The trick will be making sure that we pick an n that is big enough to achieve statistical significance. So that's going to be a critical aspect to the algorithm. So that is a good segue into another strategy which is to split up our time between exploration and exploitation. So what I just described is a way to do that where it's called Epsilon first. But more generally, Epsilon greedy is really the strategy that's going to be most effective. And so for Epsilon greedy, really just saying there's a ratio defined for our decision-making. This ratio is saying how much time are we going to spend on exploration? So we're going to spend most of our time selecting the best machine. So this greedy selection of whichever machine is best, but some fraction of the time we're going to not use our current best estimate, instead we're going to try some random machine. And so that'll give us an updated tally of the machine performance. So we're always increasing the quality of the stats, but still taking advantage of whatever the current stats are based on the machine performance. So this allows the algorithm to escape from early greedy mistakes. So even if a good machine accidentally or not accidentally, but just by luck had a very bad run but in the long term it has a higher payout then what that would indicate, then you can recover from that. So if you're greedily selecting the wrong machine, since you're still occasionally spending some time trying the other machines, you'll be able to recover from that. >> So one way we can analyze the performance of any strategy for trying to take advantage of the multi-arm bandit scenario. So trying to figure out what's the right way to handle exploration versus exploitation. We could imagine that there is a perfect agent who will always take the best action. It's like an oracle. If it's possible to get the best choice for any given time step of pulling the levers, whichever machine wins, we'll use that as the baseline of comparison against other strategies. So we can compare the agents payout or that agents pay out, the perfect agent with our agents payout, whatever the strategy is for selecting. And that difference, that defines the regret. So we can sum up regret across all of the different timestamps, all of the different opportunities to pull the lever and then that'll give us a total regret up to that point. So we can compare different strategies by how well they do relative to the perfect agent. So we might get a graph that looks something like this for different strategies. So we've got our timesteps on the x-axis and then total regret on the y-axis. So we would expect that regret would increase over time because we're not going to build or make a perfect agent. We have random processes, so we don't know what the true best choice is for something that is a random process. But at least, we can compare different strategies to one another in terms of the graph. So the greedy strategy, you imagined that you would likely get very quickly to the point where you're just picking one single machine. Now, there might be a little bit of variation early on if we had any of these variants of greedy, so depending on how we initialize the values and whether or not we're just blindly picking one machine and not tracking stats. So you might have some small bit of a curve at the beginning, but you would eventually like Lee would settle on one machine indefinitely from some point onwards. So you'd have a straight line. That straight line would not reflect the best possible outcome because there's tthat other better machines. So you'd have the regret linearly increasing. Now, with epsilon greedy, you would also have this linear increase and that's because you have this exploration cost. Even if you find what truly is the best machine, one-armed bandit to pull, you're still going to be accumulating this exploration tax basically. So that after some number of timestamps, you'll have this linear curve. But it will tend to be less than the greedy strategy, because the greedy strategy, at least on average, there's a good chance you would arrive on the wrong machine. But we could modify the epsilon greedy so that we have decaying epsilon greedy. And by that, I mean the ratio changes over time, so the degree to which exploration occurs. So I showed an example before where 10% of the time exploration was applied and then greedy the other 90%. But with decaying epsilon, we might start off with say 10, but then we would slowly decrease it. So if you do that, the idea is that you're going to collect enough information hopefully to have an accurate estimate of the payouts of the different machines early on, early in the timesteps that are taken. Then we'll assume that we have converged or come close to converging as we reduce the epsilon. So the benefit of doing this is that once we're satisfied that we have a good estimate of each machine's payout, we then transitioned to taking advantage of that information. So this is generally a desirable strategy when you have a learning problem where you're trying to balance exploration versus exploitation. The downside is that you need to have enough domain knowledge to come up with the schedule beforehand. So you need to know how much sampling is necessary before the measurement is reliable, that you can trust it. So this is something that you would need to tune and maybe be informed by the problem domain in order to come up with this. There's not a one-size-fits-all decay rate that we could pick from. But the multi-armed bandit problem, it just demonstrates this really simple example of reinforcement learning. And it's important to think about this thought experiment and understand how it can relate to other types of learning problems as well. So that's why we went through the example.
