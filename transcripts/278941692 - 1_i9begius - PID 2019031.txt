>> Hello. In this lecture, we're going to begin looking at goal-oriented behavior. So we're now going to transition from decision-making as we've considered it so far, and introduce the concept of goals for our agent, and how goals can be used to motivate the agent to find or plan for a particular solution towards achieving a goal. So first of all, I want to mention that goal oriented decision-making in academia is focused on optimizing the likelihood of an agent achieving a particular goal or a reward. But in game AI, like we've mentioned before, but it's worth bringing this up again, in our game, any of the AI solutions that we look at, including goal-oriented decision-making, is that we are focused on generating or selecting, planning a behavior that supports the gameplay experience. So again, that leads towards simplification of the problem, cutting corners, cheating in terms of finding our solution. Now that said, goal-oriented planning, decision-making in general, this is probably an area where there's the most similarity between what you find in academia and game AI as compared to many of the other artificial intelligence techniques that we've looked at so far. So in regards to reactive decision-making, we've looked at decision trees and finite state machines and behavior trees, fuzzy logic, rule-based systems. And so all of these approaches are reacting to conditions in our game world that are accessible to our agent. And so these are all based on some predefined plan that was designed by the programmer, the game developer. So successfully completing a step in the plan opens up the opportunity to complete the next step, provided that it's successful. But the agent only knows in terms of it as decision-making is just recognizing that these individual steps can be completed. So implicitly, this can lead to a goal being achieved, but the agent is never aware that it's working towards that, it's never selecting actions because it is motivated by a goal. Whatever the notion of a goal is, that is in the head of the programmer, the algorithm designer. So that means that there can be some limitations in what the agent can do. So if we can involve the goals more directly with the agent, that might open up some possibilities. So let's consider some problems with reactive decision-making, things that we're not able to do with the techniques that we've looked at so far. One is we have this shallowness in terms of selecting actions. So all of the techniques we've seen they have the agent take the next best move according to whatever is available. So for instance, a finite state machine, whatever the current state you're in, you have some number of transitions. Of those transitions, you'll expect that the most appropriate transition is selected, but that's only if the developer has anticipated it and configured the criteria for the transition such that it can guarantee that it is truly the best one. Now, if some scenario is not properly considered by the designer, then maybe the correct transition is not taken, maybe the wrong ones taken or no transition at all. You have similar problems with rule-based systems that can result from the nature of the rules that are available, and also possibly the configuration of the arbitrary. So whatever arbitration takes place to select a rule of multiple activated rules. Related to problems of decision making, this is one of the things that would lead us to think, or maybe if we can include goals in the algorithm, that we can improve things. Another issue is adaptability. So again, this shallowness that we have represented in the techniques that we've seen so far, that means there's this lack of adaptability. Our agent can only respond according to the plan that is laid out for them. This is not a plan for different particular goals that might arise. It's like one collective plan that can address any goal that might be prioritized that the designer has anticipated. So that again restricts the ability of the agent. And the end result is that any unanticipated situation, the agent is not going to be able to respond well. This is also an area where if we can involve goals in the decision-making process, then we can get some gains in adaptability. And also there's a heavy design burden. So all of the decision-making techniques that we've looked at. This requires that the designer built into the structure are all contingencies as a way to encapsulate goals that might arise at any point in the life of an agent, at least in the context of the video game. So what are the different things the agent might need to do in order to maybe survive in say a combat situation or other gaming type scenarios? And so we might be able to, through the introduction of goals, avoid some of this heavy design burden by introducing adaptability and make some gains in terms of easier to develop agents that are already able to better respond to situations. >> So we have this desire to develop agents motivated by goals for their decision making and because of that, this is generally going to suggest that our agents are performing some planning. And so we've talked about planning before in the form of path planning, and we'll actually see that there are similar types of concepts at a high level as we go from path planning to more general decision-making planning. But in video games, there are also other simple goal-oriented scenarios that we can consider or maybe simple enough that the planning is really a trivial thing. In fact, you might not even recognize it as being planning per se. So we're actually going to build up from the very simple example, following along in the way that Millington presents in his game AI Book. So we're going to look at very simple goal-oriented planning for agents in a scenario that is basically like the game, The Sims. And so if you have ever played The Sims, you might be familiar with what we'll talk about. I'll try to give a brief description of the sense if you have not played it. So in The Sims, you as a player you're like a God. Meaning that you overseeing the well-being of your Sim or Sims, and you have indirect control, which is common of what I would call the god game genre. So you're not directly controlling the character. You're not telling them exactly what to do. Instead, you are manipulating the environment around the agents and they have free will to some degree, at least according to their AI implementation. So the main way that you interact with the AI is that what you're looking out for their well-being and they have different needs and wants, and you're trying to address that by providing artifacts. So these could be different objects that help the Sim. This can include things like building them a house, putting furniture, and various items inside the house. And these can be related to the various needs of the Sim, so these little human characters. So for instance, they might be hungry or need to go to the bathroom or sleep, that thing. And so you're responding to the Sims by observing their emotional state, I think they have little thinking bubbles above their head that show what is really on their mind and so you're always having to manipulate the environment around the Sims. So a critical piece to the game implementation is the AI. And so this is a goal-oriented AI system. And so Millington describes something that, I believe he worked on a game but not The Sims specifically, but a similar game and very likely the games of this genre of work in some fashion similar to what Millington describes. So we have the agents like the Sims, modeled with various needs, and these can vary with time and conditions. So for instance, over time agents can get hungry or sleepy. And as we'll see, even some of the behaviors that they can perform can influence things like if they exercise then maybe they get hungry sooner. So the basic approach is that the agent, the implementation, the goal-oriented behavior, is that each Sim is selecting an action that addresses current needs. And so these needs are the character's goals, is basically reducing the need. So we can think of motives and goals are unified and so this is one of the aspects of where maybe in academia AI research these might be distinct, but often in games, you can just treat them as the same. So in The Sims, these needs can be things like hunger, the need for food, bathroom breaks, social interaction, shelter, hygienes, like being able to take a shower, and there's other things as well like having fun, addressing boredom. So there's all these different needs that you quickly learn about as you play the game and observe your Sim and see how happy they are. And your goal as a game player, again, is to look out for their well-being and keep the Sims happy. So one aspect of the needs is that it's not just a on or off thing. Typically in the gameplay, you're going to have what Millington refers to as insistence. And that's basically how strongly does the agent needs something. And so in the case of hunger, a one would be maybe they're just a tiny bit hungry and then you work up in the scale. Well, I guess one would be full and then two would be hungry, and then you work your way up to five where they're just really hungry to the point that is detrimental to their health. So we can represent the insistence with the value. It might be like a discrete value scale. It could be real number values that increase. And really you can change to different types of representations of insistence and still the overall concept will still be the same. >> The reason why insistence is useful is that we can make comparisons between the needs. You could include a weighting, maybe some certain needs would be weighted higher but for the sake of the example moving forward, we'll just assume that they're all the different needs. Insistence values are normalized such that they are directly comparable. Again, we are going to want that because when the agent is making a goal oriented selection of a behavior, we are going to involve this insistence. In terms of the behaviors, these are going to be actions that the agents can take. Each Sim might consider all of their different needs and the insistence values related to each, and then consider all the available actions. Now in the game for the Sims, most all actions are provided by the affordances of artifacts. Again, in this gob game scenario, you're buying various gadgets and gizmos and so on. Then each of these the agent can interact with in some way. A bed, the agent can lay down and sleep on. If the bed is not there, then they cannot sleep. This is the whole mechanic for the gameplay, is providing the artifacts to allow for these different actions. Other examples, you might buy a refrigerator or an oven and groceries and then that opens up possibilities for the Sim to address the need related to eating, resolving hunger. If your Sim is having complainers complaining about being bored, then you could buy a video game system or a pool table and that will address boredom with this fun experience. Once you can provide the Sim with these different artifacts and they've got these actions then the Sim can perform one of these actions. But each of these actions will have a side effect. Now in the simplest case, you might just focus on the positive result that being the reduction of insistence of a need. If you reduce the insistence, then that is a good thing. Sim will have one less problem at the current moment. But you can also have side effects that are negative. You might have a dynamic that say, addressing the insistence of one need might increase the insistence of another need. It's also possible that you could have some actions that don't directly satisfy the insistence of a need, but it could be a critical step in a multi-step sequence that then would lead to resolving the insistence of a need. Back to the example with the refrigerator and the oven and buying some groceries, you might have raw meat that's in the fridge. Well, raw meat you don't want to eat, you want to cook it first. We could have a whole sequence of actions of going to the refrigerator, getting the raw meat then putting the raw meat in the skillet, put it on the stove, wait for it to cook, then put it on a plate, take it to the dining room table. This whole sequence, it's not until the very end when the insistence of hunger is resolved. Now that's a bit more complicated example for this goal-oriented behavior that we're hoping to implement for our agent. We're going to focus on simpler examples first, but we are going to build up to handling that scenario. A simple approach here is that we can have an algorithm where we just select the action with the most utility. We're going to define utility as just a reduction in need insistence. We're going to select the one that got the biggest reduction. If we happen to have a tie, well in that case we can just have a random selection tiebreaker. But in this scenario, all actions that would be considered are going to directly address the insistence of any given need, that being our goal. We're not taking into account negative side effects like the impact it might have on other needs. If you consider this is a very straightforward algorithm, you can probably just imagine in your head exactly what you would do, just iterating through your different actions and then seeing the impact on the needs and then just keep track of which one had the biggest reduction. Again, we're considering that all of our needs insistence are normalized so that they are directly comparable. We can just pick the biggest of all of them. This is very similar to a rule-based system. We could actually think of all of these actions as being triggered if they're available at all, it's like they're all triggered at once. We are determining a priority. This will be a dynamic priority because we've got their state and their state being the specific insistence value. For instance, if you have a hunger of the lowest value, eating a big steak, that would normally say wipe out a Level 4 hunger. That's not going to have a very big reduction in insistence. It'll have no effects. It's dynamic because of the very variation in insistence for the particular Sim. The algorithm very straightforward, it's just O, sum of goals plus actions. >> We can consider an advanced scenario. Again, we're going to progressively consider more complicated examples. Here, I've got a bit from Millington, the Chapter 5, Section 7.3. He's got a goal where all of the needs and the current insistence levels are listed. He is at Level 4, which is very high, 4 out of 5, bathroom, is Level 3. Then we have right now only two actions available. There is the Sim could drink a soda. I guess they have a can of soda nearby that would reduce the insistence of eat or hunger by two. In this case, with this more advanced scenario we have a negative impact and increase in insistence of bathroom of plus 3. Then there's also Visit-Bathroom, and so that only has a positive effect of a reduction of bathroom of four. In this case, there are current need for eating that is at Level 4. You might think, well, if you address the the highest insistence need, then we would select Drink-Soda to reduce it by two. Now we can see the bathroom would then be increased to six or five before capping at five. We can assume that reaching the highest level of insistence in the Sims means something bad happens like a puddle on the ground for the Sim, and there'll be especially upset. We actually, from this example, see that we need a new algorithm that determines the impact of selecting and applying an action that fully considers both the positives and the negatives. For this new metric, we want discontentment. Discontentment is a metric that is applied to all of the needs insistence values. The Sim's goal is now to reduce overall discontentment rather than just selecting the action that has the biggest reduction in any given needs insistence value. [NOISE] Because of this change to the metric, we can now properly consider both negative and positive effects of our actions that the Sim might select. Instead of maybe selecting an action that then in the very next step leads to bad outcome like the bathroom disaster. Now, we have some better informed decisions on what action to select on the part of our Sims. Now, in terms of the discontentment metric, instead of a simple sum of the consistency of each need we want a nonlinear mapping of each insistence towards whatever the value is for discontentment. For instance, if we had a scale of 1-5, we might want to have like a four be more than a linear difference, greater than a three level insistence. A simple approach is just to take the square of the value. If you have a Level 4 insistences for a given need, that would be 4 times 4 and then you would add that to the square of the next insistence value in the host, whatever your total number of needs are. Now, again, the algorithm is still very simple, but we do have to go all the way through all of the goals, for each action considered, all of the goals have to be considered in terms of the impact. Then because the negative effects we can cover, we have positive effect on one need and a negative on another or some arbitrary combination of positive and negative effects. Then also in terms of calculating our metric for discontentment, we have to evaluate all of our needs. This means rather than the sum of goals and actions we now have to multiply them. A little bit more complicated in terms of time complexity, the memory requirements are still the same, they go one memory for both approaches that we've seen so far. But this is really a big improvement in the quality of actions selected by the Sims. Now there's problems with the discontentment strategy. One is that it doesn't fully take into account side effects that result from actions being performed. All we've seen is just the immediate effect on the insistence values of the needs. But we could also open up new actions in the future. This is getting back to that example, like the cooking example. If you take the meat out of the fridge the next step you have new actions available you could put the raw meat on the stove. This current algorithm doesn't capture that change in effects and what that impact might have on reducing discontentment. It also doesn't consider timing. If actions have a time element to them, how long it takes to perform, it doesn't consider that. Actually, we're going to first consider second item, which is the simpler problem to address. If we want to consider the impact of time, then we'll need to keep track of how long actions take to perform. Also, we might consider insistence of our needs that increases over time. We might have some rate at four. If we have an action that takes a certain amount of time, our algorithm could consider well, over that duration, the other needs, the insistence values would then increase according to the rate times whatever the amount of time has been taken. This discontentment calculation will be adjusted to consider the whatever these known action durations are in the insistence increase rate. Just multiply it out as part of the algorithm. >> So here is an example again from Millington. We have needs eat is currently at level four and it also changes at plus 4 per hour. And then there is bathroom currently at Level 3 and it's also a predefined rate, but it's at plus 2 per hour. So within the hour both would be maxed out at full Level 5. Then we have some different actions that can be performed. So there's eating a snack, which will reduce the insistence of eat or hunger by two, but it takes 15 minutes to complete now, so that's new information related to the action. And then eat main meal. And that involves preparing the meal. So it takes a lot longer, but it has a higher reduction of the insistence value. So eat is minus 4, but it would take an entire hour. And then there's visit bathroom minus 4 takes 15 minutes. And so we can consider each if they're applied so that the algorithm would make this determination. So we figured out what the new insistence values of all of the Sims needs are. So eating a snack, eat would now be two instead of four. And the bathroom will have increased according to the rate, which is two per hour and times 15, so that's a quarter hour. And so we would now have 3.5 and we can determine an overall discontentment. And that would be 16.25. And so we have similar calculations that are made determining the reduction of each of the needs if that action was applied. Now, this is a simple example where only positive effects, meaning a reduction in insistence is considered. You could also have negative effects as well. But knowing this duration that it takes for each, we can go ahead and plan out what the end result would be for performing that action. So again, it's just multiplying the rate of increase of insistence for any given need times the amount of time that the action takes. And so this allows us to more correctly consider the impact of taking an action and definitely lead to some more complicated behavior. Now, one issue with this, in the Sims, your action durations will generally be known just by how long an animation takes to play. But in some games, you might have an order for an action to be performed. It might be preceded by a path planning for the character has to walk over somewhere. And there's a little bit of that in the Sims, but you tend to just be in the house very short distances to travel. It's important to consider that you're not always going to be in terms of the algorithm. Not always going to be able to just have a pre-computed value for how long and actions is going to take. So not all needs are going to have predefined insistence increase rates. So this is another thing that can vary. Some will vary over time. And in the Sims, this might change according to say, maybe based on a day schedule. So perhaps the character gets hungrier in the morning. So maybe the rate will increase more quickly, perhaps at certain points of the day to try to motivate certain types of gameplay. And if you do have these changes in rates, the game engine can just provide those to the agent. The agent just as says game engine, what is the current rate and it can just plug it in in terms of calculating the overall increase in insistence as actions are considered. However, some rates cannot be predefined in this fashion according to some schedule. So you maybe need to think of other game scenarios beyond the Sims where you would have goal-oriented behavior. That might be a situation like, let's say, a real-time simulation of combat. So you've got agents that are fighting with one another. And so you have maybe various resources, and one of the resources will be health. And health value can drop according to say, interaction with the human player or interaction with other agents. And you could analyze the rate that the health drops over time according to say, an action being performed like attacking. So if you have an agent that is considering an attack, behavior, or action, then if it knew the rate at which it loses health while performing the attack, it might decide, well, they going into battle with a 100% health, it's expected it'll drop to 70 for a round of attack. That might be acceptable. But if the health is already at, say 50 and you expected it to drop to 10 or less then perhaps the agent instead picks to do something else. Maybe like further recovery before considering that. Now the challenge is, well, how do you figure out that rate? And that involves, you need some way to sample the values that are occurring as the game progresses and be able to get an average over some window of time. >> So in the example just given you maybe measure how it drops over time while performing the attack behavior and then you can preserve that information in the form of this moving average and then be able to use it in future goal oriented selection of actions. Now, in video games, this particular approach that Millington term suggests is very useful. In fact, you'll find it useful in aspects of game development beyond just this scenario. I use this quite a bit anytime that I need rates in a game. If you want a truly correct moving average or sliding average, you will generally implement this with a circular buffer, where the circular buffer has enough elements that it represents samples at uniform time intervals over the length of time for which you want the moving average. As you add new items, you're replacing the oldest data. So once the buffer fills up, you start replacing the oldest values and you keep a running tally and of course you know the number of elements so to perform the average, you can divide if it's like a boxcar filter, where all items count equally or you can have other filter shapes maybe if you want to say, have a weighted average with a bias towards the newest elements. But all of that requires memory storage, and complexity of maintaining the circular buffer. Instead, a much simpler thing is to just have a single variable to store but obviously you're giving up accuracy in doing so but this works fairly well in practice, and is to have an exponential moving average. And so you have some logic to how do you initially populate the thing. And so if it hasn't been written to you before, the very first value gets to be the overall value. You can alternatively just have some neutral value like zero. So you just assume you're starting with zero and then the values that you add to it have some little incremental contribution. And so the basic idea is you make the assumption that rather than dropping the oldest value out you're just going to to reduce it proportionally. So you'll have an Alpha value of say, 0.05% and say that this new value that you're adding to this fake running average, I often call the exponential moving average, like a poor man's moving average, meaning it's just a little cheap, easy to implement approach. But so you might say 0.05 times the new value plus what's representing all of the old values. You would do 1 minus that 0.05 or 0.95, and then that would be times the, whatever is your exponential moving average from the previous frame. So you get this new current frame value and so it will trend towards whatever the newest values are if they tend to be the same. So you'll have this exponential drop-off of the values. But this is perhaps, maybe more detail than necessary, but I think it's such a useful technique that it was worth discussing a little bit. So again, anytime that you need to track a trend in a value, in a video game this is great. But back to the topic at hand, if you need to monitor a rate for the purpose of selecting actions in goal-oriented behavior this is a nice, quick and easy way that you can do this. So next step, we're going to start to, begin to think about proper planning. We really haven't had much of any planning so far other than just evaluating this metric and then you picking whatever has the maximum reduction in discontentment. So with the Sims example consider again, the actions taken might result in new possible actions in the next opportunity to make a selection. So if you're trying to, in the current moment, that the agent is making decision, if you want the agent to make an optimal action selection then it needs to consider the effect of following actions. And so this is where we're really seeing this need for planning. So for our Sims example, we can consider maybe a short sequence of actions that can be selected. But of that short sequence, what is the best sequence at reducing discontentment? But we can't just evaluate the sequences like just all random combinations of sequences in the current step. Because the actions we have available right now, that doesn't necessarily reflect actions in the future. So this is really complicating things because if we make, say, the first action selection of like if we have 50 available right now and we pick one of the 50 and then we have to somehow figure out, well what new actions would be available and try to represent that in a way that then when we pick the second one, that we have the proper selection and then the third one. And so at each level there is a change in how do you keep track of what actions appear or disappear according to the effects of each action. And so this necessitates representing world state. And this world state it's just a reflection of what the possible actions are in the game. And also the state of the agent. So the change in the insistence of each of the needs. So world state is in a discretized form of whatever the actual simulation state is. So we're going to have this simplified representation and is just enough information to facilitate our planning that we're performing. And rather than represent everything in the world, one little optimization we could do is we can say, well, the current world state already provides us with a lot of information so instead of just re-representing everything that might matter to us, instead we could keep track of a difference of vector. And so this would be, as we consider applying each action, all we need to do is say how is it different than the current world state? And so this will be a list that grows in terms of information. As we apply more and more actions to this possible sequence, we're going to have more and more changes the effects on the world. Some other aspects to these world states that we might find useful in the future I'll go ahead and mention now to be able to copy world states. So make them easily cloneable. So this may require some software engineering effort upfront to ensure this. And similarly, it'd be nice if these are hashable. That means that we can store these in something like a hash table with quick retrieval. So in the Sims example, the world state really is focused on modeling actions of the available Sims and also the insistence of values for each of the Sims needs. So this first planning scenario that we're going to consider is going to involve a depth-first search. But we're going to have a maximum depth. So we're only going to a certain depth, meaning that is just a certain number of actions that we're considering. So this is over this depth and is generally a very small value like four for our Sims scenario. So four actions deep, what's the best action to select or the best plan of four actions in a row to select that is going to have the best overall outcome for the Sim? So in the Millington book, he describes an iterative implementation rather than recursion. So this is nice in terms of real-time performance and memory utilization to go with iterative. >> So if we did the basic situation, what we're doing is we're considering first action to take. And then in a depth-first nature considering the next action to add to the chain and so on until we reach our maximum depth. Then backing up and trying each iteration. So going all the way down to say depth of four, then backing up and then working our way back to distance from 3-4 again , considering all possibilities. And so if you do this depth-first search, going all the way down and backtracking and down and progressively till all possible combinations to the maximum depth are considered, then you will find the best sequence of four actions. And if we build upon the algorithm that we looked at previously, we can consider things like the effects of an action in terms of the amount of time it takes and both the positive and negative effects on the insistence values and overall discontentment metric. So we can pick the best sequence overall, at least to this step, but this can be quite expensive. So it's worth considering anything we can do to avoid evaluating all the combinations of actions that are possible. So Millington has a proposed heuristic that involves optimizing the depth-first search expansion by abandoning a particular branch of this depth-first search if a consecutive action results in an increase in discontentment. So what that means is, say you're depth of two in our search. So we are currently considering two actions in a row so far and we're looking to add a third action to get to our total of four. If performing that third action actually results in a discontentment score that's worse than at level 2, then we just can abandon that. It's like we're pruning off parts of the tree that's formed by the depth-first search. Now the downside of this is it means that a good long-term outcome that just happens to involve a short-term increase in discontentment. That it's not going to be found because we're just going to abandon even considering that branch. So this algorithm is big O, n times m to the k. So again, n is the number of goals, m is actions, and k is the number of steps in the sequence. So how deep are we going to consider say, four action sequence would be to the power of 4 for instance? So this is time complexity. So this is same with or without the optimization that Millington describes because this is worst-case performance. And again worst-case would mean, for any of the branches, we might not ever find a case of where we see an increase in discontentment but in practice Millington says that that heuristic worked well for Sims scenario. I saw about 100 fold increase in speed. And he's getting that because we're not searching to some arbitrary huge depth. Instead, these tend to be fairly shallow depth searches, with the depth-first search of say, four, and choice of around 50 actions per stage or that's what the agent has to choose between. This does seem to be a good optimization, especially given the tendency to have a fairly short depth. Millington found it has this added benefit of avoiding short-term discontentment increase that would be inappropriate from a human behavior perspective. This is actually something that's hard to deal with. If you had the non-optimized or the non heuristic version of the GOAP DFS then you might have scenarios where say, the Sim would just accept that they're going to soil their pants and say, oh, that's okay. It can just throw their clothes in the wash and change clothes and then ultimately be happy later. But real people don't act that way and so avoiding those type of strange negative consequences in the middle of a sequence actually can be a good thing, especially when you have these really simplified examples of human behavior that this can pay off. So we've now begun to get to the point where we have some true planning taking place because we're needing to model our world, creating the discretized representation. And we're applying operators to the state. And we are searching through possibilities to find our goal that being the sequence of actions that has the best reduction in discontentment. So we will next consider some even more complicated examples of goal-oriented planning. So we'll stop here and start up again soon.
