>> Hello. In this lecture we're going to look at Q-learning which is a type of reinforcement learning that is applicable to video game, agent design. First of all, let's think about learning for agents in a video game. So we might desire to have our agents get better over time through a learning process and in particular get better at selecting actions. But this can be difficult to do because we'd need to give feedback to the agent or the learning process as to what good actions are. But this could depend on unique human player actions that are taken like an opponent. You can't necessarily anticipate what every player is going to do. Some players might have some very unique strategies and even if you could perhaps anticipate them it might be good to be able to have some learning take place to specialize for whatever human opponent is being faced. Another practical issue is variations in the simulation environment. So like different maps that might be loaded. Some of these may be out of the control of the designer or perhaps the game leverages procedurally generated content. And this could also cause issues in being able to easily evaluate the quality of actions taken by the agent. Another big problem is determining the quality of an action. Can't always be done immediately. If an agent takes a particular action, well, at that moment they may not have achieved any particular success like killing another agent or the player or having acquired a bag of money or whatever rewards are inherent in the rules of the game may be periodic and many actions would be taken by the agent in-between these rewards. So you can have delayed or time-based effects. Maybe setting a trap for an opponent so the agent might set a trap. Upon setting that trap that's the action taken after that they're not doing anything related to the trap anymore but it might have a payoff. Any algorithm you come up with you would want to be able to reward that actions after the fact once it's finally successful. You'd have other time-based effects like maybe poisoning an opponent. Say the agent fires a poison dart and it does a very small amount of initial damage to the opponent like just knocks off one hit point for the initial hit but over time it might reduce the health of the target much more than that. So you have this time-based effect, it pays off over time. You could also have actions that are part of a sequence, one of many actions so collecting reagents to create a healing potion. Say the first ingredient collected is like mushrooms. Well, there's no obvious way to reward that because there is no success achieved by the agent from just having collected mushrooms. It's not until all the ingredients are obtained and the potion created that we could then finally consider the mushroom collection as having contributed to the reward. So there's lots of these circumstances that you would expect to have in a game that have the delayed effect in terms of rewards. So any learning that we do we want to be able to deal with that scenario. Before we get to Q-learning, let's first consider Markovian State. We've previously looked at the multi-armed bandit problem. And that is a Markov decision process that is a single state or stateless. This is based on the Markov property that given the present, the future does not depend on the past. So it's memoryless, the approach of the Markov decision processes. Everything that decision-making is based on is represented in the state. We can maybe define a problem as moving between states. So there's nothing like that that the agent needs to remember or store internally. Simply it can act on the state information. If everything that is needed to make these decisions is within this state representation to make an optimal decision then it is Markovian. So Markov decision process, we have a finite set of states and then for each state we have a finite set of actions that can be taken. For each action that we might take in a given state there is a probability that it will take us from state S to some new state S1. If assuming that transition actually occurs we can associate some reward with that. Furthermore, we have this discount factor and this is used to tie together the reward that might be gained in the future states. So you have reward associated from going from say one state to the next but then also going to that new state there is reward obtained with this discount factor which is blending from the current transition but also future reward. So if we can break up representation of the world into the states and actions then we can imagine we have this ability to optimize and figure out from any given state what is the best action to select probabilistically like what's most likely to give us the most reward considering not just going from the current state to the next but also the chain of states that we could go through. Finding this optimal action we're going to have a process we're going to call the policy. For whatever the current state is, what is the optimal action to take. So we're rewarding, transitioning from any state and so this reward function is based on what states we are transitioning between and so the idea here is that we can give these rewards or feedback to the Markov decision process on what are the best actions to select from. So we want to head towards certain states that are related to success for the agent and these rewards could be negative. So like we could punish rather than reward as well. So we could use both approaches of reward and punishment. >> We have this probability of going from the states and so this is sometimes called a forward model. We're saying that not all actions taken guarantee that we move from one state to the next. So you might choose to perform an action, but selecting it is not always going to be successful for some reason within the details of actually running the game simulation. That might be say, Mario moving across the screen, maybe could bump into something. Maybe an enemy hurts Mario and it causes a failure of being able to move. Now these probabilities like in the context of a game, they're generally defined in a more abstract way. So you're not saying that from a specific coordinate in the map that there is a percentage chance of being able to say go to the right one unit instead, this is generalized. It would be based on any square. What's the possibility of moving to say, to the left or the right? And so you can have these percentages or likelihoods of being able to change between these states. Provided that you have these probabilities, so you have the states and the actions and the probabilities of getting from one state to the next and then you define the rewards, you can analyze all of your states and you can figure out the policy for which is the best to go through. Once you have this information, you can calculate very quickly what is the best action because it's all just table lookups. But the issue that you run into with this process is, what if you don't have that transition function? The probability of getting from state S to state S prime. If you don't have those or don't have a good way to define that, then that makes it difficult to use the Markov decision process. But what if we had a means of learning those transitions? That's where Q-learning comes in. Other than some of the more simple games, you would not be able to easily just engineer what those probabilities should be. If you could learn the transitions, then that would make this much easier to apply to video games. So similarly, with Q-learning, we represent the world as a state machine since the state encodes all the relevant data about the agent environment, just like before. So if it's important for making the decision, it needs to be encoded in the state and if the information is not in the state, then that is not available to be learned. So therefore the agent will just not be able to ever act upon it. One of the nice things about Q-learning is it doesn't need to understand what makes up the state. You just need a way of mapping the game state to the Q-learning state. So this is why Q-learning is referred to as being model-free. You have to have some process that's going to perform this mapping for you. So you can think of it as like a discretization process. You may have games that already have discrete state, like a grid lattice type positioning but if you have say, floating point representations of certain values, then you'd need to discretize those. But the actual Q-learning state, you can just refer to it as an integer. Once you've performed the mapping, all you really need beyond that is just a unique name for it. And that can just be an integer. There's no need to be able to translate back the other way. So the Q-learning is just operating on these identities. You can think of them as simply just being nodes in a graph type of structure. Now with each of the states you have associated actions. So in some games, all actions are always going to be available but in many games you're going to have context based actions. So you can only open a door if you're near a door to open. Might be situation that an agent could run into where there's this context. And so you can probably imagine there's lots of games that do have this contextual action. In order to utilize Q-learning, you have to be able to identify and associate available actions with each of these states, likely discretized states that you create. You also need to be able to give feedback by way of reinforcement function. So this is after an action is taken, you're going to have this metric that determines whether to reward or punish the agent. Now, in many cases you might simply just return zero because nothing interesting happened at that moment but could have positive feedback or negative feedback. So the overall range of values returned, will tend to be from negative one to one. Many cases just returning zero but on the occasions that something important happens in the game that you can associate with success of the agent, you'll return somewhere between zero and one or if there's something bad that happens somewhere between negative one and zero. >> Now there's some flexibility here with the Q-learning approach. So there's no requirement that feedback or the reinforcement function return needs to be the same every time the same action is taken in the same state. But Q-learning can't learn to act on any missing information that may have caused the discrepancy. But at least it'll tolerate it. So if something is, say, you're not encoded in the state, the discretization maybe threw away something that causes the disparity, that's okay, the algorithm will tolerate it, so that's something nice about it. Now you can also have situations where states can change independent of actions taken. So in other words, always taking the same action won't always lead to going into the same state, and so here's a nice just simple example that Millington gives about this situation. So imagine you have a world state that's been encoded and so we'll just have some minimal details for this example, but we'll say, this is like a combat game, there's an enemy near, so we've encoded that so that you can either have an enemy near or enemy gone. In this case, the current state is the enemy is near and the agent can be hidden. So let's say the agent is currently hidden and they take the action of hide to remain hiding, so with the opportunity to make a decision or pick action, they picked hide to continue hiding. And perhaps the next update of the simulation, the enemy is still near, maybe still looking for the agent, and of course, the agent is still hidden. So it would make sense for the agent to choose hide again. So clearly, you wouldn't want this to just continue on forever where there's no progress made, but let's say the enemy decides to leave, so this is a change in state that was beyond the influence of the agent's decision-making. That the selection of action, the enemy leaves based on its own action. But that changes the state. So once you think of an example like this, it becomes obvious that, yes, state can change for reasons outside of the agents, the action the agent takes. So if the enemy leaves, the new state is going to be enemy gone and hidden. Of course, in this change of state, now the agent is likely to pick to do something else. The enemy is no longer around, so it could pick a new action, and it's in a new state, picks a new action, and can do whatever might lead it towards a reward. So just in summary, Q-learning is flexible with this type of uncertainty, either of these cases. So each time there's an opportunity for a decision to be made, we can represent that with this experience tuple. So this is just like a record of what occurred. There is the current state, there is the action taken from the current state, a reinforcement value that's generated, this is the feedback scoring how they did. Again, it could be zero or something else depending on circumstance, and then there's the resulting state. So this information defines the action at any given point that's taken by an agent. So next step we get to the actual learning that takes place with Q-learning. So the agent is going to be learning by managing a set of quality information scores called q-values. These are associated with each possible state and action. So when you're in a given state, you'll have actions, and then for each of the actions within that state, they each have their own q-value. And q-values represent belief in the action associated with the state. Well, this will ultimately provide us with the information that we need for selecting the best action to take. But this leads to an equation that we have. So we can figure out, well what's the Q score for a given state and action selected? And so that is shown here. And this got a few different parameters that allow us to tune the learning effect that takes place. So we have a table of all of these scores, so whenever an action is selected, we can take the previous q-value and we will have a contribution of the previous score plus this updated score. And so we use an Alpha value to control this ratio. So this 1 minus Alpha defines how much of the original score is going to be preserved in the updated value. And then Alpha times this new score which includes the reinforcement value and then also this other bit, with Gamma times max. The Gamma is the discount rate, and so the discount rate that is controlling how much of future states contribute to the score of the current state. And this, the discount rate as a parameter allows us to control how much benefit we get from this future state. So in particular, the max of that Q s prime, a prime that is based on whatever is the best action from the next state. So once that transition from state occurs, well, it's going to have a certain set of actions. One of those actions is going to be the best one based on the ongoing learning that's taken place of having run through Q-learning multiple iterations. And so we want to contribute wherever the best score is to the q-value that led to it. Again, using perhaps the majority of the score is what was there before, but then we're incrementally revising the score based on the reinforcement, plus this discount rate times whatever state is leading to and the value or belief that comes from it. And so we can control how much of the new state contributes by adjusting the discount parameter. >> So as you recall compared to the Markov decision process with Q-learning, we don't need to know the transitions. We're actually actively learning those through the Q-learning algorithm. So in terms of exploration, we have a policy that determines what actions to take in any given state. But the policy isn't trying to strictly just pick the best like the Markov decision process, we actually need to go out and collect data. So from the perspective of Q-learning, the game itself is like a black box, it needs to be sampled from by attempting actions and getting results back. So we need to learn about the quality of the actions in each state. But we also need to take the best actions to further expand the overall best course of action. So if we just explore completely randomly, that will mean that it'll take us a really long time to build up the data that we need it in the Q values in order to reflect these chains of actions that we're trying to identify. So we probably want to use epsilon greedy, perhaps decaying epsilon greedy to control when and how we select actions. So we might tend to want to pick whatever has the best score currently, but we also want to occasionally randomly pick an action that's not appropriate. And so you can see more discussion of epsilon greedy strategy in the multi-armed bandit lecture. And this exploration, we might want to pick different strategies for exploration depending on whether we're doing offline learning, meaning we might be running the simulation of the game but that is during the development process. So there's not a human player that currently we're trying to entertain. So that would be offline, so we could perhaps afford to be performing more exploration. Whereas if it's online, you will want to really limit how often you're randomly selecting an action. Because anytime you do that, there's a chance that you're going to get some really odd ball behavior from the agent that's going to confuse the player or frustrate them because it might appear as a bug in the game. Why did the agent do such a silly thing? So you have to really consider what the scenario is in terms of how you tune the exploration. So the algorithm involves really pretty simple, at least from a high level, is you initialize key values to some value. And then you're going to choose an action from whatever the current state is. Again, recall that we have some external process that's always figuring out what state we are in. And this is at least if we're running it with a simulation, we will determine our current state by the discretization process. But then we're going to have to choose an action that's associated or available in the state. And so we would use the epsilon greedy strategy here. So we might pick greedily or perhaps an exploratory selection of action. So you perform the action, then you've received the reward that may or may not be an immediate payoff, you might get a value of zero. But then you're going to update Q and that will perhaps be chained together the score with whatever the next state is based on that discount factor. You're now in a new state and can pick a new action again. And you can repeat this process over and over. And you're probably, at least in the simple case, you just have a 2D table. So you just buy index of the state. You can look up the row and then the columns, define the different actions that are available to you. And so when you perform the calculation, updated Q value calculation, that's what you're updating, is whatever the value is stored in the table. So pretty straightforward. In terms of performance will have big O of i in time where i is the number of iterations of learning. Then in terms of memory, because of that table, we just saw that is the number of states times the number of actions. And of course, these tables can get rather large. And it really depends a lot on how effectively we can create this discretized representation of the world. Keeping in mind that every bit of information needed to make an optimal decision needs to be encoded in the state. Otherwise, you're going to get less than optimal behavior. Now, it might be a worthwhile trade off to have agents that don't perform completely optimally, but that are still suitable for a video game. So there's perhaps some room to work in terms of the design of the discretization process. >> Millington has some recommendations for the parameters with the Q-learning, so if you have the book, you might want to actually read the whole section, but just some highlights where he discusses this learning rate of 0.3 is his recommendation if you're using a fixed learning rate. But if you are going to, you might vary this according to a schedule based on the number of iterations. So you might start off an earlier iterations where the learning rate is really high of 0.7 but then map that down as a function of I to 0.1. But if you're not going to do that, just use a fixed value of 0.3. Discount rate of 0.75 was his recommendation, nd say has a bit of discussion about what is favorable depending on how long of sequences that you might desire. I believe the 0.75 can get you sequences of around and by sequences, I mean action selected to move from state to state, so that would be around 10 actions total, but he has this recommendation of 0.75. Higher values favor longer sequences of actions, but it's going to take longer to learn, but if you have lower values, you're going to stabilize more quickly but only have like some cat, shorter number of sequences. Randomness for exploration. This is really important to set according to whether you're online or offline learning. So again online meaning live during the game play with your target audience playing the game, and offline is just if you can train separately and then the resulting table is what you ship with the game. So online value of 0.1 is about the maximum that you can go with, and that is, again, to try to minimize the agent doing really dumb things that seem odd or detrimental to the gameplay experience. And then offline is only a little bit higher for the exploration, so 0.2 is Millington's recommendation. Again, what I discussed previously is that even if it's offline, you still need to visit the favorable states because you want to be able to learn the chains of states or the chains of actions that lead from state to state for the purpose of the backpropagation of the key scores. Millington also has a bit of discussion about rewards, and so there's obvious scenarios like if the game can be won, so you'd give a reward of 1.0. You need to be however, cautious about incremental rewards. The game designer or the AI designer might perceive some actions or moving from one state to the next as being progress towards the goal, it might seem obvious to the designer to reward that. However, when you create these incremental rewards, you can create scenarios where there's like a local maxima and the agent will end up in these circumstances with Q-learning, they'll just go in a loop obtaining these small rewards. So they might pick a sequence of actions that lead in states such that they come back again to that same reward that has been configured, and they'll never even discover that there's say a path to the winning the game with the 1.0 that might just keep picking up a 0.1 score over and over. So Millington recommends you start only rewarding success or preferably the one way to be successful and then only add in other solutions or small rewards if the learning is taking too long or you're just getting poor results. Maybe the agents just never even able to discover the appropriate course of actions that ever lead to winning the game, for instance. So to summarize, Q-learning, so it's a very general decision-making technique, is model-free, you don't have to do much in the way of engineering software that's specific to your game other than just the mapping of the game, the discretization of game state into the Q-learning state. Identifying the actions that are available at each of these states, and also defining the reward. Of course, the tuning of the different parameters is also important. But otherwise, you have this very general algorithm and an approach that can be used. Downsides however, though, you have this massive potential for state-space size, and that can also lead to a long time to learn. When you do have this large state space, it can take a good bit of training, and it's really most appropriate for offline learning. I believe Millington mentions, like an order of magnitude is reduction in the number of states is really practical if you're going to have online live learning within the game itself. So Q-learning is, I think, an interesting approach in that maybe has some potential that if you're interested in more discussion Millington has a few other examples that he goes into but I think one thing he suggests is not using it for complete agent control, but using for some subset of the decision-making of an agent. But definitely something a strategy that's worth considering, especially if you value learning in your agent.
