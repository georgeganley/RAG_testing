>> Hello. In this lecture, we're going to look at perception and fairness, especially in regards to AI solutions in games. So this issue of fairness is something that comes up a lot in video games. Game players are easily frustrated if they don't perceive the game or some aspect of the game as being fair. And this often comes up when considering interactions with agents. These agents will often be like enemies or opponents of some sort in the game. One classic example of I guess some dispute over fairness is the game Mario Kart. The original and the successors all have a handicapping system built-in to the game. One aspect of this is the likelihood of collecting certain power-ups. So if you're well in the lead, you're going to probably only get inferior power-ups and these augment your ability to raise speed up or attack the other racers by throwing a shell at them or that thing. If you're in the lead, you're probably only going to get banana peels which have limited utility in terms of gameplay. If you're further behind though, you are likely to get power-ups that allow you to directly attack the players in front of you and cause them to wipe out or slow down and then you get the opportunity to catch back up. That is something that's fully in control of the game designers in terms of the rules implementation, but you can also have enemies that are perceived as being unfair. Maybe they are just too good, or in the case of perception, seem to be aware of more than they should be allowed. In terms of fairness, you will often see disputes about fairness that arise from game design or the game rules. And so that's something that the game designer has some pretty straightforward control over. But there's also issues related to agent ability and perception. Now, some aspects of agent ability are they have the potential to be easy to control. So for instance, you can make an enemy move slower, and then the player can respond better according to the slower movement or you can reduce the frequency of attacks or the power, like how much damage the attacks do. But what's harder to adjust is perception, and this can be something that is especially frustrating in games, especially certain types of games. So for instance, you might think of a game with stealth involved. If you're going to have a game with stealth, then you're really putting a significant burden on the ability to design a realistic perception for your characters. So we're going to look at some, at a higher-level, look at some aspects of perception implementation. Just in relation to artificial intelligence, perception modeling is something that is fundamental to some domains, especially robotics. So you need to be able to get your robot to be aware of the environment. So maybe camera systems or LiDAR, Sonar, other means of making measurements from the environment, and then from that being able to build a model of the world and form some task. And so there's a lot of effort that needs to go into making functional robotics solutions. But in video games, the agent has access to all the information you might need already in the simulation and it's often already in a discretized, easy to work with forms. For instance, if you want to know where the player is and meaning that the game object that the player controls, well, that is a discrete entity typically maybe like an object oriented design. You've got a memory reference to all of the relevant information for the player controlled game object that would include things like position, velocity, and so on. And if you recall from our discussion of agent movement, we know that that is really all you need to begin getting an agent moving towards the player. So this doesn't take into account any aspects of perception, we just immediately have access to this information. And this can be problematic if the player is expecting the ability to hide, for instance, just directly grabbing a memory address for the player position does not take that into account. Again, we have all this information about game state. In order to accurately model or pretend that the agent has perception, we're going to have to hold back some information, but that holding back, that's not necessarily something that's straightforward to do. We have to figure out from the context that the agent currently has, what information should be available to it. So a lot of this will be based on spatial relationships. And so this is where a lot of the complexity can come into play. So in terms of simple game design, you often will just have all of the agents just immediately aware of the player. You can think of something like Pac-Man. That's such a simple game that it's not really a detriment to the gameplay that all the ghosts know where Pac-Man is and they can make decisions on how to move closer. So there's a bit of level design to simplify things. And the simple single screen worlds, so the ghost can basically just move in the closest up, down, or left, right direction towards Pac-Man depending on what movie is available and they can get closer. They each ghosts has some slightly unique behavior as well. But that is something that the player easily accepts that all the ghosts will move. But if you had, let's say a game like a first-person shooter that has an extensive level, you would not want all the enemy agents to immediately move towards the player. It would just overwhelm the player. If you add every single enemy in the level, move towards the player, especially if they had the means to get there effectively with say, path planning. So you can control a little bit of expectation through some simple solutions like just having deactivated agents until a certain moment like the player comes into view, or perhaps a trigger zone is entered to have a simple depiction here showing where the player might walk into some zone. So its just a simple geometric calculation to check if the player position is inside of a region. Once that occurs, that generates an event and it either spawns or activates an enemy. And so you can have multiple of these trigger type scenarios to bring new enemy encounters into play. This is a very simple scripting solution that addresses this issue of perception and allows us to control, schedule these encounters with enemies according to player progress. It also of course, has the benefit of, let's say your game can only maintain an interactive and enjoyable frame rate. Let's say you have a maximum of five enemies on the screen at any given time. So this would be the thing you might consider doing to maintain that level of quality. But if you have, say, a stealth game, then you probably want to have enemies that are on screen that the player is interacting with to a degree but that are not aware of the player, don't indicate that they are aware. This thing can be frustrating. Anytime you have a more sophisticated gameplay, especially players are already familiar with games that have stealth elements to them. If you have a game that has similar like 3D graphics, fidelity and character movement, they might expect it to be able to be stealthy in some way. And so there's some obvious cues that might lead to these expectations, such as the enemies, their visual presentation to the player. Maybe if they're humanoid, they have a head that can face different directions, maybe eyes in the front of their head. And so it's clear that there's a backside that would be presumably undetected. The player would not be able to be detected if the player is behind the character or the agent and not making any sounds or otherwise would be detectable. So if you just have enemies always aware of where the player is when they have spawned in or they're active, that could easily be frustrating. And a source of this feeling by the player that the game is not fair. So maybe this would be an opportunity to model perception. We could consider, for instance whether the agent can see the player and it's figured out a way to model that. So again, just to recap, from a development perspective is easiest just to have your agent access whatever stimulation state that they need for making their decisions but that can easily introduced this notion of a lack of fairness in the game. And so because of that, we want to filter information according to these perceptual abilities. So the first simple solution when we might consider is to have a limited sight range. And so we can just do a distance calculation. We could even use squared distance compared to a threshold squared distance between enemy and player. So that relative position vector. We can take the magnitude squared magnitude of it, compare it to a, whatever our threshold distances squared. And then we can figure out whether the player can be seen just by the distance. And so this immediately we can get some improvement in gameplay especially if we have larger open or more open areas but that's still not going to fully enable our stealth scenario that we've considered so far. So the next thing we might consider is maybe introducing an angle relative to the agents head. So if we have a 3D model for our character, the agent character, we know where the head is looking. And so that forward direction. We can do a relative angle comparison. We can calculate the angle from the enemy to the player relative to the forward direction of the agent. And if it's within a threshold, then we know that the player is within the limited sight cone. So we would perhaps have two tests. We would have one as the distance test. Is the player close enough to be seeing. Furthermore, is the player near enough in terms of an angle to where the eyes are looking. Again, this is all modeled, but this will add a lot without a whole lot in terms of calculation. This will add a lot in terms of the ability to support this notion of perception and then improve the perception of fairness by the human player. [BACKGROUND] Next, we've got a scenario where we have a much more challenging scenario. And that is where there might be an object. So the player could be within the the distance that the player, the agent can see. So the player is close enough that the AI agent can see them also could be within the angle to be seen. So the forward direction of the head but maybe there is an object in the way hidden around the corner of a building or there's a rock, big boulder that the player is hiding behind. So in this case, things get a bit more challenging. We need to check to see if the player is occluded. Simple thing we could do is just re-cast from the enemy's head to wherever the player is. And if there is a hit against game objects the big boulder or wall, then we will just decide that the player is hidden. Now of course, this could be insufficient to really accurately make this determination. What if the player for instance, is hiding behind a countertop but they're standing up? They're not squatting down, but they're only the lower half of the body is hidden. Well, if you do a single ray cast, depending on where you're aiming that ray cast, it might hit the countertop but in reality, the player should be in view. This can cause some strange behavior. Solutions would be, well you could add more ray cast, may be cast the rate the player's head, maybe at their torso, maybe at their feet depending on the gameplay. Of course, the more ray cast you add the more computational resources that you are going to take up. You might consider a more elaborate solution. It is theoretically possible that you could actually perform some rendering from the enemy perspective. And if you were to do so, you might use an approach of using simplified geometry. And this would possibly be available from the physics simulation that's already taking place. Generally with physics simulation you have geometry that is similar to what you see visually, but much more course in terms of how it describes the environment. So maybe say a wall with some intricate visual pattern on it would just be approximated with a large flat surface. The characters, like the player, may be approximated with a vertically oriented capsule collider, rather than all the intricacies of the character mesh. You can have this simplified geometry for much of the physics simulation but what you might do for in terms of like lower resource cost is perform a rendering using that simple physics geometry. And you would be rendering from the perspective of the player. Again, this is all off screen. The game player is not seeing this. This is just to facilitate the occlusion check but you perform this rendering and everything could be just white, this being rendered except for one object and that being the player, maybe you'd render it and say like bright red. And so if any bright red pixels show up on the screen when everything's done rendering from that perspective, then you know that the player is not fully hidden. You might have it based on some ratio if there's more than five red pixels on the screen, that means the player is not well hidden. And then that can be the trigger but of course, this is much more computationally expensive if you're going to use this rasterization rendering approach to do so. So it says clearly a challenge to deal with, most likely you would be using re-cast. One other thing that you might do is not shown on the slide here. You might have some precomputed hiding spots. So that maybe if the player is in a hiding spot, maybe they have to be crouched for the state to count as hiding. And then maybe if the enemy is in some known discretized space, then they just simply can't see. So that way you might be able to do some caching without having to make runtime calculations. So you could have this old bay. By the way I think limit what you're doing in terms of gameplay or in level design would be restricted. Now another issue that comes into play, especially with stealth based games is attention. Let's say that's like a guard in a stealth game. And you often see this in commercial games that have that stealth genre gameplay is that they might have a neutral state. There perhaps on duty, but they're not highly engaged, they're bored. All they're doing is serving as a lookout, maybe going on a patrol. But otherwise, they're not expecting anything, they're not reacting to the player. And so if they become aware of the player or something the player has done or some undesirable thing like they hearing gunshots or the alarm goes off, then they enter a heightened state of alertness or attention. And this point, as you might expect in real life, similar scenarios is that if someone is aware of a problem, their perception is probably going to be amplified. They're going to really focus on the environment around them, find anything out of place, be able to more easily detect that there are intruders. And so this can work well in gameplay and as soon as say an enemy agent is aware of something like they find a dead body or they actually see the player then their attention level can change. So even if the player runs away and hide that attention level can stay higher and we could model this change and attention simply by attenuating, let me go back some slides here. Say are limited sight cone angle. Well, we could maybe increase the angle of the cone or the site radius could increase, and so some of the other perception type things that we might consider. We could also just change the thresholds according to this change in attention. And then maybe over time for the purpose of gameplay will dial back down attention. So the enemy agents might be at a high level of alert or attentiveness to the environment. And then eventually they say, oh well, I guess the intruder has left and they go back to what they were doing before. So you can hopefully see how you might be able to model that simply by adjusting these coefficients on these tests we've talked about so far. We could improve upon our visual perception, and this would also be something that fits well with attention, is to have foveal and peripheral vision. Fovea is just like your center vision. That's like in real life, if you aim your head and your eyes at a target, well, you can scrutinize the details of whatever you're looking at. But things in the side vision you often aren't fully aware of unless something major happens, so your peripheral vision is very good at detecting motion but it does not see details. So high-contrast visual changes, your peripheral vision will pick up. Low contrast slow movements, you might not even notice at all, especially depending on the context of where you're at. If there's other things moving around, like maybe say trees with their branches swaying in the wind and leaves moving a little bit, then you might not notice someone's sneaking around in the woods unless you were directly looking at them. This is something that can work well in stealth games and help with modeling. You can model this as simply an additional angle test and maybe have it based on the character movement. So maybe if the player is within the peripheral zone but standing still, they don't get picked up. But if they're moving faster than a certain speed, they would get picked up. So you can introduce some a bit of softness, I guess, to a transition zone as the player becomes more and more likely to be detected by the enemy based on this angle. Another important aspect of perception is that like the light level or the contrast detection of the agent. In stealth games, you often will need a place to hide that doesn't involve full occlusion like hiding behind an object. Instead, you might want to just be able to hide in the dark. So the classic game thief had this ability so you could hide in the shadows away from the the lights that were around in the environment. And as a game player, you would look for these dark areas and hide in. And then there's also a little hint on the heads-up display that would tell you your light levels. It's like a traffic light. So if you're fully in the dark, it would turn green. If you are partially in the darker, it turn yellow. And then if you were out in the open, it would turn red. Because since you're playing in a first-person perspective, it was hard to tell how you might be illuminated. But the enemy agents would not be able to perceive if you were standing still in the dark. If you were partially in the dark and you had the yellow light, maybe the enemy agent would not be able to see you unless they got close enough or we're looking directly at you. So this as you might imagine, would be something that's challenging to compute. And so it's often sort of thing where you precompute it. As a game designer, developer, you might perform a baking process or pre-processing where the level is analyzed and you identify different regions. There's some sort of discretization, maybe breaking the world up into a grid or voxels. And then testing as if the player was occupying these different spaces. So if there was an object within these areas or volumes, you could do a calculation to determine whether there say, a line of sight from a point light source. And if it's close enough, according to the falloff of the brightness of the light with distance. And you can make a calculation and store in each of these zones or these discretized areas. What is the elimination? And then that can be used for calculating whether the player is sufficiently in the dark. So this works well because many games already do a baking process, pre calculating with a radiosity based model for how light reflects in the environment, say off of walls or other surfaces relative to various light sources, so generates a shadow map in the environment. Now this shadow maps only on surfaces. So in order to facilitate this recomputing these hiding zones, there's additional steps that needed to take place. But if you do this, then you can at least figure out where the player could be, where they would be, say, within a certain dark enough area. So the idea in gameplay is you have a cache of all of these different brightness values. The player will be occupying one or more of these areas, and you take the max elimination from what is stored in this database. You take the maximum because you might have, say, most of the players hidden but their heads sticking out right into full illumination, so you would want to be able to account for that in terms of gameplay. Otherwise, things won't be very realistic. For camouflage, you might in this case, rather than any automated analysis, which I'm sure could be automated, but probably more likely you would have manually assigned values by designers. So for instance, bushes or tall grass. You might just have whole zones or areas that have been labeled by the designers that mark the type. And then the camouflage potential of these different areas can just be done is a simple comparison. You know where the player is, what zone they're in, and then you might just say they're well hidden or not, or maybe it depends on the clothing they're wearing so you just make the appropriate comparison with some Boolean logic. You can support those sorts of situations. Now in Millingtons' book, he describes an approach that he used in a game that involved a variety of different sensing abilities beyond just the vision, also including hearing, but also smell, which I think is interesting. Not too many games have used smell and those that have, often have done it in a very constrained way where it really ends up being more like a visual. You toggle a button and then you see a special visual display and you see these lingering smells. But it doesn't really account for the propagation of how smells move around in the actual detection. But in this game, that Millington describes at least the technology for, I don't believe it was ever released. But he has a sense graph and it's used for all the different senses. Basically the same way that you might support path planning by discretizing the world, Millington describes breaking the world up into these finite element model of the game world. And then you have a graph that defines the connectivity between these different elements. And so you have different information that can pass through this graph. In terms of gameplay, you determine, well, where are the different agents and the player, but also objects that they can generate the sensed information. So you could say, have a sound generated at node A, which we can maybe think of as room A, as shown in this figure from Millington's book. So you have a sound say generated there and the player is in location C, which is like a hallway. So if a sound is generated in A, well, we can just parse this graph. The sound is emitted in A and then it will traverse from A to B. But the act of traversing is going to attenuate the sound. That means that the sound's going to get softer. And so we can have pre-computed distances according to these different rooms and connections. We know the distance and we know how based on our model of sound, how much the volume will drop. And then we did the same thing with that information from B to C. So once the sound reaches C, well, it's dropped off in volume at certain amount. And so we can use that information to make a determination. Similar things can be done with vision, except the traversal is immediate. And you would say, model the attenuation differently, but according to, you could see much further distances. So it'd be much more gradual attenuation. And things like smells will be unique in that whatever generated the smell maybe is no longer around. But that smell can linger. And according to our simulation update, not only will we have an attenuation based on distance, but the sound will propagate, expand, and move out and then dissipate. So we can support these different types of senses, primarily vision, sound, and in this case also smell. But this can be useful for more elaborate gameplay with agents that engage in different types of senses. And this is also fairly performant. Rather than more elaborate modeling that would be computationally intensive, this can work pretty well. One nice thing from the vision aspect of this is, you can imagine that if we were using the ray casting solution that we talked about before, you could quickly be overwhelmed in your ray casting if you're comparing against all game objects in the world or all geometry in the world. But by generating this graph, if we know we're through a precomputing process, what can be seen from where. So if you know, let's say there is a room D not shown on this figure that room A cannot see. So maybe the room directly on the North wall of room A cannot see D at all. So if the player is in A and the agent is in D, even though the player generates a sight event. The agent will never see it because the propagation of vision is based on line of sight. Hearing, however, it is possible for a really loud sound in A wood maybe propagate and bounce around from node a to B to C. And then C could propagate onto D. So let's say a loud explosion or knocking over a bookcase. We could have that potentially be within the perception range of the agent. So this allows us to limit the propagation of events that each agent would maybe need to have processing of. So we're limiting the amount of computation that's necessary for what's going on. I mentioned hearing already in the context of the sensory network concept. But one important thing in terms of gameplay with hearing, is that we don't really want to model the actual sounds being generated and process that signal to determine if the agent can hear it or not. So think a little bit about the importance of hearing to a human. So outside of speech communication, what do we use hearing for? Well, they're really probably the most important aspect of hearing is that it brings attention to the listener about events that are maybe useful, especially from a survival context. So you might imagine cavemen from long ago, lots of dangerous things in the world. And if you want to hear the tiger sneaking up on you, it's probably important to be able to hear a twig snap in the woods. And then from that, that doesn't tell you a lot of information. We really need that visual sense, but what hearing is good at doing is we're able to figure out where the sound came from pretty effectively at least on a course level, like maybe within 10 degrees. So we hear a sound, we turn her head and then we can take a look at what's there. Is it a tiger or is it just a bird, squirrel, something like that. So we can quickly make that determination. The things that we respond to are very short duration events. Again, outside of communication. We're really not interested in doing some elaborate processing of everything that is heard. Instead, we're just interested what creates these different events. And, well, it turns out in gameplay, software engineering, well, you can have an event system. And this is common in games for things that generate sounds. In many cases, you will have an event system generating software events. And then there's an audio engine that interprets these events and play the appropriate sound. It might do other things as well, like generate a visual effect like a particle system. Maybe a little cloud of dust, say there's an impact and event is generated on the impact from the physics simulation. Software Engineering for the game leads to an event generated that plays a sound of the impact. Also, anything else that is important for the game experience within the agents like the enemy, agents can listen for those events. And the events it's not the actual audio, of course, is the message that describes the need for the audio. And then that is what can be interpreted. So that's a discrete thing. The agent can consume this event and maybe there's information in the event message that can be interpreted like what sound it is. The position in the world. You can make distance determination from that to decide based on the distance and the type of sound, maybe also the amount of energy it hit with, you perhaps have some attenuation factors for the sound. So for instance in 3D sound engines, you have roll-off factors. So a near and far distance. And this is used for interpolating the attenuation and so the sound card can interpret that, as well as what's the appropriate volume to play the sound at. And that same information can be of course valuable and easily interpretable by the agent without doing any direct sound signal processing. Now, that said, you might want to do some pre-processing of sounds could be useful. Human hearing, at least I imagined most animals, they do not hear all frequencies at the same intensity. So this is the perception of loudness or volume of the sound. So for instance, extremely low frequencies are extremely high frequencies. Humans don't hear especially well if they can hear them at all. But right around the sweet spot, they hear quite well. So you might actually perform a signal processing analysis of sounds to determine well how likely these are to be heard, maybe from a set of sounds and sound banks or maybe you have footsteps, twigs breaking, gunshots, and so on. And how sensitive would a human be. And this could then be used to generate precomputed threshold for how likely the agent is to notice something. And then you come up with a function that takes into account the distance, the attenuation that would occur according to these distances, and what the sound was through this analysis. But this could also easily be done by hand too, you could just say, ''Okay, I'm going to label this sound as highly attentive sound and these as more easy to miss.'' And then the agent will just make these threshold determinations through a simple table lookup. So again, with hearing, you're not actually doing signal processing of the sounds. You just look at the game events that occur that generate the sounds. And that's what is interpreted for the purpose of modeling perception of hearing. Next up, we'll look at projectiles and fairness. So we're thinking in the context of agents that shoot things at the player. And so this is easily an area where you can do too good of a job. So we've already seen that the calculations for aiming, and of course, we can have highly effective calculations that are predictive based on extrapolating expected movement of the target, say the player moving. And we can really have an agent implemented that's a perfect shot and does a great job of hitting the player well beyond any human ability. So because of the impact on fairness being beyond human ability, we might want to introduce model aiming error to make it more human-like. We might consider the actual aiming action especially related to the amount of time it takes the agent to aim a weapon. And also reaction time that is an important factor in both the aiming error and the aiming action. One way that we might think about this modeling human error is to consider the typical distribution of some activity like shooting a gun at a bulls-eye over and over. Well, this turns out to be a Gaussian or a normal distribution. It'll vary from person to person, but you'll tend to have [NOISE] an average point that the user will hit. It'll either be right on the bulls-eye or a fixed offset from the bulls-eye. But then there'll be an error distribution around that average point defined by a standard deviation. And so that would be a good way probably to model the aiming ability of an agent. We could just model a Gaussian distribution. So once we've determined where the exact target is that should be aimed at, we'll say, okay, we're going to back off of a perfect shot. Now, we figured out where the perfect shot is, assuming our prediction is correct if this is a ballistic trajectory, could be an instantaneous weapon in which case it's guaranteed to be a perfect shot in that case. So very likely we will want to dial things back in terms of the agent's ability just so that the player has a fighting chance. And so the way we could do this is say, all right, we want a distribution around the perfect shot according to a standard deviation that we will provide and maybe determine what's appropriate empirically through testing what we want that to be. So we need some way to calculate Gaussian distribution. I don't have the derivation of the distribution shown here. But it is expensive computation. So for the purpose of real-time gameplay, we want an efficient implementation. What I've shown here is a simple one, this box Muller equation. That is the little block of code shown in the middle of the slide. The code here is pretty straight forward though it does have some expensive operations involved. So we generate two random values. Then we need to calculate a log base 2 of one of the values and a sine of the other. And then take the square root of the product along with some other constants. And so once we do that, that allows us to generate as random Gaussian. And you can imagine it if we are doing this in a game that's a pretty expensive calculation. There is a faster algorithm not shown here, the Ziggurat, I'm not sure about the pronunciation there, but Ziggurat algorithm has a few little issues in terms of implementation, but beyond the scope of this lecture. But if you're interested in the best possible real-time implementation, that's probably a more desirable approach. But another thing you can do is if you're not really concerned about being especially precise and you really just want to have some randomness distributed around the correct value, then you might just simply use, we've seen this approach before. I think Mellington calls it a binomial distribution, but it's more a fake binomial distribution where you generate a random value minus another random value. And so the little snippet of code I show at the bottom of the screen here is how you could apply that to creating a random error distribution around a central point. So in this case, the mean, that would be the exact value plus standard deviation, which is not really a standard deviation, is more an error range, in this case. Times the first random minus the second random, all in parentheses. And that'll give you something that looks more like a pyramid rather than a nice smooth Gaussian or normal distribution. One thing I wanted to mention about this, is this is perhaps one of the cues that a cheap detection system looks for. If you play any multiplayer games at all, you're probably familiar with cheap finding, especially those cheaters that are using an aimbot. One thing that you might look for, I'm guessing it's probably nowadays implemented as a machine-learning thing. But a way that you would discriminate between an aimbot and a human would be perhaps looking at the accuracy of the shots. Just one of many things that could be considered and you in fact might do an analysis of the air distribution of shots over time. So if you're going to artificially generate noise, if you're doing something like the fake binomial of Mellington, that's probably going to stand out compared to a Gaussian distribution. Similarly, if you had a Gaussian distribution that had such a tiny standard deviation that it was just not comparable to any human ever seen ever, then that would maybe be another trigger. And there might be other types of analysis looking for the nature of how the random values are distributed and could identify that you're using a pseudo-random number generator. Hopefully, no one's inspired to write software for the purpose of cheating in games, but just maybe something to be aware of, could help inform the development of the actual effective cheap detection. The next thing to consider is human-like aim movement. In human-computer action, there is this law called Fitts' Law. And this is based on studies of humans performing these aiming movements. And they initially started, it was in one-dimension, turning a knob, I believe. And the knob moved a little visual indicator, like on an old style radio. You'd move the little visual indicator like a little tiny arrow along a line and try to land inside of a target region. The original studies would randomize the initial position of the arrow and the landing region distance and size relative to the little arrow that you're moving. This study found that the further away the arrow is from the target region, resulted in the user participant in a study taking longer to move that position. Even if they could speed up how fast they turn the knob or slow down if the head, this dynamic control. It'll still take them longer. Furthermore, the smaller the target area was, the longer it would take. If you had, say, the arrow very far away from a very large target that could be gotten to the terminating condition of the study for that trial very quickly. But if you had the arrow faraway from a tiny target, it would take longer. And if you break down the behavior of this taking place, the human strategy on average is to make a gross or a large movement to get into the neighborhood so that the individual would quickly move the arrow. But there'll be a lot of arrow because they're moving so fast. Then there would be a brief moment where they stopped the move and visually assess, what's the difference of the arrow position and the target, and then they'd make a refining move and another refining move. You think of it as in the game of golf, you hit with your driver, very far distance try to get it as close to the hole as possible. And then you switch to a club that has more accuracy. And then you go back and forth that to the point down to a patter. And you're always cutting the distance down into a fraction of what it was before, provided you're decent at what you're doing. And so that tends to be true. Also the initial study was in that one day with a dial. It tends to be true in all aiming exercises, the coefficients might change, but the fundamental relationship between these terms tends to be pretty consistent. If you're aiming a mouse like a 2D movements, trying to move a mouse cursor to a button or changing an angle. The bigger the angle and the smaller the target zone is, leads to more time that it takes to do so. Again, that large movements and then switching to progressively more refinements. It might be something that is hard to see, if you're moving a mouse cursor trying to land on a location, you'll have all of this done in probably a fraction of a second, for many use cases, unless the unit button is super tiny. But this certainly can come into play for moving and aiming radical in a video game to try to aim and shoot a target. So a human when they see an enemy like our first person shooter, and they're moving the joystick or their mouse to aim. They're going to have a big movement and then they're going to have some refining movements. Now this is all compounded by the fact that the targets tend to be moving, complicating the scenario. But you might use this notion of Fitts' Law to inform what would be fair and appropriate movement of of your agents. So when they're aware of the, say the target, maybe the human player, the agents fighting against the player and needs to aim and shoot it the player. Maybe the initial, upon initially seeing the player, the movement of the gun will be initially a large movement. Either the agent is shooting and that there's error introduced from just the fact that the movement was not perfect. The error distribution could be introduced as part of these iterative refinements, will each have some error applied to what would be the perfect movement? Some error that's maybe informed by Fitts Law. You can tune this all such that, the agent can dial in their aim about the same amount of time as a human and maybe slightly tweak that for an awesome human versus not so awesome human. So you could have a range of tuning parameters for the amount of error, that's introduces this refinement. Again, this might be the same thing that cheat detection could detect. It looks for a lack or an absent of those iterative refinement movements and the error distribution of the aiming movements. One other thing that you might want to check out for more elaborate Aim movement and maybe other types of movement as well, is GOMS, which also comes from HCI research. This stands for Goals, Operators, Methods, and Selections, but it is a predictive model for human interactions with artifacts, it's as a little bit more general than Fitts Law. So maybe if you're working with knobs and dials or other activities, so it's designed to be predictive of human activity. But if you're trying to make an agent act human-like, you might perform a task analysis using GOMS and then use that to model what you would expect to perform. It's probably good getting a bit overkill to go that route, but perhaps something to look at. Lastly, reaction time. And we could tie reaction time into the iterative aiming refinement in light of Fitts Law by also coupling reaction time into the equation. And so you may be aware that humans can only respond to something they see visually after some period of time. This is around 0.2 seconds for a visual stimulus, if it's expected, takes potentially a lot longer if it's totally unexpected, to respond. Auditory stimulus like hearing something, you can respond a little bit more quickly. That's why they use a starter pistol in races. The quickest response. If you're racing a car in a drag race, they have the lights that light up that tell you when you're allowed to go. And so it's actually there's detection to see if the car is moved forward too early. That's based on what's expected the best human could possibly do. If the car moves, actually, it's not based on when the light first turns green, it's a delay based on what's the expected best a human could ever conceivably do. So it's probably slightly less than 0.2 seconds. And that if you go more quickly than that, then that means that you cheated basically, and so you get a fault and it won't count for that case. How does this relate to creating fair agents? Well, we might not want to have our agents respond too quickly to what they see. So let's say the player runs pass a doorway. You might have an agent that can immediately recognize the player has gone by the doorway and get a shot off right away. So that would be not really realistic unless they were already engaged with the player. And even if so, probably chances of responding, just wouldn't be likely based on reaction time. You might want to model reaction time. One thing you could do is just introduce a delay on any observation. Whether that's responding to player position, presence and position, sounds, that thing. You could maybe just have a circular buffer to delay estimates. So you sample, as a simulation advances, you're storing your sampling all of these important measurements. But the agent doesn't immediately get to respond to them instead has to wait until it's provided with the information from this delay buffer. It's always working with the information from the delay buffer. And in terms of shooting, well humans, they learned to account for the delays in their own sensory system, so they learn how to lead a target just through practice. Like if they're firing a weapon they're leading a target, not only based on the time that it takes for the projectile to reach the target, but they also learn how to deal with the delays in their own sensory system. And where they're aiming is based on, of course, this expected future position. We've talked about prediction before. We could actually use the current velocity, of the target. Maybe introducing some error to say that there's an error in the estimation of velocity not have a perfect value. So you consider how we're going to introduce error into various aspects of agent decision-making for the purpose of being more realistic. But you could aim according to that. And so you always are assuming this 0.2 second delay. But the agent is not always working with old data because they can make a prediction. So simple case is that you just use the current velocity and extrapolate by 0.2 seconds. And so that's what the agent works with, again, possibly with some error either to the velocity estimate or the amount of future time prediction, something like that. You might possibly also use a Kalman filter, which I'm not going to cover in any depth. But just at a high level, this has been used in robotics and other types of sensing for predicting based on old or since their measurements that might have error themselves, that could possibly be useful. Overall, this delay might be overkill. You can certainly get reasonably realistic and fair behavior from an agent through adjusting other other aspects of their perception. So, how far can they see, how well did they aim, that might be sufficient for creating a realistic scenario. We're talking about a pretty small value here, this 0.2 seconds. And so the impact of that in terms of the game players perception of fairness, they might not even realize that's the contributing factor. So you might be able to account for it in other ways and therefore simplify your implementation. So that closes out our look at fairness and in particular, how modelling perception of our agent plays into fairness or the perception of fairness in gameplay.
