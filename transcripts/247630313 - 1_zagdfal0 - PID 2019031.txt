>> Hello. Today's topic, we're going to look at coordinated movements. So this is going to be an extension of looking at steering behaviors. We're now going to consider other ways that agents can work amongst other agents. So beyond just avoiding other agents in a very independent strategy like we saw before with the open steer examples, especially, so now we're going to have scenarios where we have agents that are moving in some level of coordination. And there's different levels of coordination which we will checkout. This is somewhat more difficult than where you'd just have an NPC, a single agent. So there's different issues that we will run into that get complicated once you do have this coordination involved. Certainly there's the shuffling around of these agents in close quarters. But what if there's an obstacle in the way? So we've talked a little bit about what to do, both in terms of steering behaviors and path planning for the single agent scenario. But what happens when you've now got a big group? How can you actually coordinate that so that the movement is efficient? But also we have computational cost issues to be concerned about. If you've got a large group of say, 20 agents, they're all meant to work together and move together and some unexpected obstacle pops up, are all of them doing path planning? Not to mention just basic movement. Do they all independently come up with plans that are coordinated or do we have some more top-down approach where maybe just look for minimizing computation by doing one path planning operation and then somehow leveraging that single calculation across multiple agents? And there's different ways that we can have this level of coordination defined. We could have just some loose notion of a group, or it could be something very precise, like a military formation where each unit has a very specific place to go or perhaps a marching band. That's very different than if you just have a group that's all working together, all going the same place. So group behaviors are certainly valuable in games. Some of the simplest group behaviors have been useful for more background animations. We've talked a little bit about boids already, we're actually going to look much closer today. But a school of fish or a flock of birds, that would be something that could have some aesthetic appeal, extra natural animation, and world interaction that you could add to a game. There's also situations like say, Grand Theft Auto where you have all of the pedestrians walking on the sidewalks and around the urban areas. So those are things that would be nice to pull off. We've seen a little bit of how that can work. For instance, with steering behaviors, we can have a wandering behavior, we could randomly generate path planning, wherever the agent is, go to some random path. So these sorts of things could add you just some appeal to the game. Now there's also, as we'll see, other types of group behaviors can be useful more for specific gameplay, especially as you transition to formations. This is especially true of strategy games, like a real-time strategy game. You will be interacting with various units under your command. And in some cases, you might be selecting a large group and asking them to move from their current location to a new location. You might have the desire to have more tight control over how they move and maintaining formations or certain rules about their movement. And so you can have those issues with game play as well. And this can also apply to agents that are enemies of the player. So providing some challenge and you want to have some degree of intelligence, or at least the appearance of some intelligence in your enemies, especially like large groups or like a horde of zombies. How should the zombies move towards and attack the player? And then what if you have more intelligent enemies and then how did you get this to work? So if we were to think of leveraging what we've learned so far with steering behaviors, we might think about ways that we could leverage, so just as core concepts of moving to a position or away from a position. And maybe we could blend those concepts together to allow agents to go towards a common position amongst a group. Maybe we define a group location and so the agents should all move towards whatever this group location is or grouping location. But then we also don't want them to get too close to one another so that they're all just one on top of another. Maybe we want to simultaneously maintain some separation. And so you could imagine also using an obstacle avoidance type of a strategy and blend that with these other forces. And maybe we could further layer together this core notion of steering behaviors, the fundamental steering behaviors, by doing the weighted average blending approach. Next up, I wanted to show a little video that the presenter's going to, I'm not going to show the entire video, but we're going to jump to a point in the video where they talk about these enemies attacking the player and various issues that had to be addressed in terms of the design. And we'll see both tracking the player, but I believe also, I think in this video it also has some issues related to path planning involving the larger group as well. So I think this is a good point to maybe jump ahead to a little bit more of a complex scenario then we'll step back a bit again as we move forward just to give you a practical situation for developing a game. Once I start this off, I'll just be quiet and at least play the part of the presentation here. >> I'm getting a play cursor, let me see if I can jump to a point. There we go. So now we're at the point where our AI could chase the player, but it wasn't enough. As you can see, they tend to clump up or in other cases, they'll conga line once they've been chasing the player for just a little while. So while they're technically fulfilling their purpose of chase the player around and hit him, our AI is still the route of boring, repetitive gameplay. We need to get the base of data spread out. This will make our combat more dynamic and challenging and it will make our enemies feel more lethal and intelligent. We started trying to solve this problem using a technique from our ratchet and client games called arc angles. An enemy approaching the player with an arc angle of zero will run straight at him while one approaching with an arc angle of Theta will take the top path, negative Theta being the bottom path. All of these arc angles are managed by a flanking manager that exists on the player. It assigns out angles to approach to each of the base AIIDE so that we don't have all of our enemies trying to approach from the same direction. And this actually works really great when we're near the player. Here you can see the same mob of base AIIDE using arc angles. You'll notice that they immediately begin to spread out and I have to do a lot more work to avoid being hit. At a low-level, arc angles work by changing the position that an AI is trying to nav to. As the base AIIDE or the player moves, we recalculate the position along the arc angle and triangle there. In order to prevent arc angles from potentially sending a character to the wrong side of a wall, we require that a line from the target, the player's position to the arc angle position be unobstructed on NavMesh. In practice, this means because our levels aren't flat empty planes like this, the arc angle destination is usually within about 5-10 meters of the player. And this has the unfortunate effect that groups of enemies that are far away from their target will only vary over the last couple of segments. We needed to find other solutions for getting base AIIDE to spread out when they were far from their target. Our first idea was to go in and make all of the polys involved in Nav path queries more expensive so that subsequent path queries by other characters would be likely to find an alternate route. Now, this didn't work. As you can see here, the size and shape of Nav polys doesn't necessarily correspond to features in the world. Right here, we've just got a bunch of polys coming together in the middle of the road. And here we've got this thin sliver of a triangle sandwiched between two large triangles. This inconsistency can cause weird things to happen. Say we have one character move along this path, and this would add cost to these polys. This will sometimes have very little effect on a path starting at a nearby position, which isn't great. Worse off, if we move both of the path start and end points down just a little, well, this becomes the lowest-cost path. Well, we needed to find ways to add cost in places that didn't have awkward knock-on effects like this. One further problem that we had with directly waiting the NavMesh that I feel is worth mentioning is that we were unable to figure out a good way to efficiently store, utilize, and access all of this data. We have a player multiplayer mode and we have some very large multi-function multi-target fights. Instead of that, the first thing we did was adding costs to individual Nav clues. Here you'll see a bunch of characters trying to get from one side of the fence to the other. And you'll see purple lines begin to stack up as the Nav clues cost gets changed. As you've seen before, our world's incredibly dense with Nav clues, so this makes adding weight to them meaningful. Additionally, since they correspond to actual visible geometry and are usually near the boundaries of Navmesh, they don't cause the other awkward problems that adding weight to Nav polys does. Seeing one character jump over a car while another runs around it isn't a particularly awkward thing. Basically, if a bot gets a path that goes through a Nav clue, it'll increase the cost of using that clue so that subsequent path queries are likely to find another path. As an additional layer, bots who get near a clue that is already in use will try to immediately find an alternate path. This is what causes the costs to go up and down here in this video. Adding cost to Nav clues combined with arc angles still wasn't quite enough because there were still times where an AI can get a path that went for a long distance without going through any Nav clues. And so enter what we called influence spheres. These are something that the ending group gave us and it's basically the ability to bundle a bunch of spheres along with a path query and associated costs. Here you can see four such spheres drawn in red. And as an example, the one on the bottom sphere will make crossing that area more expensive, but only for one particular path query. And the way we utilize these is that when each base AIIDE asks for a path, which is fairly frequently, he will ask the flagging manager that I mentioned before, the guy who manages all the arc angles and it will actually look at all of the paths that all of the base AIIDE targeting a single player are using and look for common close bend points. And it will then place these influenced spheres on those bend points to try and force the guys to find an alternate route. This works well because path bend points will correspond with features of the geometry. If they didn't, then path smoothing would've removed them. We also try and keep the size of these influence spheres relatively small so that if they don't result in a substantively different path, smoothing will be able to iron things out and it won't look like the character is just running around empty space. So around this point, we felt like we had done a lot to the base AIIDE to make them dangerous and fun when the player is close to the ground. They spread out, they surround you, and they do a pretty good job of killing you. And this really encourages our core gameplay. However, if you get up high, it's trivially easy to just move back and forth and murder them without any resistance. And while they're supposed to be vulnerable to traversal at this point, it just felt like traversal was a win button, and the game was quickly becoming boring. So our AI was still causing problems and our last goal was to get the base AIIDE to occasionally become a threat to a player who is traversing well. This isn't something that we needed to have happen all the time but it's something that we needed to avoid the player going into autopilot. So our proposed solution was to try and get base AIIDE perched up on individual traversal pieces that the player is probably going to use. If the player happens to get too close to the individual guy, the guy will try and hit the player. In order to figure out what traversal to use, we actually collect all pieces of traversal near the player and then score them on a number of factors. Every time the player uses a given piece of traversal, we make it tastier. And this is very important because the key thing that we're trying to encourage is the player to keep up their forward momentum and movement, we don't want them to be able to use the same grind or bounce over and over. The second is that we predict using dead reckoning, the area that the player will be in for about the next second or so and make traversal in that area more attractive. Finally, we factor in how close each individual base AIIDE is to each piece of traversal. Here you'll see that as I traverse up onto wires, there'll be a bunch of boxes that appear. These are potential places for base AIIDE to wait for me. Base AIIDE can wait on bounces, they can perch on top of grinds, so they can hang from grinds. The lines going down to the ground correspond to Nav and collision tests that we use to validate jump-up positions. We also restrict them so that no more than 25% of the group can be waiting on traversal at any one time. I also feel that it's worth mentioning that we turn on this ability as well as many other base AIIDE abilities gradually from the beginning of the game. At the beginning of the game, they never get on traversal, don't lunch at the player, and are much nicer in order to ease the player into our frenetic mechanics. I think it's worth putting it all together and taking a look at how all of these systems interact with an encounter with base AIIDE light in the game. Now, you can see that they're already starting to spread out, and there are guys using our volts to get over the trucks. Here's a guy perching on traversal that's about to hit me. We have another guy use this car to jump over me. I'd use the jump to get over the car, and guys are really spread out now. And I have to go through a lot of effort to avoid getting mobbed as soon as I come back down to the ground. And this is great because this really encourages our core gameplay of traversing and fighting at the same time. Now, another thing to notice is that as I get up on top of this roof, I'm going to come down and you'll be able to see clue waiting working because I'll have a big clump of guys after me who all take different paths down off of the roof. >> Here they all come. And another thing to notice is that I really start fighting back now and these guys go down pretty easily. But about here I've taken about a quarter to a third of their number, but their ability to chase me and to force me to keep reacting, to keep improvising it stays high. And a good side effect of having enemies that are this aggressive or this capable of chasing the player, is that we don't really have the problem at the end of an encounter where the player needs to hunt down the one last straggler. Everything comes to the player and the fight just wraps up when it feels like it or feels like it should. So let's move on to some of the problems that we face with range enemies. These are three core shooter classes. The guy on the left and we call the blower. He allow. >> So I'm going to stop there in the video. Of course, you can watch more of it if you like. I forgot the link in the slide notes, lecture notes. By the way, that game was Sunset Overdrive that we're looking at. If you didn't recognize it. Take these off. So we saw a lot of interesting issues that they encountered to deal with this group movement. So the fact that we have a group in this game is really because they all have the same goal. But otherwise they are largely independent. There's no intelligence involved. That is, they're not actively or individually coordinating. Instead, it's imposed by the structure of the game logic. So for instance, they all differ to that manager that tells them signs in these different arcs to approach. That was the first concept we saw is where the group is all following the same goal, which is the player. But in order to know how to approach the player, at least when there's a line of sight, is to be assigned that arc and basically swing out. And so that allows the ability to accomplish this spreading of the enemy. It's not getting too close. And then when path planning became involved, you saw that their first thought was given that they had a NavMesh available, is that they would just augment the Navmesh with additional metadata. And so you saw how in that talk they discussed adding these temporary counts or scores to the NavMesh polygons to say like, hey, this is a really congested area and then that would adjust the weights in the A-star search for the agents, they would be forced to solve a different solution. But that ended up not working well because of the structure of just the nature of the way that the different convex polygons can all perhaps meet at a point. Like a triangle fan scenario. It ended up not really working well in that situation. So instead, they relied on embedding some intelligence into the maps with those Nav Clues. That's actually a pretty common thing in game engines for AI support, is that you might have a NavMesh represents a normal traversable area, meaning the enemies or agents can stay on foot. And then if you get to a pit to jump over or some spikes or just anything that's in the way that you usually coincides with a different animation. To get past the obstacle, like hopping over a car, hopping over a fence. So you'd have a clue in the map. We mentioned before how sometimes you put the intelligence into the map rather than entirely in the agent. That's basically what the Nav Clues are. And so those were being leveraged in a way to add this temporary knowledge that there was congestion. So if there was a large group and in this game there's often always large groups chasing the player. So if one agent is hopping over a fence in a particular spot on the fence, well, that's going to become congested and that spot temporarily, it'll quickly return back to normal. But it stays at this higher weight so that it forces the other nearby agents to go select a different location. And then in the video they mentioned how that's not sufficient because there are large parts of the map where there just aren't enough of these Nav Clues. So instead, if the A-star path planning is occurring on the NavMesh, there's going to be many agents that simultaneously there going to be passing through the same route. And they also will all have the same inflection points, which will be the bins going around various obstacles. Now this relies on using string pulling. So this could only be facilitated by the fact that they have the string pulling algorithm as opposed to a steering behavior and active tightening of the path or a cleaning up at the path. At least in this case, it would be an argument for doing string pulling as a post-process on the A-star return because that allows you to more easily aggregate common inflection points. So if you've got 20 zombies and they all have determined a path to their goal like the player position. They might be fairly spread out, but they're going to share a lot of these inflection points after you do the string pulling. So at that point, once that inflection point is identified, then AA marker is placed at that common inflection point. So it's like a trigger zone, so is determined by a distance. It looks to be a circle or a sphere. Or at least the way they are visualizing in their map. That's pretty easy to calculate whether an agent is within or not. It's just based on a distance to the center of the region. So as soon as the agent is within the radius of that trigger zone, then they're within it or as soon as they're part of the desired planned path is intersecting, then that could cause a rerouting of the path. So you can see how there's a cascading effect of design decisions in terms of what strategies they could use to spread things out. Then we saw another application specific approach to dealing with windy agent hops up on the power lines and slides along. They needed to occasionally bring those agents up there. But that took a little bit of care just because they wanted to bring some of the agents up, but not all of them would find the right amount of balance for that. Not to mention making sure the head animations and things were physically possible, and so on. But the end result, it looked really impressive. Again, this is a situation where the agents are really largely acting individually. And the cool results in the quality of the gameplay is really, it's more of an emergent outcome. This is true of many of the group behaviors that are short of being true formations. Just so you have group structure, you end up getting this more emergent potential there. >> So next step, stepping back. So we're going to look at flocking. So this is related to the Boids approach. In fact, Boids is probably really the starting point. Let me share that picture of the Houndeye there from Half-Life. But yes, flocking use the Boids algorithm as a starting point, and there could be some variations to this. This is a case though where we do have agents that are aware of other agents. So they're actually making measurements to one another. This is a little bit different than what we just saw in the previous video, where a lot of the separation was accomplished through a manager. So the agents weren't querying one another. Instead, they were just asking a common AI element, like this AI manager, "Where should I go? What arc should I follow? Or performing path planning, and the A-star returns, taking into account the changing weights according to those weighted inflection points and the changes to the nav clues. So when we'll be looking at flocking in the next few slides here, we're now introducing that the agents query one another and have a very primitive communication, just in that they're reading the positions and the velocities as well. So the minimal thing that you need in order to support flocking is to be able to, amongst your group, build a move towards the centroid. That's the best way for the group to make a tighter group. But there's a problem there, and that you're going to end up getting too close. If you just blindly have agents go to the centroid without any caps on that behavior, they'll just all clump up right on that centroid. So I've got a little clip here actually of the Houndeyes. Let's see if I can get this to play. So by the way, these are the Houndeyes again from the game, Half-Life. I think my personal opinion is that the Houndeye is an unappreciated enemy, especially depending on the way you play the game. I think a lot of people that played Half-Life developed this very cautious way of moving through the levels, probably a lot to do like the danger of the Headcrabs, for instance. But it was very slowly moving forward, picking off enemies as they were alerted to you. But the Houndeyes actually had this very interesting behavior. So there's some interesting, like state machine logic involved, and they would communicate. So if a Houndeye spots you, they call for others, and they will come from wherever they are in the map doing some path planning. And if you don't kill them off quickly enough, they will group up together and they'll follow you like a pack of dogs, act like little bulldogs, and then if they corner you, they have a formation, and they synchronize. And when they synchronize, they will do the sonic attack. So normally, if you only encounter one at a time, if you're efficient in picking them off, they might give a little small attack that does very little damage. But if you say get five or six of them altogether, and then there's five or six do the synchronized attack, then it's like an exponential increase in the damage. So it's an interesting enemy. I think it had some cool AI, especially, for the time. So with that said, let me just show you a demonstration. So yeah, that was the Houndeye. So I would love to see more of similar enemies that work together and have that cooperation or coordination. So now, next up, we'll look at this specifics of Boids. Craig Reynolds was really the pioneer of steering behaviors, at least as we've presented them so far. And so he developed the Boids algorithm based on an extension of the steering behaviors, layering together the individual steering behaviors, but doing it in a way that allowed for this group behavior. So he deals with exactly the problem we just described. We want to move towards the centroid, or have our agent move towards a centroid of its group, but we don't want to overdo it, so we're finding a balance between going to this desired position, but at the same time, of trying to find a balance between getting too close to neighbors. So that's this avoidance force. And there's also a desired velocity component as well. And this is based on doing similar action, similar movement through the environment as your neighbors, so direction and speed are coordinated. So we can think of this as these three key components. So that's going to include separation, cohesion, and alignment. So we're going to find a balance between all of these. So separation is a desire to not be too close. So if you think like this ring of preferred distance, and if any other agent is within that ring, that's going to start to form an avoidance force. And these are all weighted average together. And then there's cohesion, and that is desire to be at the centroid of your neighbors. And then alignment is, again, the orientation, the direction of travel, so the direction component of velocity, as well as the speed component. But that, as you recall with the steering behaviors, you can't just set it to a value, or you're breaking the fundamentals of the steering behavior. Instead, you might identify a target position, and then you have to apply a steering force to turn the agent towards the right direction. You might not get there based on the constraints of the vehicle performance envelope. You can only turn in so faster rate, and you might not get there within one tick of the clock. You might have to wait over several frames. Same with speeding up or slowing down. So there's the smoothing effect that is applying. And since every agent is also making the separation cohesion and alignment determinations, it's going to take a while for these adjustments to trickle and propagate through the entire group. So you'll see basically a wave of change that will propagate through. Just like when you see a school of fish in a nature video, like if they're swimming towards an outcropping of coral, the leaders of the group will suddenly have an obstacle avoidance strategy employed, so they'll steer away, they don't want to hit the coral, and then all of the fish behind, they're trying to do some strategy similar to what we have here, so they don't want to run into the fish in front, so they might start to slow down. They will start to turn and match the turning and direction of the leaders in front. And then the whole thing will play out as this propagating from the front of the school of fish to the back. And so you get exactly those same characteristics with this implementation strategy. Now, in terms of doing this in a practical way, there is an issue of the computation. So how are you going to be able to perform this in real-time? And in back in 1987, when Reynolds worked on this, in fact, I believe most of these were run not in real-time, but rather precomputed for animations and animated movie-type content. >> So you do want to come up with a way to do this efficiently. So you need to maybe not calculate every single boid against all other boids. If you have 200 or hundreds of agents, instead of looking at all neighbors, you might only look at a subset of the entire school of fish or the entire group of boids. So perhaps have a smaller neighborhood defined to look at. And this could be done through a spatial partitioning scheme. So we're going to divide space ways pretty similar to our discussions of discretized space for path planning. Very similar strategies we would employ here. And depending on whether these boids move in 2D or 3D, we could have a quad tree or an octree perhaps. Or just not the hierarchy, but just a lattice, a grid lattice type of structure, either 2D or 3D approach. But otherwise, if you have no way of performing this and comparing everything, then there's going to be a quiet a load in terms of computation and comparing against all the neighbors. Now additionally, as these calculations are made, whether you have a spatial partitioning scheme or not for identifying a neighborhood, you can take advantage of caching. This is the situation because for any two boids that are in a neighborhood, like boid A and boid B. If boid A is measuring separation distance with boid B, that's the same distance that you get from the perspective of boid B. So we might want to come up with a strategy to cache these values rather than expensive square root computations, especially, it can be valuable to cache this and get noticeable improvement in performance. So if we were to use the brute force method for n in boids total in our group doing no neighborhood determination, this is going to be O n squared to do so. But we'd really probably want to avoid that if we can, especially if we have a large group. So in that situation, we could make a bin-lattice. This is very similar to a grid lattice. The bin-lattice, it just keeps some list structure per cell or per bin, and this is updated from tick to tick of the simulation. So every time you do your next frame update, you track the movement of boids and reassign them as they move from one to the next. So if you imagine in the 3D case, we could just divide space up into this 3D grid structure, and then we could have an initial bootstrapping process, where we say, well, based on the positions of all the boids. And we'll say, there's three boids in this cell and there's five in this one and two in this and so on. And so you have all of these different lattice bins that have some number of boids in them. And then the boids, if you want to determine the neighborhood of a boid you say, what bin do you belong to? So that's going to have a few boids in it perhaps. That's some of your neighborhood. Well, what about the cell directly above you and the one directly below and the one on front and back and left and right, and then all the diagonal corners as well. Now, this is a case where you generally want to also include the diagonals with a bin-lattice. And that's because what if you happen to have an agent boid in a bin and they're already really close to a corner. So you really want that 8-way connectivity because you could have boids that are very close to you in the diagonals in the case, again, near a corner. So this is generally done with all direction connectivity. Whether you've got the 2D or 3D cases, both the axis aligned directions as well as the diagonals. And so that defines your neighborhood. So that's going to be all of the boids you find in all of those bins in the neighborhood will be used in determination of the separation and cohesion and so on. So that's going to really simplify the overall load on the system if you can employ this. There's a bit of overhead because when you finally decide where your boid is going and how far it goes in the frame update, occasionally, it's going to cross a boundary from one bin to the next. And then when you cross that boundary, you need to make sure that you actually update. Remove it from its current bin and then reassign it to this new bin. You generally want the resolution of the bin-lattice to be such that you never move more than one cell or one neighboring cell at a time. We get to O n times k, and this is based on the number of surrounding bins to consider. So if you were moving really fast where you could traverse more than one cell at a time, in that case, you're going to have to increase your neighborhood to more than just adjacent cells. That's why you tend to want to size it correctly, but you also have to take into account how many boids there could be per cell as well as the other issue to consider. So for this separation, again, we want to steer our agent to avoid getting too crowded. So we're trying to maintain this separation, and you want to try to avoid an issue where your agent will backtrack for instance. So you might have a cone of perception perhaps that you're only considering agents that are in front of you. And so that can help to main that more forward style of movement that's maybe a bit more natural to only focus on avoidance in dealing with the forward direction. And then for cohesion, we're going to steer towards the average position of the local flock mates. So again, that's going to be our neighborhood. So you take the center of mass of the average position of all of the boids, and then that's where your agent is being pulled to. And so they're going to want to steer and move towards that position. But you keep in mind this is being balanced out with the separation as well. So we've got cohesion and separation, but now we also need to add this velocity in alignment. I'm going to show a video that someone else made. They're really nice demo where they turn on and off these different elements. And so you'll see that without the alignment that they'll be adding. If you just have the separation and the cohesion and what you'll get, it looks more like a bunch of critters all huddling around something to eat. A bunch of ants swarming to get some dessert before they've started doing their little trail back. But you really want to have this purpose in this movement. And in fact, that could also be something that you could leverage in terms of gameplay. So the alignment, what you're doing is you're steering towards the average heading. So similar to the center of mass calculation we made, we can also just calculate an average heading, and then there's a desire for the agent to align their current steering heading the same way. Again, they can only turn so fast. There's a limit to their turning rate. So whatever their goal is, they might not achieve it instantaneously. So there's this damping effect as these adjustments propagate through from boid to boid. >> Here's just to recap again. So we've got the separation. We can think of this as being a radius around the boid in question. So they're seeking to separate if any are too close within that radius. And then there is alignment where they're attempting to at least within a neighborhood, trying to face the same way. If you think about a large group of boids, even though I might compare boid A to boid B for neighborhood, they don't necessarily share the same neighbors. Node B might compare for separation alignment cohesion with node C, but boid A is not aware of boid C, because they're not in the same neighborhood. So you have lots of these overlapping neighborhoods, and that also contributes to the way that information propagates through. So let's go up to this video I wanted to demonstrate. I'll let this play for you and you'll get a good sense of the impact of each of these steering strategies independently. I've got two videos here. I believe that this one is from some of Craig Reynolds very earliest work. And then I'll show you this more recent thing where they turn on and off. There's not much to that one, but interesting just to see the earliest work in the boids algorithm. Next up is the video it's just describing. >> Here's my implementation of the boids clocking simulation, which many people have done over the years. There's one boid and it's moving randomly and it's avoiding some obstacles. But what gets really interesting is when we put a lot together, they behave like a flock of birds or fish or whatever you want. They do this with a couple of different rules that when combined together in a group, you get this really interesting emergent behavior. They can use a buzzword. You'll see they break up and reform and do all these interesting things just based on those rules. So the first rule here is alignment. So what each boid does is it looks to its neighbors, its friends and a small radius. Maybe about two or three body lengths as long as they are now. And it averages the position of all of its friends headings, the direction that they're facing. And it moves a little bit towards that direction. So if all of its friends are looking at the left, it turns a little bit towards the left. And every boid does that for every other point that's in that little radius of its friends group. If I turn that off, you'll see they just go everywhere, they don't really have a reason to go in any one direction or another. Now there's some other rules. Obviously, you can see they like to be near each other. And that's the second rule. There's a cohesion rule. They take that same radius around themselves of friends and average their positions, they'll say, okay, for all the boids around me, roughly, where's the center of all of them? And then they seek towards that a little bit. So there's also a balancing force against that. If they get too close to all the other boids that are near them, they back away, they avoid crowding. So you can see these look pretty realistic to maybe little fish swimming around. They like to be near each other but not so near. But because they don't care about what direction they're facing right now, since I've turned off alignment, they just float around, and as soon as I turn that back on, suddenly they looks like they have a purpose. They're swimming fast. Immediately that group just reformed with another group. Those are the three rules. And I can turn these things off at will here. If we turn off crowding rule, they'll get really close together. They want to be near each other and they want a copy to those headings. So they formed into just the line. I'll turn that back on, they'll pop back out. There's some other stuff going on here. They have some noise. If I turn off the noise, they're pretty smooth. And if I turn it back on, you can see them jiggle. If I turn off all of the rules here, now they're fully random and they don't care about each other at all. If I turn off the noise, they just drift. They still care about getting away from the barriers here on the edge. And in fact, I can come in here and [inaudible 00:56:40] moves back on and lay down a bunch of barriers for them to avoid, and lay down a bunch more boids and you'll see lots of interesting stuff happen. They avoid areas, stick back together. All sorts of stuff. So if you want to download and play with this, it's on my GitHub link here or in description. [NOISE] >> So hopefully that gives you a good intuition on how the boids algorithm works, and as you saw, it can be pretty effective. Certainly is fun to play around with and it can add a lot to certain situations in games like the birds and fish type scenarios. So next step, we're going to start to think about formations. So we've had about the most limited coordination that we could have so far. How might we go about introducing these more complicated scenarios of movement? So just in terms of terminology, we're going to be thinking about units that's the single agent groups where we have collections of units. But a more relaxed than a formation which is a group, but a group that has positioned assignments. Each group member has a role to play specially. So I've got another little video here where I'm going to show some features. This is not my work, but someone else that had in developing some libraries for unity doing different formations. So this example, we have a formation, it's very simple. Trying to maintain a line, but you can see that there's different scenarios that challenge the formation units that can't maintain that structure. Also another interesting thing is that it's an oriented formation. So the direction of travel defines the orientation and layout ultimately, so we have this wheeling or rotating of the formation that's involved. We saw different types of obstacles. We saw obstacles that forced the individual units to have to spread out or split. And then we saw a pinch point walking through like a canyon, squeezing together. And so those are the challenges that you face if you're trying to implement this formation movement. Here's another example. >> So here we have some pretty complicated formations involved, more like some battle long ago, wherever it is, highly regimented battle formations. So that also demonstrated changing between formations. So there's a challenge of how do you do that effectively? How do you move from one configuration to the next? And also another example of dealing with obstacles. So definitely, some tricky issues to deal with once you start having this higher level of coordination between units. So in terms of implementation strategies, you can think about having individual agents that are planning and moving all on their own. So they make their own decisions, but they're sharing a common goal. And so that's the coordination we saw at the beginning of this lecture, is that largely moving independently. And we did see a few cases of indirect coordination like the arc manager that was assigning to the zombies the different paths to go. And then we've also seen a little bit of individuals making these complimentary decisions. So some shared information and certainly with the boids. There is a primitive sharing, but it is effective just to query and respond to the state of other agents. And then there's also the possibility that you could have your groups that are making decisions as a whole. So this is where some notion of a group where actually, we have some AI entity, probably not something you see, or it could be, you could actually assign this more important or higher-level decision-making, could be assigned to a single unit, that can make sense in some games like you have a general or a hero or something, then that unit could be making decisions and the rest are listening to the decisions. In terms of moving as a group, if you do this in an independent way, you have agents that are making a determination of the shortest distance to the target that can often result in a traffic jam because you're going to have units that are attempting to occupy the same locations. And they might all be trying to land on the same location at the end, but even during the traversal, they're all going to be aiming for the same inflection points. And we've seen exactly that issue earlier in the talk and some of the strategies for adding some smart to the world or adjustments to a star as well, in order to deal with that. So if you are running into these problems with these traffic jams, you're going to have agents that might stop. You might have if everyone's working totally independently, every agent, then you could end up with agents trying to plan around, setting their group members, and might not be the most effective approach. You could end up with a vicious cycle. Even if agents have the ability to be flexible or respond to changing conditions like, okay, well, there's something in my way, whether that's an agent or not, something in the way, and then they choose to go around, that might not always be the best strategy. Maybe there's some level of cooperation that could have been more effective or just waiting a moment and things could have worked out better. So once a strategy involving some coordination is that we could have say, a single calculation made for movement. So this notion of a group, we can say where it is that our group is maybe defined by their current centroid. And then we part that and so we say that's where the group is. And then we can perform path planning from that spot to a destination. And then the actual position to move to is an offset from that centroid. So each agent is aware of their offset from the centroid at the start, and then they try to maintain that same offset as they move. And so you need some notion of path progressing. So what are the operators that we're performing on our state-space? The agents need to be able to check against this moving centroid and trying to maintain this position. So all agents are trying to move. So you've got that one plan you're following along, but of course, there's all these issues that you can run into that we saw in the previous video we just watched so obstruction, choke points, and then also occluded destination. So you might be able to have your agents maintain a formation the whole way, but then when they get there, it's impossible to maintain. What do you do? You err on the side of having one agent have to go to a different location or rather do you reset where the entire group goes so that they can better create that layout. Now, we've already seen like the grid lattice before and looked at discretized space. And if you have a game with discretized space and have group style movement, there are some nice features for multiple agents moving about in the environment, and that's because you can very easily say that certain cells are occupied or not. So you can just update the state of any given cell or grid location. And so that has a nice feature if the grid size coincides with the size of an agent and you have a one-to-one correspondence so that any one cell can only hold one agent, and you move from cell to cell. So that even though it can have a very wasteful resolution in terms of the graph, in terms of the dynamics of the space, it has some nice features. So again, we can add runtime easily mark cells as being used or unused. And so if there's something in the way, then agents can just plan around because they can easily tell that there is some occupation of the cell that would otherwise be considered. You can potentially some like coordination type of approaches that you might want to take like sending a message to a unit to say get out of the way, might be a better move. In some cases, let's say it was a strategy game. If you send a group on a path and you have some other units that are just sitting around blocking, then you might want to have just automatically recognize that and move out of the way rather than triggering this full re-planning. So it can get a bit more complicated as well, and that's really true. A lot of these group formation type of approaches is that you can always consider a different approach or a little bit more complicated or the context is different because of the nature of the gameplay. Like one approach might not work just because of the type of game that you're making. But as you're trying to come up with a better or more capable strategy for coordinated movement, you start to have the need for more data structures. Some way of planning better, especially like in the sense of a timeline. Really your plans that you're making moving multiple units, think in terms of like a real-time strategy. You could have separate groups that all belong to the same army, they have different marching orders. And those marching orders, ideally, you would want the best possible coordination between all as possible. >> And so in order to get that to work, we're going to need some way to track through time like planning into the future and anticipating future collisions or congestion. So that's where things can get really, really tricky. So one practical impact on games is how are you going to deal with collisions between objects? So, so far we've really just talked about AI and the decisions related, and to a lesser degree about any physics simulation. We have, in terms of steering behaviors, we've mentioned like the performance envelope of the vehicle model for the steering behavior agents. But in terms of any physical reality of our simulated space, we haven't talked as much about it. Let's say where you are implementing a real-time strategy game, there's going to be, it's going to be a real challenge to get formations and group movement and so on to work especially on arbitrary maps with fish might have tight canyons or pinch points and various types of obstacles and all these units moving either in groups or formations. So in terms of the physics of how these units interact, you probably want to have some physics that disallows units from occupying the same space. And your first thought might be to do that in a very rigid way. I'll say, well this unit is, let's say we have tanks. You'd say, well, this unit is a rectangle shape and it's never going to intersect with this other rectangle. I'm going to impose lift through it like a physics calculation that if this unit runs into the other, they can't continue and get closer and closer and overlap one another, it's just not going to be allowed. The problem with doing so, is that most of your AIs that you're not calculating, I'd say, a guaranteed solution. Instead you're making more of a best effort reactive agent. Sometimes it's going to make mistakes the same way that we saw in the last lecture, the open steer, sometimes the agents would say cross into an obstacle, they just go right over it. There's no physics and post in the open stair demos. So it would cross over. If that was in a real game, you would have a collision. So your rigid body physics simulation would give you a collision. And so if you're making a real-time strategy game and you have a collision, you could impose is very hard constraint. But in practice you're going to find that being so rigid in your collision detection, you're just going to end up with units getting stuck. So instead, what you probably want to do is soften the constraints and make the collision detection, maybe there is some element of hard detection size, but then you're going to have a softer collision detection. And so some of this could be more of an avoidance force, you might have agents that visually look like they occupy subsides, but in reality, there is a soft separation that's slightly larger than the unit. But also often that soft radius will actually continue down well within the visual limits as well. And it can vary depending on how the quality of the AI, how effective it is at avoiding getting stuck and so on. This might be less obvious, I guess, but yes, so being able to have these two notions, this will allow in worst-case maybe units to get in really tight even to the point of not being very visually appealing. Also potentially maybe recognizing collisions early before that they're actually touching. This would be similar to having in video game physics. Many solutions have a skin width, so you have a hard constraint on a rigid body but then there's this additional skin width and it has some nice properties just for basic physics out outside of any AI. Once you've established this means of collision detection, we could also think about not just real-time collision detection, but what about collisions in the future? And so this is where things can get a little tricky in terms of very complicated planning. So if we've got, again, a real-time strategy and we want to deal with planning out paths that formations and groups are maintaining and traversing the environment and they could be crossing paths and so on, we could have some defined collision detection. So how do we detect collisions in the future? So what we could do is we can make a movement line. Movement line is a bit like a path that any of the paths that we've considered before for path planning. In fact, it could have started with path planning and then we're going to extend the path with additional metadata. Tracking, not just say after doing string pulling, you would still have inflection points, but instead we're going to need to have samples along the path that reflect points in time like a timeline. So you might have a long straight path in it if all you needed the path was for path planning, long straight lines would be great, but we need additional information. So we're actually going to sub-divide these long paths based on some temporal sampling and so at each of those points, we can say, all right, well, at future T, some value, this unit is going to be here. So you can have lots of movement lines and they're all overlapping. And even if you have intersections of the path, that doesn't necessarily mean anything because you also have to consider the moment in time. Now, the resolution of these samples could cause problems because if you compare one movement line to another, you might skip over or not have any intersection. So if you just had every couple of seconds a new mark on the map, you might not necessarily have an intersect with other things. And so you really need to have some way of connecting the dots, temporal sample to temporal sample and so this can generally be done with a ray cast or some similar geometry that you could test for intersections. But then if you do have intersections, you also have to figure out the time comparisons. So you've got a t-value from one movement location to the next, and so if that intersects with another line, you can say, well, does that intersection matter? And so you could check to see if the schedule of the two are similar enough at that point of intersection. And so that allows you to identify problems or if something is occupied, and it helps you deal with the situation that your space discretization might not provide this easily for you the way that a grid lattice would. So grid Lattice, we could just start adding metadata straight to the grid for this purpose. But if we've got say, a nerve mesh or path network, then we start to need to be able to think about these movement lines as providing this ability. But yes, so if you can represent units in this fashion, you can then start to consider where there might be problems in the future and then go ahead and plan accordingly. So say you have an intersection and a path, then you could go ahead and replant. So this is fairly similar to the metadata for the zombies and trying to keep them spreading out in the video that we watched earlier, how you might be able to do that. But this can also include additional plan information for a unit. So your plan might be to have your unit slow down. Maybe they don't go full speed, and so there could actually be queues in the movement line to capture this information. So you'd say, all right, you slow down as you follow this movement lines slow down at these points and then speed back up here and so as there's these refinements that are occurring to this coordinated plan, you can embed that knowledge in these unit lines. >> In terms of how do you deal with collisions? If you are detecting collisions, what do you do? And the simplest approach is just anytime there's a problem with a path, you just recompute it. But again, when we're talking about large groups of units or formations, you've run the risk of reaching a vicious cycle where lots of units are just constantly recomputing a star. And if some units are doing it, they're getting in the way of other units and it's just, again, a vicious cycle. And it's going to hit your frame rate. Ruin your gaming experience. So what you might rather do is don't search a new path at each collision. Instead, this is assuming these are all units on the same team. So, therefore, it's desirable for coordination like in a real-time strategy. If it's all your army, you can have a priority structure where higher priority items or units, they get to maintain movement without any re-planning. They have authority, and then lower priority agents, they get out of the way. And so I think an example here for if we're sticking with a real-time strategy scenario, you might have the big heavy mega tank, like the really expensive slow-moving unit that doesn't turn very well. Well, it makes sense that, that unit one being so powerful and important strategically that it just maintains course. And if you have some small scout units or infantry, well, they need to get out of the way. They're similar to the rules of the ocean. The big ships get to maintain their course and all the smaller ships get out of the way approach. And so that can work well. And there have also been approaches at using some higher-order decision-making, such as case-based reasoning. This is a topic we'll cover later. But just consider it like recognizing a common scenario and already knowing a solution that works well through like a caching scheme. So you have like a dictionary that you look up a scenario. So you have some way that you are sampling from the current scenario. You go look up old similar situations before. And well, what worked last time? So that's, like a higher level, but how that might work. So in terms of resolving, if we have these unit lines and we see these collisions are going to occur in the future. Or even if this is a real-time decision-making collision resolution. So we see that this is a pending problem. So we want to favor our high priority agents over the low priority. And then next step you might favor, well, what's moving versus non-moving? So again, let's say we have a cluttered map in our real-time strategy. You've got maybe some units that are just sitting around guarding the base and you're sending this carefully orchestrated attack on your enemy base. You don't want anything to slow them down or mess up your timing. And you may have forgotten to get the non-moving units out of the way. So it'd be nice if you send your attack force, they're marching along any of your own units that are non-moving, there would be lower priority and so they are then given a new task they just automatically triggered is a movement or a path planning to get out of the way. And so again, this would be like based on analyzing the movement lines and detecting this future collision. And then that would trigger this generation of the correction to move those non-moving units out of the way. In terms of things like lower priority units would have to do to get out of the way. They could either generally do one of two things. One is they turnaround, or move out of the area, but the other is that if they themselves are moving, they will change speed, possibly stopping and pausing, waiting for the higher priority units to move first. So if you have this sort of strategy. So we're coupling the movement line, which again is like a timeline. With this priority scheme, when we how these anticipations of collisions, then this gives us a good way to deal with those problems. So giant slow-moving units are going to be high priority. You really have to think also about the gameplay. Some more powerful units, if it's more important strategically, certainly, those will tend to have higher priority. If they're important to game objectives or story like if it's a hero, not just some anonymous grant, but something important to the game a leader, and then also the sub-situational aspects like the moving versus not moving units. We want the not moving generally in terms of gameplay, it makes sense for them to get out of the way. And then you might have some contexts provided by objectives. So if a unit is dealing with an important objective that might temporarily bump up the priority. All right, so in terms of the distinction between groups and formation, so groups will say, "When we have a defined group and they're all moving together, they need to stay together." So that means that all the units move at the same speed. So for you to say, implementing a real-time strategy game. It would be dangerous if, say, we've done say, dragged our mouse cursor over a group, and had them move across the map. And then we had some really fast scout units race ahead at full speed, and scouts tend to be the faster units. And then you might have some slow lumbering units or units that can't defend themselves. Some harvester unit, so there'd be a danger of you just letting them all go off on their own. Instead, we want to maintain the group and movement. So this is a straightforward thing to do. We just will impose a speed limit based on whoever is the slowest. And we might have to do some additional consideration of turning rate as well. So if there's turning involved and there's a big difference, turning that could have an impact as well. That you might have to address. >> One strategy that you might build to take with a group is to have, again movement that's not everyone moving individually, but where we have a group structure involved. So once a group is formed, it initiates this new agent entity like on demand entity that is the group. Often you don't see it, but it would just exist in terms of memory of your computer. It thinks about the group moving as one cohesive entity. But you can't occasionally have it as part of the game. So again, as like a hero or commander unit in the game, that can be effective. If you have this notion of a group, then you do the path planning only for the leader or only for consideration of this abstract group and then all the units move towards that position, just following along. Basically, just chasing that position or leader unit. But then you can get more advanced. We already mentioned before having the offset position from the leader. So that could just be regardless of direction, you always maintain that relative position. Or if you want to get really fancy, you can have the rotating or wheeling. That is assumed that the orientation is always with a forward vector, the current movement of the leader. If the leader turns, those are way off to the side. So they're going to have to really hurry up to get to their positions. Or the leaders going to have to really slow down and wait. Because you can imagine those that are way off to the sides. The turning would impose impossible velocities for them. So the wheeling really can introduce some challenges. Then you say it's just some basic simple steering behaviors in this mode and just following the leader or that offset position. And that works well until there is a substantial block or getting stuck in a local minimal situation that might initiate a path plan for an individual unit in addition to whatever the group has done. But ultimately, rather than planning all the way to the goal all alone, their plan is to just get back to line sight with a leader. And once that happens, then they can go back to their old strategy. Now you may also want some level of communication where this units that got stuck and had to path plan let the leader now or this abstract group. But in any case, let that AI agent entity know that they're stragglers into slow down and let them catch up again. So formation this is where we have layout. When you do have a formation, we usually are introducing some additional states to the basic group behavior, and that is that you can have a situation where the formation is forming, formed or broken, then only a foreign formation can move. The idea here is that you may in some cases need to break the formation. Either in terms of gameplay. Like maybe you have some units scattered, and they have an assigned formation and so you hit a hotkey. And they all rush to the formation. Or it could be because they're forming and breaking as they deal with challenges in the environment or combat and so on. So they might have a situation that breaks them up and then they have to reform in order to move again. Formation or the forming side of formation is tricky, and that's because a lot of times you have a formation, they're really tightly packed. Especially like military, which is probably the most common situation we have a formation in a game. So this military structure that you would potentially have with these multiple agents. Think about how everyone gets into position and how to do so efficiently. There's probably like an ordering of setting up that works better than others. And so you really should come up with a strategy that involves forming at the middle and working the way out. So agents that are in the middle would be assigned to go to their positions first and then you add more and more agents. So say, "Okay, now it's your turn, you go to your spot and so on." And as you do this, it might vary each time as well exactly the specific. So even though you know, you're starting at the middle, well, which of the middle units or which of the next layer? As you work your way out, and so you might want to have some heuristics looking for the least collisions or at least distance. Maybe you can do a re-cast. So you'd say, well, these agents up next there, the next layer building up from the middle and so of the ten total positions, three of them can do a ray cast and not hit anything. You'll pick the first one that's the closest of the three and then the second and the third. And then once you've exhausted all of them, now you check for new recatenate since they just move. Probably a few more agents now can re-cast safely. And then maybe as you diminish those numbers that need to get into position. Maybe there's a couple that actually need to do a star or path planning because they're such a big group or obstacles in the way they need to do a little bit more complicated or expensive movement. You can also incorporate the unit a priority. Again, if you have the big powerful tank that slow in lumbering, that would probably make sense to move them first or minimize their movement overall. This figure here, I think, gives a good idea of what could happen. If we have a target formation that is like a three-by-three, very tightly formed grid formation layout, you could end up where all the positions on the outside get exactly where they need to go and there's no room to go in. Instead, you would want to work for again, from the middle outwards. You don't even if you have the ability to say in this case 4 and 6 could you get out of the way for nine, there'll be a little bit of wasted effort, maybe not so bad. But if this was a very large group and you made no effort to plan this, you could end up with a vicious cycle where the four and the six getting out of the way, are bumping into other units even outer layer and just could end up where all the units are in a infinite loop of vicious cycle, getting out of the way and getting into the way of some new units. Here's a look at wheeling and orienting. If the starting point, if the direction of travel is straight down and we're trying to maintain straight across 1,2,3,4,5. Let's say we turn to I guess from their perspective and going down, they would turn to the left. If they start to turn left, this is what I mentioned before about creating a problem because you have that. The outer lane there for the one, how much longer the path is to swing around relative to the five. So the five can easily make that position, but the one has that impossible to maintain rate for the term. This is a situation where you would often break formation. The wheeling has been initiated. The brake formation instead, each unit goes to an individual strategy of getting to where they need to be. Units hold, so they get to their position and they just wait. They're waiting for the formation to re-form. That shows the benefits of that state machine logic of formations where you can be forming, formed or breaking. That is a way to deal with wheeling. >> And just to reiterate, when you're trying to maintain groups or formations and want them to stay together, you'd have to be aware of the fastest that the slowest unit can go in order to be able to have a realistic formation. Otherwise, you're just going to have everyone planning and going with their own individual movement. Let me see. I think we've already covered the issues with the turning and the wheeling. Now formations with obstacles. These are more situations where you might have breaking, but in this case, it could be valuable to break a formation but not do so in a way that's just completely all or nothing. Instead, you could form a new formation that's a sub formation. So you say, well, these two units are still close and still roughly in their relative formation position. So let's just make a sub formation of them where they split briefly. So this is a fairly complicated approach but their complexity could be worthwhile in a really detail-oriented strategy game where the maintaining of a formation would be critical to the gameplay. So rather than just say, oh well, the units can't be in a formation anymore. So we're just totally broken and just wait and reform on the other side. Instead, this would minimize the recovery and also maintain the military capability. Now, if you're going through a pinch point, is possible the units can still fit. And so in those cases, you might want to first try a scaling of the formation. So the formation manager could say, it looks like they don't fit in terms of analyzing the NavMesh, this formation geometric shape does not fit. So then you could say, well let's apply a scaling to make them fit. And then if that scaling was applied, you can then analyze individual units and say, are these units in a valid position? Meaning that they aren't breaking hard constraints. If they're breaking soft constraints, it may be okay. But if they're breaking hard constraints, then you would just say, we'll break formation completely and go a single file or in a row of two or three and so on but if the scaling can occur and it can fit, then you can go with it. So similar to our discussion with just general groups, we can also have a hierarchy for the path planning problem with the formation. And you might even do so in a way where you're also scaling the complexity of your environment as well. So you're planning for the formation or the group, not the individual units. But we're not going to compare that to the entire map or the entire NavMesh. Maybe we have a simplified version that works with a larger size, so a formation you could think of is like a single giant unit. And then if you do that, you might ignore small obstacles. That's because, in reality, your formation has this fluidity to it. It could break apart and reform and your price still fit through areas. So you can have that lower resolution path planning exist just ignoring smaller obstacles, and that's going to prove computational complexity. But also it works with this notion of a formation that we have and then we can work our way down to the lower level as needed. So if we enter a situation where the formation has to break, which might happen because we ignored the smaller obstacles, then we're doing path planning on this different hierarchical level and on a different resolution of the world, and then we just reform the formation on the other side of whatever that challenging area of the formation is. And in terms of how we might manage this computationally and with data structures is that we're maintaining the high-level path. We have a concept of a path stack. So we're maintaining a high-level path strategy but we encountered this issue that requires these lower levels breaking up the formation into sub formations and each moving. So in those cases, we still cash or preserve the high-level strategy that we were following but now we push onto the stock stack, then this lower-level strategy, and follow that until it's complete and then it pops off the stack, and then we're back to the high-level again with the reformed formation. And so this is one way that we can deal with that. And then this can give an idea of what might be involved. So at the high level, we have a unit that's just following. It's relative offset to the hero or leader or abstract notion of the group moving as a whole like a centroid. And then we hit some issue and so we divert from the high-level path and employ the low-level strategy. Do we do this avoidance path planning just briefly to get back to where we need to be around the obstacle and then go back to the high-level strategy again? So just to wrap things up, we have this notion of high-level and low-level pathing to deal with our groups and behaviors. So the high level will be the thinking of just the entire group moving. There's different ways in terms of actually synchronizing the individual units, the high-level plan that we can employ, whether it's just chasing the leader or trying to maintain this relative offset. And we can play around with whether the agent logic for our group or formation is hidden or is it embedded in one of the agents as a leader. And then as problems crop up, we will have to somehow relax constraint of our group or formation. So in case of formation, might actually break the formation, formations being stateful and then reform after this lower-level strategy is dealt with. So embrace the fact that units will overlap. Maybe have some preferred separation or physical constraint, but that'll be soft and then you're going to have something smaller that is the hard constraint. And in practice, you probably not going to be able to push the hard constraint to perfect fidelity where someone playing your game will never see one tank graphically intrude on the other. You're probably going to see that. If you played a lot of RTS-type games, you've probably seen overlap before. Understand time and the update loop, but also be prepared to think about time in the future in the form of a movement line as well and a lot of the more advanced strategies will involve that. So I guess to wrap things up, when you have this group movement especially when you get into the more elaborate strategies like formations that you see in real-time strategy games, often things get a bit more complicated and more error-prone in terms of unit movement and then the formation movement. So there's a real art to it and trial and error and tuning things and getting it right. It's a genre that has probably employed some of the most advanced AI techniques, at least related to path planning and unit movement just because of the complexities of the genre. So we'll stop there and I'll see you next time.
