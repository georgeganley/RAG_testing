>> So next up we're going to look at blended steering behaviors. We've already defined fundamental steering behaviors and from those some delegated steering behaviors. And now we can further mix and match through a blending strategy. So consider a scenario where our agent needs to avoid a danger, but it's also seeking a goal. So we might have two different steering forces that aren't in agreement with one another. So how could we deal with that? So one solution is that we could blend the steering forces additively or we could arbitrary. So we'll start first considering adding them together. So we might just add them directly. So that would just mean adding the, say, the x-y terms of the two vectors. And then you'll get a vector pointing in a new direction. And so we could just use that as our acceleration cat to our max acceleration to head to, or we might do some averaging strategy. So maybe each has a contribution that is not the total length of the vector, but maybe some fraction of the vector that's added. But in the figure shown here, we see the agent trying to get away from danger, but also trying to get towards a goal, the fish. And so the idea is that when these two vectors are combined, we might get something that's a blend of the two; something in-between. The agent heads towards that on the given simulation frame. And then the next frame we recalculate these vectors and their contributions to the blend and repeat this over and over. And the idea is that we would get a path. If you look at the whole history of these calculations for the steering behavior movement, you would end up with a path that looks like it's optimizing to avoid being near the danger but also going towards the goal. So in terms of additive strategies, well, one is we can just take the sum of the forces. We would need to impose a maximum acceleration so we add them up. We need to make sure that those don't add up greater than the maximum acceleration allowed. And that would tend to be the case because any of these steering behaviors in isolation, the agent will tend to go at full speed towards it. So we're going to be clipping the length of that vector, the resulting vector when they're added up pretty much all the time. Now we can also take a weighted sum, and this is each force generated will have associated with weight, and that's multiplied by, we don't need these to add up to one. It could be something else. You could normalize them by dividing by the sum of the weight perhaps, but that's not necessary because we can still clip them. The waiting is really more to set a relative bias. For instance, one force or one acceleration is weighted higher than another, then it will have more influence. So we might have, say, a bias towards staying away from the danger more so than seeking the goal or vice versa. And we could define steering behaviors. We could just play with those weights and get different patterns of behavior, say, personalities of different agents that might be using the same steering behaviors. But we play the weights such that one is more afraid of dangers or one is more willing to go towards the goal even in the face of danger, and we could possibly even make that dynamic perhaps tied to health. So the weights could vary according to the state of the agent. So there's problems that you've run into with blending. You don't always get the ideal scenario that was shown in the first example. You can have your forces balance out like shown in this figure. And so in this case, both the goal and the danger are in this same direction relative to the agent. So that means we have an attractive force for seeking our goal and then also this repulsive force or staying away from the danger. Those can balance out or cancel out. In this scenario, this would be an unstable equilibrium and it's like balancing a broom stick in your hand. There's going to be a tendency in an unstable equilibrium for the agent to slide just a little bit left or right, maybe related to numerical accuracy. Just a slight nudge one way or the other, can then get the forces to no longer cancel out perfectly as zero. And the agent will begin to accelerate either to the left or right depending on which way it starts to slide. Now, that's not to say that just because the agent can break out of this, that it's not a weird thing that the game player might notice that the agent does. So it's still, I would say in anomaly that would be nice if it could be avoided. But there's also a stable equilibrium. And so we can see this demonstrated in the next figure. So now we have two repulsive forces that the agent is trying to avoid the two dangers. So we've got these two separate vectors that now stabilize the potential for sliding to the left or right based on a numerical accuracy induced instability. But we still have that attractive force for the goal. And so somewhere along that line, the forces would balance out to zero. So the agent can just get stuck there at that equilibrium point indefinitely with the forces all balanced out to zero. There are some other problematic cases for steering behaviors as well. Even if the agent is not getting stuck, the agent can make poor decisions and it can be pretty much impossible to design steering behaviors alone, such that you get the desired complicated behavior you might need for your gameplay. This is especially a problem in constraint environments such as indoors or in canyons. Scenarios where you have lots of agents can also create problems. So what can happen is that all these constraints lead to lots of steering forces and this will create these failures that will impact the gameplay negatively. >> If you think about how steering behaviors work, they're going to act most strongly to local influences. And that creates this nearsightedness, so that other points of interests that might influence the agent through steering behaviors, they're not going to be able to influence these more nearsighted reactions. And I'll show you a few figures from Millington's game AI book. Here. These are figures 3, 35, 336, 337. So these scenarios demonstrate blended steering behaviors where things won't work out the way the designers of the AI would probably like. So one is pursuing an enemy, in the case of perceived, this would involve prediction, so trying to steer an aim ahead of where we're extrapolating the position of the enemy. But as that turning towards that future position occurs over the next few frames from what's depicted here. The wall gets in the way. And so the ray cast tests that would be performed will get a collision on the wall. Along the normal of that collision point on the wall, we'll define an offset for wall avoidance and the agent will need to be steering towards that. So what will happen is the agent will starting to turn towards the enemy, but then the wall suddenly gets in the way. And then turns away in a big circle and not a very efficient route. Instead, what really should happen is the agent understands that the wall is there and steers such that it can go around the wall and follow the enemy. But that would simple steering behavior, additive blending, that's not going to happen. We can also consider a scenario with a doorway. So the agent is trying to follow a target and maybe that passes through a doorway. But then as the agent, It's following the agent quickly goes to the doorway that re-cast again looking for wall collisions, gets a hit and then it turns away and is never able to turn into the door. And so we can have these different scenarios like depicted here, where the agents just not going to be able to deal with them. So what was the solution to work with steering behaviors that might include these blended steering behaviors? So really what you need is higher-level planning. This could be in the form of path planning, but probably also some form of higher-level decision-making, like a reactive decision-making strategy, such as maybe using finite state machine perhaps, or decision trees, rule-based systems. There are a number of different strategies that we will look at in the future that we could use to drive steering behaviors. And then recognize different situations where we switch to new strategies and use some new set of steering behaviors. But before we get to those different types of blending systems, we can consider a small variation to the blending that we've talked about so far. That begins to introduce some arbitration, and that is priority blending. Now this is beginning to blur the line between just our basic steering behaviors and integration of some reactive decision-making system. And so this is one that you might actually implement by leveraging something such as like a rule-based system perhaps. But the priority blending is based on the observation that many of our steering behaviors that we've seen so far, at least some of them will return a steering force only in the presence of a stimulus. So like obstacle avoidance, it doesn't return a steering force, if there's no obstacle there, there's nothing to return, so it would have no impact. It's only when the obstacle becomes imminent. In terms of collision that it didn't begins reporting this correct of force or acceleration. So the ones that tend to be variable like this, they also tend to be very important in scenarios where they trigger because of imminent threat. Now, if we just blend these all the time with other forces, like say, a goal-seeking force, the avoidance force can become diluted by the other forces, basically becoming less effective. Maybe the fact that we're blending, it's going to make it more likely that there's still a collision that occurs. So the priority blending, we can prioritize steering behaviors that activate above a certain force threshold, that this would be a threshold you set in code empirically. Or if it's anything over zero, you would maybe haven't just a small epsilon value, but it could be a lot larger value perhaps in some implementations. But you can apply this to a single staring behavior or perhaps a set. So a set meaning that a group of string behaviors that are blended, maybe additively. And so if that collection doesn't generate above the threshold, then we skip to the next set. And so you might have a rather elaborates sequence of evaluation of like set A, set B, set C, and so on. Checking to see if the sum of the forces activates above a certain threshold. If it doesn't, you go onto the next one. So the priority is implied by the order of evaluation. So the first one you evaluate is the first one that can win. Directing where the agent goes. But if it doesn't activate, then the next set has an opportunity and so on. So this can immediately avoid unstable equilibria. We saw before that with unstable equilibria, they tend to destabilize and fall apart, at least the agent being stuck, but they just still briefly impeded by this, and so priority blending can just immediately avoid this scenario by, say, prioritizing, avoiding the obstacle. So there's no opportunity for the forces to cancel each other out or balance out. You can also avoid some stable equilibria if the basin of attraction is small enough. And this is because you'll tend to have like an obstacle avoidance that will move the agent away. And then it wasn't news far enough away that it's no longer activated, then something else can be prioritized, such as going to the goal. But if the basin is huge, the agent will move away, but then head right back to the goal. So you end up in a vicious cycle. So you're not stuck at a fixed point, rather it's stuck in a pattern of movement, of going in a big circle over and over. So priority blending isn't like solution to all problems, but it does help. And you still have issues with constrained environments [NOISE]. And of course, stable equilibria where you have a large basin of attraction where it would also be an issue. But otherwise, the blending is just further empowering the strategy of steering behaviors. There's a lot that can be done with steering behaviors and where there are deficiencies. A lot of those deficiencies can be addressed with higher-level planning systems.
