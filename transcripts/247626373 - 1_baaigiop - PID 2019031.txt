>> So next up, we're going to look at delegated steering behaviors. So we're going to build off of the fundamental or basic steering behaviors we looked at last time, and create some new ones. So we previously defined seek, align, and velocity matching. Basically, any of these variable matching steering behaviors can be used to build more complicated, delegated steering behaviors. So one that we might want to develop is pursuit. And so this is like seek, but we're going to introduce prediction. So basically, just come up with a new position to seek, but it's going to be defined by where our target is moving. Say one agent chasing after another agent, or perhaps chasing the player. So this calculation needs to be made every frame, because every frame target is going to be updating and possibly moving. So we're going to need this calculation to be efficient. And therefore, approximating is probably okay as long as we're converging on the solution as we get closer. So we don't need a perfect solution. We can come up with something simpler to calculate, but it's still effective. And so the way that we could implement this, is to calculate first based on a zero acceleration or constant velocity assumption. So we're extrapolating our target's position according to its current velocity. So no acceleration, that means no changing of direction or course that is heading on. And the other thing we're going to do, is rather than calculating a precise intercept, which we could do with the constant velocity, we can figure out exactly where our agent should be aiming for an intercept. But we don't need to do that. Instead, we can use a heuristic, and that heuristic is that we're going to consider the current distance to the agent at its current location, and we're going to say that, well, that distance can be the basis for an estimate of time that it'll take to get to the target, or at least in the neighborhood of the target. And so, the way we'll do this is we'll take the length of that position vector or the distance, and we're going to divide it by the agent's maximum speed. That tells us how much time it would take to get there, approximately. Then what we're going to do is plug that approximate amount of time it'll take us to get in the neighborhood. We're going to plug that into an equation of motion for our target. That's just assuming constant velocity. So that would be future position is equal to current position plus time times velocity. Well, that will give us is an updated future estimated position of our target. And that's what we're going to aim at with seek. So it's very straightforward calculation. And again, of course this has been calculated every single frame. So we've kept things simple, straightforward to calculate as we get closer to the target, assuming as possible meaning, like our agent that we're writing the code for, can actually gain ground on our target. If that's the case, then this t value is going to get ever smaller as we get closer. So even if it's wrong in some way, and it could be, maybe the target is heading straight at us, which would mess up the t value or it's heading directly away from us, which would also cause our t value to be wrong. But it's okay to be wrong because this will still converge. Also, you can evade by fleeing from the future target. And so it can be useful to recognize where the enemy is going to be in the future. Assuming you're trying to get away, then you want to dodge that position, which would be more effective. So with this pursuit behavior, we're going to get a more effective or more efficient route over time. And so you would see something like what's depicted in this figure from Millington, in his Game AI book Figure 312, you can see that the pursuit route would not involve such a steep curve which would tend to be always approaching the moving target from behind because it's always moving to an old position. So it'll avoid that and intercept the target more quickly as opposed to just seeking the current position. Now with prediction, you need to watch out for some situations. So one is that you can end up with extreme predictions with a very large look ahead t-value that is very unlikely to be correct, because a target will tend to only be following a certain path for a short period of time. So this, you'd probably set empirically, just test things out until you come up with a good value. Don't look ahead any further than a certain amount. You could also consider looking at where the target could end up relative to what's known about the environment. This works especially well with NavMeshes. So if you know where the target is on the NavMesh, you know its current velocity, well, you can cast a ray along that current velocity. >> Based on velocity times I'II lookahead t value. That's going to find the length of the array that we're going to cast. And so that will tell us, well, if we get a hit against the edges of the navmesh, then we know that, clearly that's impossible. The target would've hit the walls of the obstacles that define the navmesh. It would've had to have changed direction. So that can lead to a strategy of maybe abandoning prediction in that case, or clipping the extrapolated future position along that cast ray so that the point of collision would be where the agent heads towards. We can also face a vector position. So this was mentioned for the steer first strategy for steering in the previous lecture. One thing I didn't mention then is that if we don't have an angle identified, we'll need to come up with an angle from a vector. And the way to do that is using the x and y components of our vector we can plug that into Atan2. And this is a special implementation of Atan or our arc tangent, rather abbreviated to Atan. But it's a special version that tracks the numerator and denominator sign to figure out what the sign should be on the angle. Otherwise, if you just were using Atan rather than Atan2, then you would just have a positive angle and you wouldn't know the proper turning angle because you lack that information and you could figure it out by analyzing the signs yourself, but it's already implemented in Atan2, it's available on almost every math library and game engine. So it's the simplest thing to do, is Atan2. Looking where your agent is going, this is a simple one to do. You just align towards the current velocity. So it's basically face, but instead of using a relative position vector, you're just using the velocity, so still using the Atan2 with the x and y-component of the agents of velocity. We also can define a better version of wander than what we saw with the kinematic wander. And the way this works is clever. You put a circle, you can't see the circle in your game, but it's a hidden or an imaginary circle in front of your agent by some offset in front, you place the center of this circle, and then the circle is defined by a certain radius. Then there is a target point. Perhaps initialize it at the furthest point on the circle away from the agent or maybe the closest. Usually you would pick one of those two locations. And then each frame, the target rotates around the circle. We could use a random function perhaps the same as from the kinematic wandering that you can refer to if you want to see that random function. And that will define how much rotation left or right occurs each frame. Then that target is used by the agent to decide where to face. And the end result is that you can't improve the frenetic changing of direction because the target has to rotate around the circle. And so even if it changes direction back and forth quickly on the circle, if it doesn't rotate very much, that's not going to have a big impact on the steering direction of the agent. So you'll tend to have these gradual meandering terms of the agent. And it looks much more natural, especially for a human NPC. This just casually walking through a town or a village. Once you've determined this target and you updated every frame with the random movement, you can apply face to get the angular acceleration to move towards facing that particular direction. And then you apply the maximum acceleration forward, so whatever the fastest that the agent can go. So this is like the term first steering behavior that we talked about in the last lecture except it's just specific to this wandering. Path following. This is another one where we had a very simple implementation described for kinematic movement. But now we can come up with a much better and smoother animated version for steering behaviors. In order to implement this well, first, we need a path. And so this might be a linear path defined by linear line segments. So we will have different endpoints for each segment that are chained together one line segment after the next. So we need to be able to find the nearest point on the path, which is at least for line segments, is finding the nearest point on a line segment for all the segments and then pick the nearest of all of those potentially in a naive implementation. We could however, improve upon this by using some space partitioning scheme and identify a neighborhood of the agent. And then select a line segment or a small number of line segments in the neighborhood and then perform the nearest point on segment tests for that subset of segments. Once we have identified that nearest point, we can pick some point a little further down the path. May be based on a velocity and time or some fixed distance. And that would actually be what we steer towards. So a lot of the complexity of this implementation really lies with how we've defined our path and the equations that we use for determining the distance to the path and the space partitioning that likely we'd want to use for anything but a simple example. >> Of course, the type of path is going to have a big impact as well. Line segments maybe are easier to work with. We've already seen a way to find the distance to a line segment in terms of computational geometry calculation. We could consider Bezier splines, which are curved paths which do tend to be popular to use in games, even though there is added complexity. Having curved paths, the aesthetics of curved motion can be beneficial so they're popular to use. That's beyond the scope of this class, the specific implementation of this nearest point on path. But just be aware that we could generalize our path, whether it's linear line segments or Bezier splines or some other type of curve that can vary. We're just seeking this target that's further down the path, and this is going to be based on how the path is oriented, the data structure, so like the sequence of connected points, so we'll know what would be the next segment in the sequence. And so we could either go in positive or negative direction depending on what makes sense for the forward direction on the path. Now, there's also a predictive path variant. And in this case, rather than find the nearest point to the agent's current position, we can extrapolate out based on where the agent is going. So some future position may be some amount of time times the velocity and we'll extrapolate along the velocity vector to come up with a new point. So that predicted future position we will then use for determining the nearest point on the path. And then once we have that spot on the path, we'll then pick a target that somewhere further down the path that's actually on the path. That will be the seek target for the agent to move to. So this predictive path following can be smoother in terms of the movement of the agent over time, or downside, and it really depends on the scenario how big a deal this downside is, is that you can end up cutting corners. So your agent might predict a future position as depicted in this figure, which is from Millington's game AI book Figure 317. And it shows how the predicted future position might cutoff a corner. That would probably not be desirable for if you want a guard to go on a particular patrol and truly visit different points along the path, or for racing games would be another one where this isn't good because you'd have an agent that cheats, so it's a compromise and you can tune this perhaps also just adjust your path to avoid these scenarios. Now, for path following, it's very useful to store the last known point on the path. This can be used in conjunction with space partitioning and perhaps even avoid some of the need for space partitioning. If you know where you were last or your agent was last on the path, then this information can help you identify where it is in the future. You can disambiguate in this situation here from Millington's book Figure 319, you can see that we have a pretzel path or the path of wines back and self-intersects. So it can be a real challenge to figure out, well what is the closest point. And so we might come up with a means of measuring the distance along the path from the previous point of different candidate closest points. And so what you actually use for the selection is the distance from the previous point to the candidate closest point. This is an example of coherence where we're tracking where the agent is on the path. We've seen coherence as being beneficial for space discretization as well. For instance, on a NavMesh, it's really useful to just track where all important game objects are that the agent needs to know about on the NavMesh. And so that way you can always find the information you need within the data structures that you're working with. Another steering behavior that we can define is separation. And so this is used to separate agents, but unlike flee, we don't want them to separate forever. This is based on some threshold distance. And then if they're separated sufficiently, the agents don't need to separate further. This is good for things like crowds simulation or flocking, especially when the agents are all moving in the same direction. This can help spread out the group. So there's a couple of different approaches we can take and calculate a strength of the separation force. We could use a linear force, in this case, separation times a threshold minus distance. And this threshold is the separation distance that we're desiring. The maximum acceleration times threshold minus distance over threshold, so we're dividing by the threshold as well. And another approach we can take is inverse-square. And this we're taking a minimum of either the maximum acceleration or coefficient k over the distance squared. So depending on which of those we use, we can come up with different behaviors, maybe where it gets much stronger with the inverse square, much stronger, really close and then decays the further you are. So it has more of smooth effects of gradual or tiny bit of pushing when they're further apart, but stronger when they're close. So it looks maybe a little bit more organic. In terms of how to calculate this one, you have lots of agents while you could just pick the closest neighbor to calculate the separation force. Or maybe you pick a number of nearby neighbors and take the sum of those forces and then of course you would clip this to the acceleration max. The sum of all those forces if it leads to acceleration stronger than you want to allow, you just clip it. Now, this needs space partitioning and coherence as well to quickly identify neighbors. So we might have a ben Q. So divide the world up into a grid Lattice, basically where each grid cell is a list of entities that are in that cell. And then when you're looking for a neighborhood, you look at the gateway connected cells, including the current cell. If our agent is in currently in a cell, we'll look at the list of entities within the current cell, and we'll look at the entities up, down, left, right, and then the four angled directions, and so that would be our neighborhood. Of course, as agents move around each frame, we need to leverage coherence to track as they move from one bin to the next, so deleting from the queue and add to another queue. We can also create an attraction steering behavior. It's the opposite of separation. It's not often useful, mainly you would use it in a flocking group behavior. We'll look at this one. We discussed group behaviors. But in this case we'll use the inverse square with a negative k or constant of decay, and so that will just make the force in opposite direction pulling together and we would probably balance that with a separation force. If it gets too far away we're pulling something in, but if it gets too close we'll push it away, and that's one of the key features of boids. We also might want our agent to perform collision avoidance steering behavior. And so this would be to avoid other agents, especially large numbers of agents moving around in the same space. So we could do so by creating a collision avoidance cone, it's like a actually a triangular wedge in front of the agent. And we're performing this test with a dot product. And so we're going to use the current direction that the agent is moving. That could be either the forward heading of the agent or if it's moving separate than the facing direction, we could use the current velocity and convert it into a unit vector and then perform a dot product with a relative position vector that's also been normalized into a unit vector. So take the dot product of those two. So against the velocity dot-product with the candidate's entities that probably another agent, that relative position vector to that entity, and if that dot-product is greater than a cone threshold, or the cone threshold is the cosine of the half angle of our cone, then that would determine that there is a collision. >> So this can work well for occasional isolated collision potentials, but it starts to fall apart when you have lots of agents. You also will see that agents tend to overreact, that's especially apparent in these two figures from the Millington Book and you'll see that based on the cones, you might have the cone angle large enough, that two agents could be oncoming, say walking down a hallway or a sidewalk, and the cone test would say that the agents are in imminent danger of a collision even though there's actually not going to be one that occurs. You can also have a scenario where two agents moving in roughly the same direction are starting to converge on in their paths and so it might not be avoidance and time and the collision actually occurs because they're not able to say accelerate away from each other with enough warning. We can improve upon the collision avoidance strategy if we add a constant velocity assumption. So in this case, we can now look for an intersection of the paths of two agents. So assuming constant velocity, so we're extrapolating along the current direction, we can do so taking into consideration the speed along that direction that they are traveling. Rather than the intersection of two rays, we will instead be looking for the points of closest approach. So that's going to be a function of not only the direction of the two agents but also their speeds. So we're going to find the time of the closest approach and then that'll allow us to identify these two separate points. So to find the t at the point of closest approach, we take the dot product of the relative positions of the two agents. So that would be like position target minus position of agent defines the d_p vector, and then we do similarly with the velocity. So the velocity target minus the velocity of the agent is going to be d_v. So we're taking the dot product of d_p times d_v and then dividing it by the magnitude of the d_v and then squaring that. So that will give us the t value. If it's a negative t value, the entities are moving away from each other and we don't have to worry about a collision. But otherwise what we want to do is find this future position for each of the two agents. And so we just use future position equals current position plus the velocity times time for both of them. So they're using their own unique current positions and their velocities. Plug that in using the t that we just solved for, and then those are the positions that we'll use for avoidance. So each agent will be avoiding the other agent's point that is derived from this closest t. In terms of implementation logic, we might need to consider scenarios where there's more than one agent that's possible to collide with, so it's best to just only use this strategy with the closest of possibly many agents. Don't try to, say, add all of the positions together to find a centroid; that will not work well, so it's best to just pick the first. The other thing is you need some special case logic to deal with a scenario where two agents are already colliding, which could possibly happen. We also might want to avoid obstacles or walls. So in those cases, we're going to cast a ray that is based on the current velocity multiplied by some lookahead amount of time. That lookahead t you would generally determine empirically, just test things out, what's appropriate. But it'll probably vary with how quickly your agent can turn or accelerate away from appending collision and so you tune it until things look good basically. But if when this ray is cast, it gets a hit, then you find the normal from that point that'll be based on the obstacle. So whatever, say in the case of a wall, the line segment that makes up that part of the wall, we can determine the normal which goes out from the surface. And then that normal is multiplied by some avoidance distance. So the normal is a unit vector, multiply it by a distance that will offset from the wall a certain amount. That is the target to seek. And so the agent will now be steering towards that avoidance position. The problem with just using a single ray from the center of the object or the agent rather, so casting a ray from that point forward along the current direction of travel, it doesn't take into account the size of the agent. And so a ray cast might miss the wall if, say, a corner of a wall is within the path. And so a way to deal with that is to cast additional rays in the form of whisker rays. And these are often fanned out at angles and they're shorter than the main ray that is cast. So the shorter rays are able to detect if your agent is starting to get too close to an obstacle and then the agent will respond accordingly by turning away from the whisker that is triggered. Unfortunately, this is susceptible to problems, most notably the corner trap. So you can have a scenario where the left or right whisker, when the ray is cast and intersection is found and it triggers a movement in the opposite direction. And what that does is then it causes the other whisker to turn into the obstacle if a corner is being approached, like shown in the figure. And so the agent will wiggle back and forth slightly, just homing in on the corner like it's an intended behavior and then just get stuck. So this is a challenge to deal with. One approach is to make a wider fan of the whiskers. Basically, that the angle between the two whiskers, if you make it wide enough, then the agent can safely turn without getting caught in this trap. But then these whiskers can end up so wide that it has trouble walking down hallways or fitting in narrow corridors, the whiskers will set off the obstacle avoidance strategy. An advanced solution is to have an adaptive fan angle, so when there's a collision that occurs the whisker start to widen the angle, and then over time they will narrow back to their steady-state. Now you could use other approaches, such as with the support of a physics engine, using the collision detection capabilities, and testing against a projected or extruded collision volume or area. We could get relatively efficient collision detection, but this will be much more computationally expensive. And then probably a bigger problem is interpreting the collision results. So if you get a collision on a particular part of this collision volume, then what's the appropriate strategy for the agent? There's not a clear cut answer and I don't think a demonstration in commercial video games that has won out as the go-to strategy. So many solutions still focus on the whiskers, probably the fixed whisker length with the dynamic or adaptive fanning out when collisions occur is probably one of the more popular strategies. We now have a library of these delegated behaviors that were built off of the fundamental behaviors. And so from a line we've got look where the agent is going and facing a target, there's also the seek or flee which allows for creation of wandering, pursuit and evasion, path following, collision avoidance, obstacle avoidance. Then there's velocity matching that can be used for the end part of arriving. And then we have the force-field approach for separation or attraction. So this allows for some pretty powerful ways of defining our agent movement, but it's still pretty simple to implement.
