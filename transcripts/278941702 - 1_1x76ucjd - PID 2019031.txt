>> Hello. In this lecture, we are going to look at Na誰ve Bayes classifiers. So this is a way to conditionally determine what an appropriate action for an agent is to take that will be informed by human provided training data. So we're able to actually learn to replicate the human decisions just by analyzing what has been done previously. So Na誰ve Bayes classifiers are simple learning strategy based on statistical analysis of reference behavior and it's built on two concepts. The first is, what is the probability of an action being taken given the current state? This might be something like a race car scenario. What's the probability of breaking if the distance to the next corner is near and the car is going fast? We might have a lot of previous examples from a human demonstrator and be able to leverage that data to make this determination and the other thing that Na誰ve Bayes classifiers relies upon is that if you have more than one feature in the current state that you're using conditions for conditional determination of the action being taken like breaking, then those are naively considered to be independent. In the example just given with the race car, you assume that the corner is near. Measurement is independent of the going fast measurement. We will need data from a human player and we'll have lots and lots of examples of different points during which the human is performing their decision-making. So in this case, one of the decisions that a human driving a car would make is when to break and so we can either be pressing the Break button or not and we have different world state that might be considered and so in this case we have distance to the next corner and speed in this particular table. Just looking at this, it's hard to interpret. You have a lot of floating point number values and many different examples. So it is hard to work with in its current form. It'd be nice to try to simplify it in some way, and that'll also help with our base classifying. One thing we could do is somewhat similar to fuzzy logic is to have categories for the distances and speeds. In this case, we would maybe pick a threshold for two different states for near and far. Any distance that's less than the threshold will be considered near and any distance greater than the threshold will be considered far. Same thing with slow and fast with the speeds. We could potentially have additional categories as well and it will really depend on the problem. How many different states that we would want for this discretized representation. But if we can make it so that it's easy for humans to understand, then it'll probably also work well for the Na誰ve Bayes classifiers. Once we have this discretized form, the next thing is we want to define our conditional probabilities. And so what we need is the probability of A given B. And so the syntax we use is the P function and then we'll have A given B defined by the vertical bar or pipe. A pipe B means probability of A given B when it's within a pass as a parameter to the P probability function. As an example, we might be interested in the probability of breaking being selected given a particular distance and speed. Now since we have this discretized form, the distance and speed provided can be discretized and we'll actually have examples of it. But in the floating point form, we did not have necessarily the possibility of finding a match. In fact, it would be very unlikely to ever find a exact match in the table and even if you did find a match, you probably wouldn't have any statistically meaningful information about it. You need lots of examples of your data points in order to be able to come up with a probability. So by discretizing, we're increasing the likelihood of particular examples fitting into these bins and therefore, it makes it easier to come up with our probabilities. So next step we're going to apply the Bayes rule and so this allows us to rewrite our probability of A given B as the probability of B given A times the probability of A over the probability of B and we can restate this, an even simpler form, which is going to help make things easier once we start to analyze the human provided data in the table. We're going to get rid of the denominator and we'll actually hide it away. We're going to create a variable A and assign it to 1 over the probability of B. So now our Bayes rule applied, we just have alpha times probability of B given A times probability of A. >> So now we can consider some different scenarios where you might be interested in the data in our racing example. So you might have the desire to determine the probability of breaking given the distance and speed. So we can apply the Bayes rule. The probability of breaking given distance and speed can be rewritten as the probability of the distance and speed given breaking time is the probability of breaking. And if we further apply the naive assumption of conditional independence, that means we can break apart the distance and speed into two separate probabilities. And so the probability of distance and speed given breaking becomes the probability of distance given breaking times the probability of speed given breaking. So if you use the Bayes rule and the naive conditional independence rule, you end up with Alpha times the probability of distance given breaking times the probability of speed given breaking times the probability of breaking. This is really, it's just moving things around, just leveraging some of the principles of probabilities. And the reason why we want to go to all this trouble is because we want to make it easy to work with the example data we have from our human reference player. Let's say we have a particular example. So we've got our table of data, it's been discretized. And then during the game, we have some example state of the game. And the agent needs to make a decision about whether or not to break. So we want to know, given the distance and speed of the current situation, should the agent break? That's the question. So the first thing we need to do is discretize this world state that is relevant to this situation, which is the distance and speed we want to discretize those. But before we can then analyze the data in the table. So in this case our distance is 792, so that's going to be far and speed of 12, That's going to be slow. So now, what we can do is using the discretized terms, we can apply the Bayes rule and the naive conditional independence. Now it gives us Alpha times the probability of being far given that breaking was observed, the probability of going slow given that breaking was observed, and the probability of breaking. So now we can look at our table. And again, since we have everything rewritten in this way, we can easily look up this information so we can find the examples where distance is far and breaking is a y for yes. And so we can just count these examples up pretty easily. And so in this case we have two examples. And we can also see for slow the probability of breaking and slow, we see that there are two examples. For both of these, they're out of five examples of breaking. So for far, that is two examples of far given breaking is a two out of five, and also out of five for slow. Then we additionally need the probability of breaking. And so we have seven total examples of breaking or not breaking, but five of those are breaking, so five out of seven. So this allows us to just multiply out these three different terms. So 2/5 times 2/5 times 5/7. That gives 20/175, which we can simplify to 4/35. And, yeah, we also had the Alpha, sorry, I had to do a little jump cut edit of the slide. So we could also look at the probability of breaking, being off or no instead of yes, but with the same conditions for far and slow. And so this is important in order to actually determine what an action is. So we want to know whether or not to break. So you might look at the breaking examples, yes, for far and slow and the breaking samples, no, for far and slow. And so you want to pick the bigger probability of the two. And so if you go through the same process of calculating, you'll end up with 1/2 for far, 1/2 for slow, again, those are for both with breaking as no and then the probability of breaking being no is going to be 2/7. So what you end up with is 2/28 or 1/14. So we still got these Alphas, so we got the Alpha times 4/35 and the Alpha times 1/14. It turns out that, that Alpha we don't even have to worry about because we're making a comparison of these two values. And since we know Alpha has to be positive, then it could just as well be a one. And we would still have the same relative comparison here in terms of which probability is greater. So we might pick to break based on the data and making this comparison. So this fundamental approach can be used to drive selections of different actions so that you could implement an agent that just analyzes current conditions and selects different actions. And is this flat rule approach, so you just look at what the current conditions are. You discretize the same way that you discretize your human-provided data. And then just each time the agent gets an opportunity to make a decision, you just pick the greater probability. We could also use this to inform decision-making with other strategies we've already seen before. So if you say had a human make determinations when to switch between different states and a finite state machine. You think of it as selecting different high-level strategies as conditions change. That might be how the agent determines what are the best transitions to make when. We might also use this with fuzzy logic rules where we're already needing to create these different discretize state representations for different input values so that there's already a bit of similarity with fuzzy logic from that perspective. >> So what we could do is just consider all combinations of fuzzy inputs and outputs. And so we'd have a very large set of candidate rules. We wouldn't want all of those candidate rules because some of them would be at odds, so for instance, like breaking based on the distance and speed. We wouldn't want to have simultaneously break if the distance is far and the speed is low or breaking and not breaking. So we want to pick one or the other. But we can have all of these be candidates and then we can look at the human data and so that would decide, well, which rules would be present in our set of rules. So we just need to work out in advance, what's the structure of our rules going to be. So for instance, if we're going to have compound Boolean like And, then we want to just go ahead and set up that structure and know what inputs are being used for our particular output rules, and then you would need to figure out how to deal with the fact that fuzzy logic will tend to have degree of membership functions that overlap. So in this case we need to pick one way or another. So having overlapping degree of membership isn't going to work here. We need to just pick one or the other for each of these human examples, whether we're going to classify say, a distance as being near or far, or a speed as being low or high. So in this case, as we're applying the degree of membership function or discretization purposes, when we do have overlap, we just pick the larger one for using the Naive Bayes. So if you have, let's say, a speed this falling in the middle of low and high speed, even though they're overlapping, you're probably going to have a degree of membership that's higher for, say, slow than the first value. And so you just say, okay, well, this particular human example in our table, we're going to say that that is slow because of the higher degree of membership. And so you could use that for determining what the actual rules are going to be present in your set of fuzzy rules. After that, the fuzzy logic runs as before. So there might be benefit of using, say, this type of a strategy as opposed to just using the simplest form of the Naive Bayes Classifier because with the Naive Bayes Classifier, you're just going to be jumping from one decision to the next based on the current condition. So you're going to have this hard transitions. Whatever was the most likely choice by a human given the current circumstances, you might have like this hard threshold that you're transitioning between. So there could be some benefit in smoothing things. So fuzzy logic would be one way to do that. You could also just do a more straightforward smoothing of the selected actions just straight from the Naive Bayes classifier, but that is something you might have to deal with regardless in some form of smoothing. This isn't specifically the Naive Bayes classification, but it is leveraging some of the things we've talked about with the Bayes probabilities is a Bayesian Network. And so this can be useful in situations where you have these cause and effect dependencies between game phenomenon. This could especially be the case in situations where you have incomplete information. So the agent might observe something that a human player that's battling against the agent is doing. The army that the human is controlling will occasionally interact with the agent and so the agent will be able to observe it, but can only observe where they have units or structures like buildings. So anywhere out of range of sight of the units that the agent controls is unknown. This concept's called fog of war. In fact, in many video games that are real-time strategy or turn-based strategy, they literally have a fog that is covering parts of the map and that depicts where you cannot see. So the agent doesn't know what the human player is doing except what they can see. So let's say that the agent is attacked by a few archers. So the agent might be able to infer that, well, if their archers are present, then that means that there's an archery range present. And therefore that means that the opponent could also train crossbow men and maybe they're especially dangerous against certain units or resources that the agent has and would need to respond accordingly with a strategy, some protection, build some special armor or protective structures. Now in this case, that was a very trivial example based on the figure shown here on the table. But you might have much more complicated structure between these dependencies and knowing that certain buildings or units are observed, but having an incomplete picture of all of the assets of the opponent, the agent may be able to infer something by applying these Bayesian probabilities according to these dependencies. So certain observations, you can apply the probability of the A given B. And so the given are the observations. So what's the likelihood of certain structures existing or that certain types of attacks might occur. And these could be strategies that the agent would be especially vulnerable to. Let's say the player, the opponent is close to the point where they can build aerial units, maybe like blimps, and then drop firebombs down and right now, the agent has no protection against aerial units. But they see some hint that maybe the opponent is close to being able to build that, if not already. And so once they get that little hint that you can analyze it through the Bayesian network and then up the priority on building certain types of defenses where otherwise it's not worth wasting the resources to build that unless it seems likely. So you can also apply the Bayesian probabilities for this scenario and build a network and these dependencies and probabilities that the agent can learn. It's a little bit debatable, I guess, how useful this is because in many cases, the AI always has access to the full game state. So why would the AI not just be able to directly access memory and see whether or not the player has something or not. So this gets a little into the issue of fairness and whether it's worth the effort to implement something complicated like this just to try to make it as authentic decision-making as possible versus maybe just doing some cheating, but dialing it back a little so it's not too difficult or too obvious. But hopefully you see some of the benefit of some of the simple, straightforward conditional statistics learning these probabilities and then being able to apply them to decision-making in your agents.
