>> In this video, we are going to wrap up looking at steering behaviors. We are going to first consider some applications of steering behaviors and then conclude with a summary. So first of all, I want to discuss creating unique agents. So we've seen all these different ways that we can apply steering behaviors to create different compelling patterns of movement for an agent. So in terms of creating a unique agent, of course, we can select appropriate behaviors for the particular activity of the agent, and maybe to further make behaviors unique, we might blend in steering forces that are not necessarily related to efficiently performing an activity, but maybe something to define character or just look unique in terms of motion or movement. So for instance, we could add just a slight perturbation based on a wandering. So the agent could be seeking a goal, but we add something that causes them to move just slightly. Instead that maybe that would make it look like more of a predator type of creature if that is what we are going for. So there is different things we could do in terms of blending. We can also change parameters for how far ahead we were predicting, how obstacle avoidance has handled the weights if we are doing blended steering behaviors, what are the actual weights that we are using. Maybe priorities if we are using some kind of arbitration scheme. But we also have control over the performance envelope of the agent. Especially if we are directly implementing the agents. So in that case, we can set unique maximum velocity or angular velocities. Same with acceleration, we can set. Or if we're using a fancy animation system with root motion, of course, the animations that we use, that will create unique movement for the character. So even though a lot of the underlying algorithms are the same maybe across many different agents, we still have a lot of freedom. There is a lot of flexibility in how we mix and match the blending and how we define the performance envelope of the agents to get unique behaviors, and therefore, this allows us to apply these techniques to lots of different types of games. A little bit more about selecting steering behaviors is that it's also important to realize how you can apply steering behaviors, leveraging the rules of the game. So for instance, let us say we have a game where the human player is the prey and they are trying to get to the bag of gold, and then we have an agent, the predator that guards the gold. So in this case, the steering behavior would be, maybe we tried to determine a midpoint between the prey and the gold, and so that is a very simple exercise. We know how to define relative position vectors, and we can just find a point halfway along the relative position between the prey and the gold, and then we have the agent always trying to move to that midpoint. So basically, the predator would be guarding the gold, making it more difficult for the player to achieve that goal. Something else I wanted to mention about steering behaviors is that one of the techniques of steering behaviors were originally designed and developed by Craig Reynolds. And he wrote some different papers. One I have got a link to here for a web version from Game Developer Conference in 1999 that you can check out. There is also a website that has a lot of details about his work. And he implemented a lot of his work in a software package written in C called OpenSteer. And OpenSteer has been around for a long time, it's a very popular project to port to different languages. So if you are not interested in the C code, you can find it ported to Java, C-sharp, I think even some of the code has been ported to unity somewhere. I have seen it before, I think it is in JavaScript. All sorts of different implementations of the OpenSteer project, and there is demonstrations of all of the steering behaviors that we've talked about pretty much that you might find interesting. So I'm going to demonstrate this code real quick. Let me set this up. Switch out from PowerPoint over to Opensteer. So this is just a compiled version of the OpenSteer project. It is written in OpenGL using glut, which is simple wrapper so that you can run a windowed OpenGL. So the graphics are very primitive, but it does a good job of demonstrating. Now, one thing interesting about these examples is that there is no physics engine, collisions are not enforced. Everything that you see is just based on the AI behaviors. This first example that I'm showing, this is capture the flag. The little bluish-purplish triangle there that's staying centered in the screen, that is the flag seeker, and then there are the guards which are those pinkish triangles. So they're chasing the flag seeker. And if they tag, then they win and the trial is over, and it restarts, so the flag is in that white circle. It looks like we may have lucked out and entered an orbit. So the flag seeker is seeking the goal while being chased by the guards. There is no collision detection enforced with the guards and there is no separation force either. So actually, all the guards are currently superimposed on one another. But we've talked about the orbit problem before. There is no code in this example for capture the flag to deal with the orbit problem. I think there is a slight processing around. It is hard to see. But because of the fact that the flag seeker wasn't able to take a direct line on the flag, and that was because it was evading at that point with the guards, that when it did finally re-enter the flag seeking exclusively, it was within the turn radius. So this just demonstrates how easy this problem is. I think I can reset with c if r is the right key. No, maybe I don't have focus here. I guess, it's correct. So I'll restart our session here. So the hollow circles, those are obstacles, and if the flag seek here gets too close to an obstacle, it will steer around. Same for the guards, they will also attempt to avoid. Now, they are not always successful, if you watch long enough eventually either the flag seeker or the guards will cross into the obstacles. Here is a case where the flag was successfully grabbed. Now, the strategy is that the flag seeker users are selected by a state machine. So there is arbitration in the form of a simple state machine. You can see the states written in text over the agency. You see seeking evade, that was a blended steering behavior at that point, and then there is a state for seek goal, and in terms of deciding how long does the green outline shows a corridor. See if that green corridor or pops up again here in a second. But the green corridor is checked to see if there is any guards within it, and if a guard show up within there, it will switch to seek and evade, or perhaps exclusively evade. So I think we are in an orbit again. But yeah, it is pretty interesting. Of course, you can check out all the code to this example. There is a few other examples as well. Let me switch here. If you hit Tab, you can go to the next example, and this one is a group behavior. Pedestrians like NPCs walking on sidewalks, perhaps. And so the red lines are the paths to follow. They are trying to avoid running into each other and there is also obstacle avoidance. >> And you can see all sorts of different visualizations that reflect the different strategies in calculations that are taking place. And so these are similar to the cone strategy we saw before, so checking the angle. I think that is one of the approaches that are used. And then I think there's a different approach to use for the obstacle avoidance that occasionally there are the obstacles, the fixed-position obstacles. But this is interesting to see. And of course, this is all running very efficiently with a large number of pedestrians. In the top left, if you can read the text in the video, there are some options there, where you hit "Function" keys to change the crowd size, for instance. Let's see if I can increase real quick just to see what happens here. I'll hold down. So this was written many years ago, so it should be able to handle quite a number of pedestrians, one from 100 to almost 400 here, so it's quite a mess. Still running pretty efficiently. And also, there is a setting for using a bin lattice. And so that is a space partitioning scheme and all of these agents they belong to different bins in this bin lattice. And they're transitioning from one bin to the next. And then when they're checking for neighbors to avoid, it's looking in that neighborhood. So you might recall from previous lecture videos, you look at all the adjacent cells if you're using the bin lattice strategy. This is now quite the mess, but it's still working. Maybe, as you look at the video, look for agents to completely run over one another, which is happening quite a bit here now. Because again, physics is not being enforced. All of the avoidance that's occurring is just based on the AI strategies alone but of course that can fail; the steering behaviors don't have guaranteed outcomes. I'm going to can skip to the next one. This is an example of boids. So the boids are like school of fish or a flock of birds. They like their friends. And they identify friends by someone going in the same direction as them and the same speed. And if they find a friend, they will be attracted but if they get too close, they'll separate. And if you let this example run long enough, they all group up together as they move around. So I just increased a bunch more boids and you can see it's still running efficiently. This is also using a bin lattice. And in this case, I believe it's now a 3D bin lattice instead of 2D because the boids are moving around in 3D like swimming through water or flying around. But we'll talk more about boids when we get to group behavior as a topic for lecture. And let's see, I will hit "Tab" again. And some of these are really more demonstrating smoothly animating the animation parameters or the position according to velocity and acceleration. Now this example is a strange one. There is a seeker that wants to eat all the other agents and then those agents themselves want to be eaten. So it's really just demonstrating the prediction capability that you can use with steering behaviors. But it's really strange in terms of scenario that you might ever want in a game. I don't think it's really realistic for a video game rather than just demonstrating some of the algorithms. This next one is showing the smooth animation of turning. Another one with the turning. Again, just showing how smoothly the update of the steering behaviors is over frame to frame. And this one's interesting, it's like a slow turning tank and it's using a more advanced form of collision detection for obstacle avoidance than what we've discussed so far. Instead of casting a ray, this agent casts arcs based on the current term rate, so that is more accurate. That more accurately represents the current path of the agent or the tank in this case. And those act like whiskers. They're tested for intersections with obstacles. And then the agent can decide based on the distance to impact, whether it needs to slow down in order to make a tighter turning radius or to speed up. And it's not perfect. You'll often see this vehicle will crash. Maybe it'll crash right here, I think. Just barely, surprised it survived that one. But this is pretty neat example. I like how it uses the curved rays rather than linear. And this works well when you have such a slow turn turning rate, and therefore a very large turning radius. It's more important to be very precise about future predicted collisions. Now I can skip to the next one. So this is the last example. This is a way to test out robot soccer. So you've probably heard of the competition where the robots play a soccer game. I think it's usually once a year. This was a starter for early versions of the robot soccer competition. And this example is really pretty primitive, but you could use it as a starting point for prototyping in software. And so you'll see that these agents, they have like little simple state machines that if the ball is close, which is the green triangle, the agents will go try to just bump into it with little thought of where it's going to go. But if the bug is too far away, they go back to their home positions. They often score goals on their own team rather than the opponent. So this open stair software package is really interesting to check out. It's helpful for understanding steering behavior implementation. And I said before, you can often find it implemented in your favorite engine and maybe even for your game engine if you're using a game engine. So let me go back to the slides, last slide here. Just to summarize steering behaviors. So steering behavior is a simple but powerful technique for implementing real-time agent movement. It's especially good for handling local movement. It is, however, susceptible to getting stuck. That your agent who are using steering behaviors is susceptible to getting stuck in equilibrium or not performing behavior with a desired outcome based on especially constrained environments that we saw examples of before. More advanced agents that rely on steering behaviors will also need support of a higher-level planning system. So really, you can only get so far with steering behaviors. You often will need to couple you're steering behaviors with something else like a finite state machine, perhaps. And so that is the direction we'll be going. So we'll need that reactive decision-making coupled with a steering behaviors, and then probably also leveraging the path planning strategies that we've already looked at.
