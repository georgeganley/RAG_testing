>> Hello, in this lecture we're going to look at object placement for procedural content generation. So we've previously seen how to create a terrain and so we might want to automatically place objects on that terrain such as trees, or bushes, and rocks, and so on. So we will need to come up with a way to do this such that it is aesthetically pleasing. That we don't have objects that are overlapping when we place them, that we can control the frequency in which they're placed, perhaps based on some desire to have maybe lots of a certain object in one part of the world, and then fewer as you had it in a certain direction. So like maybe a gradient of the frequency that they occur. So we're going to look at a few different ways that we can do this. So probably the simplest approach we could take to placing objects. Well, we've already learned about random number generation, how we can create a seed. And from that seed generate values, and for multi-dimensional values like a 2D position, let's say. We could just call the random number generation twice, get two separate floating point values. And immediately we have a spot that we can place our object, and then repeat that as necessary to place lots of objects. And if we do that, it'll work we'll get a spread of objects placed randomly. But there is a tendency for there to be clumping. And this appears very unnatural when you're like, say, placing trees, or some other natural features. It doesn't look natural when you place things in this way. There's a tendency to have an almost uniform separation, like a grid, but the average separation tends to be pretty consistent with natural features. Like for a given density, you tend to, like trees in a forest all tend to be a certain distance apart even if it's a very dense forest. And then if you have features that are more sparse, they still tend to be spread out pretty well. We would like to come up with a way to sort of create that sort of separation. One popular technique is to use a Halton sequence. Halton sequence relies on leveraging co-prime numbers. And you do this to generate a fractional sequence of numbers. And this is repeated over many, many steps. So the idea is you might have a number for the X term in a 2D number, and then another number for the Y term. And so that'll be the base for the generation. These two numbers would be co-prime, and that would mean that the common divisor between the two numbers is one, there's no other common divisors between the two numbers in order to be co-prime. So by leveraging this, you can come up with a repeating sequence of numbers to generate a lot of values, say, over a 2D space. And it will create a generally nice patterns that have pretty good separation, avoiding a lot of the clumping problems you would get if you were instead just directly calling random number generation. So the way it works is you pick a controlling number, like you'd pick one of your the numbers. And so this would be maybe an example for say like the X dimension of a 2D position for placing objects. We start with that number, and you then generate fractions within the range Zero to one, where N is the denominator. You find all of the values going up in integer sequence. For the numerator order, all the values that works. So you trial, for instance, one over three, then two over three. And then by the time you're at a three over three, you are at one. So that value won't count. For three, when we first start off, the only valid numbers within that range is one over three, and two over three. So then what you do is you go to a now a smaller fraction by taking your controlling number N to the power of two, so it is one over nine. So now what you're going to be doing is you're starting with one over nine, but you're still counting by a third. So you're going to just add that to the sequence so far. So that currently we have one over three, and two over three, then we're going to have one over nine. Then counting by thirds, we would then have four over nine, then seven over nine. We can't add a third to seven over nine, and stay within the Zero to one range. So we've run out of that sequence, but we're not done with the one over nine, or with the nine version of the controlling number yet. So we're next going to try to two over nine. You start with two over nine plus, so that'll be the next in sequence. Then you add a third. You end up with five over nine out of 30, again, eight over nine. And then you continue on, but you always skip over any values you've seen before. So when you get to three over nine, that's actually one third. We've already visited one third before. We would skip that, and then go to wherever the next value is that hasn't been RD in the sequence. And so you just keep continuing that process going to the smaller, and smaller fractions. So it's is a little confusing describing it as this example, but the algorithm for the implementation is much more straightforward. So here is the code. For just the one-dimension is on the left what was just described. The algorithm, you can generate the values with that little bit of code. And then of course to do the 2D, you need well to separate co-prime bases, or controlling values. And then you need to know where in the sequence it is, which is the index. So the more values you would need, the index would be increasing. So you just keep grabbing one after the other, generating these values. And so this can be pretty useful for creating these patterns that are good for procedural content generation, and low overhead. Now, if you get to where you're using large control numbers, that can result in artifacts, and that's especially the case in the first few numbers. So if you have like say one over 13, and then you're counting up in this first first few values, you will have some artifacts in terms of the placement. So it's typical to skip over those values, and possibly even beyond that. It'll generally settle down to the visual patterns that you see will settle down if you skip over certain values. And in some cases you might even count by K. So like if you say that the index value you would skip ahead by this value K, where K is also co-prime with the n-value. Mellington says in his book that is rarely needed to do the skipping by K, because often you'll just be using a small value for your controlling values, for your X, and your Y value will be pretty small, and you can get variety just by simply starting with some different index. So you might jump ahead by a 100, or something like that. And then that will allow you to get some variety. Halton sequences is neither random, nor pseudo-random. It's actually considered quasi-random. And this is because it's the same every time. It's not seated. You simply get different results through the control values in the offset. It does have some characteristics of like a random distribution in terms of the patterns of values that occur, and that can be desirable in some use cases. It's not actually a random number generator, but it gives you information that sort of has a flavor of randomness in terms of the distribution. >> Now another approach that can be used for breaking up the clumping that you can see with object placement is to use a Poisson distribution with a grid lattice. So if you have a space that's subdivided into, say grid cells based on the desired density of object placements that you want to generate, you can figure out, well, what's the number of objects that should go per cell to achieve the desired density for the overall area? So if you have that situation, you can just say, we'll go and iterate through each cell and then within that cell, use the convention of random number generation to come up with your x and your y. And then if your target was two, you place your two points within the cell and then you go to the next cell and place two more, and then just keep going through every single cell placing your two points. So that's the naive approach to this strategy. It will give improved results in terms of limiting the clumping that you'll see, especially with more sparse numbers of [NOISE] features place. It'll get them spread out in a fairly uniform way. The problem with this approach is when every cell is guaranteed to have the same number of points, it creates a bias towards the edges of the cells. So when you have adjacent cells, there's going to be a tendency to have more points close to the common edge of adjacent cells than the interior of the cells. Like to say, if this is a video game, those playing your game will notice that there's this weird change in density near someplace in the environment. And they'd probably be able to see basically where this grid structure was in the game like your trees or your grass or whatever, they would see this weird change in density. So the solution to this problem to get rid of what you see near these boundaries is to vary how many points there are placed in each cell. And the way to do that is to use a Poisson distribution instead. So rather than say that every single cell has exactly the same number, instead that number we're going to say is the expected number of points or the average or mean. So we might say that on average there's two or four or whatever the number is, points per cell, but the reality is, it can vary. So even if the average is four, there's a chance that there'll be 0, or 1, or 2, or 3, or 5, and so on. And so if we assume that this is a Poisson distribution, then we can use the equation shown in the slide that will tell us the probability that the given number of points k is what actually occurs. So if we're trying to figure out, while the average is four, what is the possibility that we actually get five points in the cell? Or we can just plug five in for k. Lambda is our expected value of four and then that's all we need. So it's Lambda to the k power, e to the negative Lambda power, and this is over k factorial. So you can plug this in for k equals 0, k equals 1, 2, and so on and you could keep on going forever. With a Poisson distribution, there's a greater than zero chance to infinity, but it becomes incredibly small chance to go much further than the mean. So what you actually want to do is only go until the probability of some value m is below a threshold. So rather than calculate all possible values to infinity, we're going to stop at some point and say, well, we're just never going to even consider the possibility of these extreme numbers of values. So you can replace that particular value of p of m with 1 minus all of the values that you've seen so far. So you have your p of 0, p of 1, p of 2, p of 3, and so on up to p of m minus 1. Now if that p of m is very small, then 1 minus everything up to right before it, you probably going to end up with a value that's like a fraction of 1. It's going to be probably pretty small, but we're cheating a little bit. We're saying the p of m value is slightly more likely than is actually the case. So all of these values together add up to 1. So the p of 0 plus p of 1 plus p of 2, and so on all the way up to p of our special version of p of m that catches the whole tail of the distribution. All of that will add up to 1. So what you do is you take all those values that you just calculated and this generates some threshold. So it's like a two-part threshold that we have and it breaks up the range of 0-1 in floating point. For the possibility of there being zero points would be from zero to whatever p of 0 was. So that is a range of values. And then we have from p of 0 to p of 0 plus 1, and then p of 0 plus 1 to p of 0 plus p of 1 plus p of 2. So each of these will be these increasing values in the range of 0-1. And what we can do once we have this list of threshold ranges is we can generate a random number with a pseudorandom number generation algorithm and then we can find, well, which of these m different ranges , where does it fall? Are random numbers generated in the range 0-1? And we know all of our test thresholds are in the 0-1 range. So we'll find a particular level in this threshold where the number we generated fits. And so that determines how many points we generate in our cell. So once you've gone through all this trouble and set up these thresholds, you can quickly determine how many points to generate in each cell. And so if you go back, look at this figure here. So when we would generate our points to go in a cell, each cell has the potential to be different. It'll tend to be right around whatever we set our Lambda to, whatever that value is, and that's the expected or the average amount that we believe to be in each cell or want to be in each cell. But the fact that we're using the Poisson distribution, these variations means that we're relaxing some of the pressure like statistical likelihood of there being too much information along these edges. So even though we're generating in this discretized form, we'll minimize the potential for any artifacts to show up because of that. >> So the next thing we can consider is so far we've only been talking about points and not considering that many of the features we place we really want to consider the area or the volume that they take up. So for instance, if you're placing a tree in the game world, you don't want to place another tree right up against that tree. So we've discussed a little bit about avoiding clumping, but there's still not a guarantee with these approaches so far that we won't occasionally get to trees that are right on, almost on top of one another. So we would like to be able to offset at least by some Collider radius. And that's typical in games is to define the shapes of objects. Even 3D objects tend to have a circular offset defined by like a cylinder or a vertically oriented capsule. And so we might want to say that a tree is in terms of being placed on our terrain height map, that they're separated by a radius and that defines a circle around each tree. So that might be the extent of the branches or the trunk of the tree. So we don't want the trees to get any closer than that radius. So right now we don't have a way to deal with that. So one thing we might consider is to use Perlin noise for feature placement. And this is something that you might consider if you've already implemented Perlin noise for say, terrain generation and have some ability to maybe manipulate the noise with maybe shaping or mapping functions and other hierarchical rules that we've looked at before. So it's a bit of having a hammer and looking for a nail if you already have something that works that you might save time, perhaps just use that for our feature placement. In fact, we already have a bit of feature placement in terms of the mapping functions can create feature like manipulations of the terrain height map. So I've got a picture here. This is one of my terrains I generated where I'm placing lots of little craters and rocks in the environment. And you can see that these are not infinitely small points in there but they are all well spread out. And that's based on setting a scalar for the Perlin noise. So we can just zoom in or out on the noise and we know that because it's gradient noise is frequency limited so that we can expect only to see the frequency information like periodically over the dimensions of our terrain, which is going to be based on whatever the scalar is set to. If you're using a shaping function to take lower frequency information and then create higher frequency information at the minima or the maxima then we can have some guarantee of separation between these objects. So that works well for these terrain features and we might even use it for other object placement like separate 3D models like trees that we would want to drop in the train. You could potentially use Perlin noise for that. You just have to determine a point measurement of where these minima and maxima are. So you could write a mapping function that would just give you a single point. And so we could just collect the points from the mapping function. You could do this with image processing techniques. So this would be perhaps some thresholding and then perform what's called an erosion. So once you've thresholded you'll have lots of little circles or blob like shapes from the Perlin noise and then the erosion will shrink down those blobs over a few different passes. It's a fairly efficient operation, but you can shrink down these thresholded blobs down to single points. And then it's just a matter of walking through that. It's basically an image, you walk through the image and then according to the coordinates where you find these isolated points, then that would map to where you place your features. So I don't believe that that's typical in uses of procedural content generation object placement to use Perlin noise in that fashion. I think certainly that would be something that would work and probably much more common especially looking at various books and documentation about procedural content generation for object placement is Poisson disk. So this is a fairly simple algorithm that considers placing objects in an environment taking into account that they take up space, so like an area. So it approximates the objects as being circles. And so you start with say, the space that you're going to be placing your discs or your circles. You either start from the middle of the area or pick a random point and then you place the disk. And then from that disk you start picking random directions offset from the current disk and placing new disks. And so each new disk that is placed is added to an open set of disks that we might visit later. And so we keep iterating on the current disk trying to place these offset disc. And again, they'll be limited range. So the center of the next disk can be no further than two times r plus k, where r is the radius of all of the disks that are being placed. So a distance of two r would be the closest that two disk could ever be to one another in terms of measuring from their center points 2r would just be half the width of one disk and then half the other to get from one center point to the next. Now we can also add this k parameter, so 2r plus k. And that is like a density controlling parameter. So if we have a large K, then we can place the disks more sparsely. So you keep placing these disks. And once you've exhausted all the disks that can be placed, you set that the current disk to no longer being in the open set and then you randomly select another disk and go through this same pattern of trying to generate new disks. And you just keep working your way through until you've exhausted all of the open set of disks. So it's really pretty straightforward algorithm in most of the challenge of implementing this is the test for overlap. And so in order to streamline is often coupled with some space partitioning, like breaking the world into a grid lattice and then determining neighboring cells. So a circle that overlaps with particular grid lattice cells will mark them as already being occupied. So that can streamline the testing. So you're not having to do this complicated test. >> And you might want to be placing objects with variable density too. So not just the fact that we're trying to avoid overlap, but we also want to have varying object placement according to say, a density map. And this density map, it might come from a noise source like Perlin noise. So perhaps the Perlin noise would provide areas of the map that are, what we want lots of features placed but then there'd be parts of the Perlin noise that are a low value and we would want fewer features there, so we have this range of density. Additionally, you can do this as just a man-made process or handmade process, where someone just paint the features onto a density map, maybe like a spray paint type tool. So this would be similar to how objects get placed in the Unity Terrain editor. So this density map once you've got it either generated or handmade, you can then use that to be an input to whatever algorithm we're going to use for our object or feature placement. So you'll look at the density map and decide whether a feature goes there or not according to matching with the desired density. So a value of zero would be like for replacing trees, that would clearly be no trees where the zero is. And a value that's somewhere between zero and one probably have some intuition that intuitively that means that as you go from 0-1, we're going to get to more and more trees that are placed. And a value of one is maximum density of trees. So there's this issue of well, what does maximum density mean when we're talking about trees? And we'll go back to that in a couple of slides here. We know that we're representing our objects like our trees with the circle colliders and we know that it can't get any closer than their radius to other objects, like with the Poisson disk. In fact, we could use Poisson disk for this purpose of a variable placement, we just need to consider, as we're selecting an active disk and placing these new disks, well, we just need to figure out, well, what is the density in that area, and that we could map to the offset radius, like adjusting the k value or the overall where the radius of how far out are we looking for placing these disc. And so that will allow us to attenuate the density of placement just by varying that value. Now the downside with a Poisson disk is that it's not necessarily good at dense packing. So if you're especially concerned with the nature of whatever content you're placing that you want it to be packed especially dense, then the Poisson disk algorithm, just how it selects randomly through random number generation and places features, you can end up with some poor choices for placement that aren't very densely packed but don't leave room for additional disks. And if the algorithm was more aware it could have left room for maximum packing. And so this is shown in the figure here with the blue disk say maybe these are the disk that had been placed so far with the active yellow disc. And you can see that there's two spots that could almost hold another disk, but they've already been cut off, there's these partial gaps that, if we had just slid one of the blue disk a little lower, perhaps it would have left room for an additional. And so you can have these problems with the Poisson disk approach and that means that we can still attenuate, we can still adjust the density by mapping this density parameter to the k or the offset radius. We might not get as dense as possible for a full density value of one. Now another thing we can consider with Poisson disk is tiling. So it's pretty common in games with procedural content generation that you don't want to generate the entire world, you want it to go on indefinitely. And the way to deal with that often is a tiling scheme. So say the player is moving through an environment, he might walk for a certain while, and they're going far enough that new content needs to be generated. And so this can be done through a creating a discretized space like a grid lattice structure. As you approach the edge of the generated content, you might generate new content and perhaps unload content you're getting further away from. So as you get close to the edge of the map, your load in new tiles and as you get far from other edges, those tiles start to be unloaded. And if you do this right, just the nature of our random number generation that is seated, we can recover that information again, we can regenerate the procedural content just by recognizing where we are overall relative to these discrete tiles. So we can load in tiles that we visited before and they will be recognizable at least in terms of the terrain features and hopefully also the object placement and other aspects of gameplay as well. >> If we were to use Poisson Disk, we would like to be able to tile. So this is not something mentioned in a Millington's book. I think it is covered in some other places. There's probably a number of different strategies to deal with this. And it is something I think that is pretty important for a lot of games. And so this is just something that I considered would be a good solution. There's probably some others out there as well. But one approach I think that you could take is for each tile that is loaded, we can figure out a contracted boundary. It's like an expanded geometry that we've discussed in previous lectures, but we're actually offsetting towards the interior of our tile. And so if we have that offset b, the radius of our disks, then we can guarantee that any disk we place remain entirely within the bounds of the tile. Now, if we only stopped there, we would be able to easily load new tiles and we wouldn't have to worry about any, say, trees that are generated, overlapping with trees of the adjacent tile. So that would be a good thing. But let's say, Legend of Zelda: Breath of the Wild and have this approach and link is up on the mountain looking down on the forest. What you're going to see this obvious tiling because none of the trees overlap across these tile boundaries. Or even if you were down in the forests and just happened to look down the line, you would notice, well, there's this perfect gap separating all the trees. But I only see that gap periodically and as walking through. So that wouldn't be a very good solution. You would want to have trees that partially overlap these boundaries. But the challenge is that you're loading and unloading these tiles. So how do you know where these trees are from, adjacent tiles when you're focused on generating content for the current tile. So one thing that I think would work is to keep track of these overlap regions. So you generate all your disks using the normal Poisson Disk Algorithm within your tile. Any disks that are entirely within the contracted boundary, those you have in one set. And then any that spillover past that contracted boundary, meaning the center point is beyond those extents. That means part of the tree overlaps into an adjacent cell. So you can figure out where each of these trees generated that go beyond, but still have their center point within the current cell, that would still be a requirement. But any that go beyond, we would put into one of eight different sets. And so this would be up, down, left and right. And then also top-left, top-right, bottom-left, bottom-right corners. And so we'll stick with trees, so you have all those trees identified. Keep them each saved in a separate set as candidates. So when a cell is loaded, only the trees that are entirely within the contracted boundary are immediately rendered. And then if an adjacent cell is loaded, then you go through a process of negotiating which trees in the overlap are used. Because both cells that are neighboring or both tiles that are neighboring will have candidate trees in the overlap region and some of them might overlap with one another. So you have to go through a negotiation, allowing each to decide, well, who gets to keep the trees in the overlap. So you have to be very consistent with this because we're depending on random number generation. We want to make sure that we always have the exact same approach every time. So we need some way to determine rank between tiles. And so you might do this simply by looking at, what are the X and Y positions of the cells? Maybe the center point or the bottom-left corner determines those coordinates, determine where each of the tiles are. And then we can just make a comparison. We might say that the cell closest to the origin wins and we first check X and then Y dimensions. So once you've determined the tile, that is the higher of the ranks between the two that we're comparing, we can then go through a process well that the highest ranking tile gets to pick one of its trees within the overlap region, and say, I'm selecting this tree. And then you see if it collides with any of the overlap candidate trees from the other cell and all of those get removed. And then the lower ranked tile, if it has any left, it gets to pick one and place it. And then you continue this process of pruning away any trees that are blocked from forming. So this should be a consistent process, and like our repeatable process for determination because you're always determining rank between the tiles in a consistent way. And each cell always uses their same seed to generate all the trees, both the ones that are within the contracted boundary, but also the trees that are held aside that are in the eight regions mentioned before. So you always have the same trees and then the negotiation deciding which trees actually get finally selected, that will be consistent as well. So that would be a way that you could hide the boundary. Now, there might still be some problems with tiling with Poisson Disk. I haven't fully explored this, but I think perhaps if you have a very large K value for like especially sparse layouts, there might be a spot where it's difficult to get a tree to place right in this the common area between adjacent cells. That so if there's a tendency for the algorithm to basically decide that it's exhausted all possibilities for the active disk and never placed in these common edge areas, then you might still see some visual effect there. So there's probably a bit of tuning that would be required in order to get this to work. And again, I'm pretty sure that others have thought about this issue of tiling with Poisson Disk and how to best address it. Back to the issue of the density of placement. So again, this it's very clear what zero means in terms of density. There's nothing there and it's easy to put nothing in terms of object placement. But as you approach one, well, what does that mean? And you could just decide, Well, it's arbitrary, it's just best effort. We'll say it's whatever our algorithm can do, maybe Poisson Disk, whatever the densest that it can place objects on average, that's what we'll call one. But if we really wanted to strive for the densest possible, this becomes a disk packing problem. So in terms of the study of algorithms, this is something that comes up it's, well, how can you determine what the best or most optimal disk packing is? And so this is a really hard problem. It's I believe, except in certain special cases, that it is not solved in for general cases, it's usually only solve for a small n, where n is the number of disks. And it gets a lot more complicated when you have a variable size in the disk or as n gets larger, even placing an n, like a uniform structure, like a square, that it can still be quite complicated. And perhaps like algorithmically intractable to solve. And also in cases where there is easily determinable optimal packing that tends to be highly uniform. So say, fitting circles into a square, they might be so uniformly packed in the optimal, like you would never want to use that for placing trees unless you wanted it to look like it was a tree farm, so where they're all in perfect rows and columns. We really want them to have more of a natural look, in most cases with their procedural content. >> What I'm getting at here is probably, whenever going to feel just come up with a straightforward algorithm that gives us the best solution. So very much in AI problem that we're getting at. But we might build a do better than Poisson disk in terms of density. And so there actually is another approach and this was documented in a GDC, I believe as GDC presentation for the game Horizon Zero Dawn. And so this game has some really impressive procedural content generation, especially related to object placement and you should definitely watch the presentation. I'm not sure how to pronounce his name. I believe it's Jaap van Muijden, but he goes through in his presentation how the entire system works and it's all in the GPU. The GPU handles figuring out where objects are placed based on some rather complicated hierarchical rules for placement. But one of the fundamental pieces of the placement is that they have a pre-computed disk packing tile. So it's a repeating tile and this tile is coupled with ordered dithering. Dithering is an image processing or has its origins in image processing, especially mapping color spaces down to a reduced color space like say for a newspaper print or early computers like CGA and EGA graphics when you wanted to represent lots of colors, but you only had just a small number of colors to work with. So the dithering worked well there. And so dithering makes sense when you have a high-resolution density map and you're mapping it to these discreet on or off, yes, there's a feature there or no there's not a future there sort of decisions. So by having this predefined circle or disk packing tile, you've already determined something that is approaching the maximum, most optimal packing possible. Then you couple that with the dither testing, so you're testing the probability from the density map. You're testing that against the thresholds that are assigned from the ordered dithering approach. And then that allows you to be able to handle these smooth gradients in changes in that define your object placement. So this, while it does have this random structure, it is a repeating tile. So I've got a picture of the tile, this is actually from the GDC presentation that was given for Horizon Zero Dawn. So this gives an example of their circle packing. And you'll notice if you look really carefully at the edges where the circles show up, you'll see, if one is jutting out over the boundary, well, there's a recessed area and just go from the top, fold over to the bottom and you'll see there's a perfect little spot for that disk to fit and go left or right and see the same thing. And you'll also notice that these disks, they have very tight packing, but it's in no way uniform, so there's lots of random little variations. And I believe this was accomplished through slight variations in radius and then perhaps also allowing some overlap and I bet there's some other little tricks they used as well to not allow any uniform structure to be present. But this gives us a tile and then each of the disk in the tile have assigned a dither value. Again, that is used for the purpose of mapping. Let's see. Oh sorry. This image here, is an example of the dithering. So the top right image there. So this is showing that underneath we have black representing a density of zero and then y, we can't really see it very well, but y is in the areas where we have all the disks placed. Now, this is a very uniform looking here just to make the example simple. And then we have progressively more spaced out disks as we transition to the gradient. So this is useful for the edge of a forest where you want to have trees that are very dense and then they taper off as you transition to a meadow, for instance. That again is used in Horizon Zero Dawn for their placement. I've been working on this off and on for a while trying to recreate their tool. Unfortunately, they don't give a lot of details in their talk about this particular aspect. They say that they implemented a circle or disk packing algorithm that they made it where it tiles. And they also describe briefly there dither values. So I will go through a little bit describing what I did. I'll have a separate video where I show a little interactive demonstration. First thing I did was to implement my version and I suspect the Gorilla Games folks may have done the same thing but I made a wraparound coordinate system, so it works like asteroids. If an entity moves off the left side of the screen, it gets mapped to the right side. So I have a bunch of spheres that are restricted to a plane and this basically becomes my disks. Since they're restricted to a plane they're effectively disks. And this was implemented in unity and the 3D physics is more capable than the 2D physics based on, what? Box 2D, I think is the name of the other library. So this was more robust and I can still get 2D constraints out of it so that's what I went with. I use contracted geometry to determine where these spheres are of the disks. Let me step back and I'll say that the disk are not all of uniform shape, I wanted to get some introduction of non-uniform structure. I think there's still some room for improvement there. It's much better than if they're all exactly the same radius. But I think there might be some other little tweaks to get it anywhere close to as good as the Horizon Zero Dawn tile that I showed before. So then I've got point gravity in the middle. And so all of the disks are being pulled to the center of the tile. And so they're all bumping against one another and being pushed apart by the others. If a sphere goes partially off beyond the contracted dimensions, it will begin to overlap with the other side. And so as soon as it's passed that contracted boundary, I spawn a copy or a doppelganger that begins to appear on the other side. And so you can see there are orange disks. The orange ones mean that they are associated with a red copy of itself, but approaching from the opposite side. As soon as I do that, I changed the center of gravity to be between the two and I actually, there is a joint that connects the two by an offset and there's only three degrees of freedom actually, I guess there's only two degrees of freedom because it's in 2D. There's no twisting allowed, so that way other spheres bumping against one or the other can't cause a torque and twist them. They can only push them up and down or left and right. And then they can possibly crossover from one side to the next, just like an asteroids, in which case if it fully crosses over, then it'll swap the orange and the red. And eventually if it somehow moves to the interior, it might switch back to green, meaning that it's no longer got a copy and then I just delete the copy. That actually ended up being easier than I thought it would be to get just the disk packing with the physics working. That was pretty easy to do. There were some issues like trying to get the full square to be filled. And so I played with just clicking and dropping extras. So I maybe start with a pool of some number of zeros, and then the corners wouldn't be full. So I'd fill them in by basically painting them in with the mouse. >> Actually, a better approach, I found was to just scale all of the spheres with the hotkeys. So I'd hit "Plus'' and make them all slightly bigger until they began to fill out. Or if there's too much fighting between them, I could shrink all the sizes. That might be better if you want to say exactly a power of two number of disks, which would probably be desirable rather than just adding more and more with the mouse clicking. So, yeah I think that works pretty well, some room for refinement. So next we get to the issue of creating the dithering values. And so if you look into how dithering works, specifically it's called ordered dithering. And the reason it's called ordered dithering is your dither values are in order. So let's say you have a range from 0-1 of values. So the number of dither values you will have will be the number of pixels or the number of disks you have in your dither map. So originally, this approach was used with pixels, with uniform structure. So that's depicted with the EMS, two-by-two or in the four-by-four and then eight by eight are shown. So that the two times two gives you four total ditheir values. And so these are shown in that first example. But they're very more interestings, when you get to the four by four, so you get 16. But you have to place all these dither values, so you divide up your range of values. So if your range of values, is actually normalized to a 0-1 floating point scale, you would just divide one by 16 and you would count by this fractions. But you want these fractions place in this repeating pattern as far from similar values as possible. So if you notice in this case, we don't have the 0-1 scale, we just have the 16 values shown. But if you look for the matrix with a four-by-four, you'll see it starts in the top left with zero. Then it places the next value as far as it can get away. Keeping in mind that this is a wrap around asteroid style coordinate system. So the furthest you can get in this case, is not going to be the bottom right corner, because that would actually be right next to the zero diagonally. So instead you see where the one is placed and then 2, and 3, and 4. And you'll notice this pattern and it's a similar pattern for the eight by eight. So if you look where the zero and the one and the two are, it just looks very much the same but just spread out. And that is because you want these dither values to be as far as possible apart. The reason that you're doing that is because each of these values, they're tests values, that is a threshold. And so you overlay your dither map on top of the image that you want to remap, like say colors. You overlay it and then you test each pixel. Where does actual, like high-definition color is you test it against each of these thresholds to see whether it's above or below. And then that determines whether you turn on or off the pixel. And it gives you the dither pattern that you're looking for. The problem that we have is this buyer pattern, which there's an algorithm for and you can pick an arbitrary size matrix and then easily generate the values. And it will have this optimal spread. Well, one thing is that it can give these unnatural patterns, and that's shown in the figure here, especially the gray scale here you get this weird hash patterns. You can see it also in the color. So it's not as natural to use, but we also have this issue. Well, how can we generate a similar pattern with our disk that they're all densely packed but they're not perfectly aligned like pixels are. So what can work, in that case is blue noise. And so with blue noise, it's very similar to the buyer matrix in that we're going to be placing each of the order ditheir values as far apart as we can. But we're going to do that just by brute-force evaluation of all of the disk will pick one randomly to assign the first value. And then we want to place the next disk as possible way. So we have to take into account this asteroid style overlap distance. So whatever is the closest distance to cross any of the edges or just directly across the middle interior of the square. So whichever is closest, going in any direction is what we're going to use as the distance. So we're going to take into account the contribution of every disk assigned so far to any unassigned disk. Well, what is the intensity? Like the contribution of intensity from all of those assigned disk to any given disc. And the one that is the darkest is the next one to receive assignment. So it ends up being a pretty straightforward algorithm. You again randomly you pick a disk, you assign the first in the order sequence increasing. So like if you have 64 disks in your tile, you would divide one because we have the range 0-1 divided by 64. So we have that first value. So we randomly pick a disk to assign that first little incremental value. Then we go through all of the disc and find out, which one is the darkest based on the contribution or that one disk so far? So all disk we're going to cash, we're going to keep track of the accumulating intensity being shined upon them. And the way we determine that is we use a Gaussian distance. So this has a smoothing effect. So their contribution of any given assigned disk to an unassigned disk will be based on the distance plugged into a Gaussian distribution. And that the variance will be a fraction of the whatever the furthest distance you could be apart is, so that'd be like going from one corner of a square to the middle, would be the furthest that you could ever have two disks from one another, taking into account the overlap. And so some fraction of that will be the ideal variance for the Gaussian filter. And so we use that to re-map each of the distances. So that would mean that closer neighbors have more contribution of intensity than further neighbors. And you keep iterating through and you just keep finding out, of all of the disks which one is the darkest? And then once you've got a new darkest one, you add its newly assigned intensity, which happens to be it's ditheir value. You just keep it on through and then you end up with all of them and assigned. This picture shown here shows in my code after this has been applied and also the tile is no longer present by itself, it's actually repeated several times, so you can't even see where the tile is. And all of the dither values are mapped just from black being a value of zero, up through white is a value of one. So there's a grayscale range. So it's really hard to see any structure here. So this blue noise, it has minimal low-frequency, so you don't see any distinct large-scale features. And there's no distinct high frequency bands either. So it's really hard to see anything other than some randomness with some small-scale high-frequency structure. >> So that's gotten to the point where we can perform our dithering. And so this is an example taking that same pattern that we just saw, that all of the disks have been turned white, just so that there's not any optical illusions. But there is a gradient here where to the far left and far right is a very high density, And when we perform the dither, we basically, for each disk, look at the center of the disk as it overlaps with the density map, so whatever the value of the density map is. In this case, it's just evaluating a function that I wrote or the distance from the center line. But that gives me the density. And then I test that against whatever the dither threshold is. And if I go back to this slide, the dither threshold is the value between zero and one, that corresponds with the grayscale value you see here. So black could be very close to zero, white could be very close to one. So whatever the density map value is, if it's very close to one, it would very likely be above a low value dither threshold test. And so that will toggle the disk to be turned on but if it's below, I turn it off. So that explains why you see very few disks in the middle, and it progressively increases in the likelihood of seeing a disk as you go out to either the left or the right. So the way in which you would use this is you determine the size of your features in the environment. Say you have a tree with a radius of maybe 10 meters or five meters, something like that. So you would scale your dither map repeating tile. You would scale it such that the disks that make it up have approximately the same size as your object. You scale it up so each disk is tree size. So that's going to make your tile take up a lot of space of the terrain. Then you separately have a density map. And again, this might be defined by something like Perlin noise, and it might be defined by something that a level designer has painted themselves. And so when you have the overlap of the two images or the dither map with the disks and the density map, you sample probably with nearest neighbor, just whatever the closest density value is. You test it against the dither threshold. And that determines whether the tree gets placed or not. So if your density map has lots of close to the value of one, then you're going to have super tightly packed trees. And they're super tightly packed because we use the physics-based disk packing to get them all very close together. And once you come up with all of these positions for placing your objects, you might want to apply some small perturbation, like just a little wiggle with random values. And that's acceptable with trees and other foliage because you're probably not going to notice if the limbs of the tree are interpenetrating other trees. And it'll just probably look like the lens are slightly tangled. And most people wouldn't notice, but just the little bits of movements will help reduce the game player ever recognizing that there is this repeating structure and applying the tile over and over. If you want to do the same thing with small structures like little tiny bushes or grass, you would just scale the texture, the dither map appropriately. It's not really a texture because you have the non-uniform disks in it, so it's more of a coordinate system and then you're mapping this hierarchy of all of these disks to that. But once you scale appropriately, again, you're just applying it to your density map, and it will tell you where to place the features. So be sure and check out the Guerrilla Games Horizon Zero Dawn presentation on procedural content generation, and they'll go through their implementation. There's a lot of special cases and other details that they share. Actually, one more thing I want to talk about is, still something that Guerrilla Games worked on. So this is when you have mixed object placement. And so this might be a case where you want to place a mix of objects, say like different types of bushes, or grasses, and things like that. So if you have these multiple features when you don't want them to overlap. So you could maybe use separate density maps, certainly would make sense. You want to maybe have different levels of density for each type of thing. You might also want to have some hierarchical rules to generate dependent maps. So you might be able to say multiply two density maps together, you're using Boolean type logic so that maybe you could restrict certain features or favor, say, plants that need a lot of water would be near riverbanks, and then have other plants in meadows. Not put a tree where a boulder is and that thing. However, we might not want the overhead of maintaining these separate density and dither maps for every single feature when there might be just minor variance in, say, the type of flora that we are placing. So we perhaps need a different solution. And so we could combine the thresholds for these different objects in terms of the dithering. So the thresholds have now a minimum and a maximum. And the object selection is determined by the density value in relation to these threshold ranges. So let's see. This is what it looks like in the example from the Horizon Zero Dawn presentation, is that they now have a combined dither map where for each sample point, which is, again, set by where these disks are in our tiled packed disk setup. But rather than have a single data value test, we have these ranges of thresholds. And so that way, you can only ever trigger one. If they didn't use this approach, one challenge that they face is for, I guess, GPU performance reasons and memory utilization. I believe I said in the presentation, they only use a single pack disk tile. They don't have lots of them. They generated exactly one, and reuse it for every single type of content, no matter the size or scale that is applied. And so if they overlap things that happened to have the exact same disk radius, they would have the issue that if they were separately testing the threshold these objects would try to place things in exactly the same place. And so that is one of the key reasons why in Horizon Zero Dawn, they use this approach with a layered dithering. I think that covers everything in terms of our look at object placement. It is a pretty important problem and something that could take a lot of effort to get it just right, especially in terms of fine tuning and special rules. If you're trying to get lots of different assets to work well together, so like this mixing of object placement, you can quickly get to where you can place things, but then the fine tuning and getting everything to look just right, that I believe there'll be a lot of effort in the fine tuning to get everything working.
