>> So next step we're going to look at Steering Behaviors from the context of animated characters. So for instance, characters you might find in a 3D video game that walk around, run, and jump, and so on. So first of all, I want to give a brief introduction to how animation works in a lot of games. I'm not going into the same level of detail that I go into in the video game design class, but just try to give you an idea of what's involved. Animations that you having games that involve say, a walking or running and jumping character. We're going to have movements that you might think of as like a puppet. So the legs bend at the knees and rotate at the hips, the arms swing and so on. And we might have different animations in a loop form for walking, and running and jumping. And those animations that involve the limb movement, well they can also include translation and rotation in the environment. And this is generally referred to as Root Motion. So this is translations and rotations that are embedded in the model itself. So these are often defined with skeletal animation, which is a topic that you can easily find information on online if you're interested. But the root of the skeleton is usually the hips of the skeleton. And so that is where all this translation and rotation information is stored. This is very common in your typical 3D game, and it is largely used to drive the movement of agents or NPCs in games. And furthermore, all these different animations like the walking and running, they can be blended together. We have these different, like separate loops of animations. They get blended together which affects the different poses of the limbs, like the bending of the knees and rotation of the hips and arms and all that. And they get blended, like the walking can be blended with running and make a jogging animation and variations in between. So you get nearly infinite different combinations of these movements. And that can drive the animation in the character is like the performance envelope topic. We'll look at a bit more later in related to Steering Behaviors. But it defines what the character is capable of just by the set of animations and how they are all blended together. Now, so far what we've seen with Steering Behaviors is that we are defining the movement of the agent programmatically. We've written code to update the position and the rotation. And then we're updating the velocity according to these steering forces that we're calculating. But with an animated character, we don't have that direct control, especially one with this root motion and blended animation. All of those values are outputs from this animation system that's running. How can we make that work with root motion? So this is a real challenge. What you would probably want to do is you can still calculate your Steering Behavior targets, like where you want the agent to move to, rotate towards and so on. But how do you tell this animation system to do what it needs to do? To [NOISE] seek a particular target or a line orientation. And generally the way these animations systems work, like say in Unity, is you pass to the animation system. This already been initialized with all the different animations is going to be using and blending together, you pass to it some animation parameters. And these are often normalized to a scale of negative 1 to 1. And you might have different dimensions for say, rotating left and right, or translating forward and backwards, or maybe moving left and right rather than rotating. It depends on the character. What you really want to know is, what is the result? What is the output of the animation system if you pass it these parameters? So you can tell it will turn a little bit to the left or a little bit to the right. So one thing you could do is just fit a function to what the animation system does. And you just do this empirically. This sometimes works, I've done it before. In fact, I'll show a demonstration in unity in a few slides here, where I've done that. But when you have complicated possible animation blends, this can be challenging. So another strategy is you can create a lookup table. Basically, you can say," If I want the agent from its current position end up at this spot in the next frame, then what would be the animation parameters that could do so if it is possible?" So you can actually create this table through some analysis of animations. Probably this could be automated, so you'd say that, well, if you blending this animation in this animation, if that's what the animation system is doing, blending some set of animations. And you say, well, if you're blending these with these weights, you end up at a certain location. So you can work backwards from that. So what would be the animation system input parameters that lead to that new pose, whether it's related to translation or rotation or both? So that is a possibility. I'm going to just keep it high level discussion now because this can get complicated. But you can make this lookup table. >> Now, this can be further complicated because a lot of times, the different blending possibilities, you could tell the animation system to immediately go to any of these. And that would be instant acceleration to these different velocities. And often that's not what you'll want. So the input parameters will perhaps be filtered. And so the filtering is basically an acceleration in practice because you can't move, you can think of is like these animation system input parameters is like joystick, offsets, so like pressing forward or backwards, left and right would be probably like a range of values. Say like press all the way to the left would be negative 1, all the way to the right would be one. All the way forward is one, all the way back is a one. And so that's like an x and a y on these normalized scales. And then 0,0 would be the stick at top dead center. So those are the parameters that you're trying to recover through this lookup table. But if you quickly move the stick on a joystick from one angle to another, those might be filtered such that it takes some amount of time before the parameters settle on the new location. So that'd be like an acceleration. So that's something else that you have to take into account, is that there is a maximum acceleration for which you can change these input parameters. And you'd have to take this into account as well as you're looking up positions in this lookup table. But if you do this, you can come up with something that works fairly similar to what we've done so far when we've had direct programmatic control of the agent movement. Is just we have this added complexity of something that's almost like a black box and not quite, but the animation system just adds a lot of complexity, and we need a way to account for how that defines the movement. In terms of like, if we're trying to align or accelerate to a position, we want to basically have the challenge of getting the right parameters to the animation system to go where we want the agent to go. And we can improve on this table approach by interpolating between positions like table cells. If we have a position that's not a perfect match in terms of like a movement, let's say, then we can interpolate between the cells in that neighborhood. So maybe this is cell, we're in-between cells in the vertical and the horizontal so that we might take the average of four cells. And so that's just like a linear interpolation. And that can improve the quality of the result, which again would be the parameters that we want to pass to the input system to get the desired output behavior. So next step I want to switch over to a little demonstration of the simple approach of just fitting a function. And so the fitting of the function for this animation system is just based on orientation. So these are robot agents that they will orient in the direction that you need to go. So they're just following from one waypoint to the next. And so that they need to be able to turn accurately and face the correct direction. They're just always going full speed forward. Actually I take that back. I think they will turn in place if they're not close enough to facing their targets. If so they have to double back where they're going, they will just turn in place. But that's just using a simple dot product test. Let me switch this back to full screen here. And I'll hit "Play". Now as you can see this robot agents, so the camera focuses on just one of them. We also showed the path that the agent is using. And then this is using the Unity NavMesh. There's lots of other agents that I have running around. So you see there's some that are similar to the agent that the camera's focused on. They just happen to be running a different route. And then there's a few special ones that just to make things challenging. They look like they're wearing a giant robot dress. So those agents, they're big and they're get in the way. And so demonstrating some of the efforts to avoid obstacles. And honestly, it doesn't work all that well. This implementation in Unity, it could have been improved. This is running in an older version of Unity. So the NavMesh framework and the NavMeshAgent could have improved. But the way that this works is, is using NavMeshAgent which has its own vehicle performance like the turning rate, and acceleration, and so on. You can't see it. It's attempting to move each frame, but it's like a hidden object. And then the animated characters, which are all controlled by root motion. So the animation system completely drives, orientation, and rotation of the agents as well as the translation moving through the environment. So the animation system is fully controlling that and what's happening behind the scenes is there's a simple algorithm that's telling the animation system to try to go to where the agent goes, the NavMeshAgent. And then each frame the NavMeshAgent gets reset. So if the 3D animated character is not able to perfectly follow the hidden NavMeshAgent, doesn't matter because hidden NavMeshAgent gets reset. And that by the way, is actually the recommended strategy that Unity suggests in the documentation is if you have a root motion character, to have it follow in that fashion. There may be new tools provided by Unity. They actually have a new updated NavMesh toolkit. I just haven't lately check that out. This I made several years ago. But one of the challenges here is when you send the animation parameters to turn say, left or right. Well, you can overshoot. If you pass the wrong parameter, you'll end up turning too far to the left or too far to the right. And that means that if you do that, you won't be heading on the correct heading. In fact, your agent will look like they're drunk or something, swerving back and forth, never heading straight in the path. They'll usually eventually get there, but they're just swerving back and forth because they can't dial in the turn rate. And so I just wrote a little function that had tunable parameters that allowed me to scale their turning rate. And I just played around with it until I could get it to work. And then these agents, in terms of deciding where to go and you might notice they occasionally will stop in place for a few seconds. That's all driven by a simple state machine. So there's actually a state machine that is driving the move from one waypoint to the next and exactly what they do when they get there. But again, all of these agents, they're moving based on steering behaviors. But the actual translations and rotations of the agent in the environment, it's all driven by the animation system. So the challenge is figuring out what are the correct parameters to pass. So I'm going to stop there, and in the next video we will summarize steering behaviors.
