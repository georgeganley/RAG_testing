>> Next step, we have depth-first search as part of our look at path planning algorithms. So like the breath-first search, students are probably also familiar with depth-first search, especially as applied to trees. So unlike breadth first search, where you're expanding all of one level of descendants and then the next, instead you pick just one descendant and then check to see if it's the goal. If not, then pick a descendant at the next level. So you're just picking one branch of the tree to go down and you go, keep looking for the goal. If you reach a leaf and haven't found the goal, then you backtrack. You go up a descendant check if there's any additional edges or descendants to check. And just keep trying to go to the bottom of the tree and then backing up to the point where you can try and alternate branch, and just continue this until a goal is found. So it's very similar to search a graph in the depth-first search manner, easily implemented. We do need to address the possibility of the graph containing cycles, because that means that as you're expanding nodes and edges, you might go right back to where you were before. So similar to the breath-first search, we need that bookkeeping metadata so that we know where we've already been. Instead of a queue though, like we had with breadth first search, we're going to use a stack that's last in, first out or whatever the node most recently added to the stack will be the first one out. So it's a to push pop interaction rather than the enqueue dequeue. One thing I wanted to say about the implementation that I'll demonstrate and also the code that is part of the game AI assignment framework, is that this is not following the convention for a depth-first search in that it doesn't free memory for nodes in dead end paths. They stay in the working memory of the search. And honestly, I'm not exactly sure how to properly free the memory. We'll see some issues that I've encountered. So one is that, if you remove nodes, if you're using the same approach as breath-first search to rebuild your path with the from node in the metadata and then being able to work your way back, you might accidentally remove that. So in terms of keeping track of what the current path is, I think you would need an alternative way of doing so, perhaps building the path in a forward direction. But then if you ever have to backtrack, well, how do you trim your current path if you're not using the from node? I imagine that is possible, I just haven't come up with it. Part of that is the reason why is that, it's fairly easy to make something that works in a depth-first search manner minus the memory savings just with a small modification to the breadth first search. And that was really all the effort that I was willing to put to my implementation. There's also referred to, you may have RD read, the Russell and Norvig Artificial Intelligence : A Modern Approach. So I referred to that particular book regarding depth-first search. And they mentioned, of course, the memory savings, but they don't share their implementation. So I've had trouble trying to figure out exactly how that would work, but there do seem to be some implications that perhaps I am just not thinking about the problem the right way. So in any case, my version doesn't have the memory savings, but the graph depth-first search is known for having better memory performance. Then breath-first search, just mine does not. And certainly many other AI problems, depth-first search is valuable for that purpose. And is easy to see, of course in the tree search version. But I'm not sure how to implement for the graph version. If you're not removing the nodes that you've already, as you're expanding in the map, similar to the breath-first search that metadata as it expands to your open and set in your close set, you might imagine that potentially the path that spreads across the map, once parts of it become close, that might cut off from finding the solution. But as long as you're not limited in the depth that you can search, like how deep in terms of a node count, as long as you don't have a limit like that, you will always be able to find a path if there is one, because there will always be some. You can always go back to previous node and try a branch off of it that would lead to the goal. However, if you do have a depth limit, there is a scenario in which you can cut yourself off. So potentially, there could be a valid solution within the depth that is a sign, but your algorithm won't reach it because the metadata that the open and close set, the frontier is blocking you off, it creating a firewall where the solution is not possible. And I've got a little visualization where I can show that to you in a bit. But as for the algorithm, you can take the algorithm that same as used for the breadth first search and just change it slightly by switching to a stack instead of a queue. So you got the same process of priming your open stack and set. And then you're going to just keep iterating until the open stack is empty. So rather than dequeuing, you're going to pop the current node from the open stack. You're going to look at all its edges. And of course, if any edge is leading to the goal node, then you're going to break and do the parsing and reversal to return it. But otherwise, for each edge you're going to push to the open stack, the edge nodes that are not in the close set and not in the open set. Now this is a point where there's an another little variation from breath-first search. In that if you do track the open and closed set, in this way, as your path is potentially meandering around the map, it will be excluded from going over nodes previously marked as being in the open set. But that means that you have the spacing. Especially if you look at a grid Lattice map, you're going to have the spacing imposed. By that, those open nodes that are a fringe around whatever the path that's forming. So you can't have super close folding back on itself. There's always going to be this offset. So we could allow that to be rewritten. Now there's a downside to doing that in that, what was our stack structure that only had the push and pop, we now need to be able to remove from the middle, because the frontier edge or the open set, you're modifying it. So if you're modifying it, you need to go and move nodes that were somewhere in the middle of the stack. You need to bump them up to the top of the stack to do so. But if you do allow for that, if you change data structures to be able to do so, you can then have the tighter path. Now if all you're concerned about with depth-first search is to find the goal, then that would not be of any concern. But if you're using depth-first search for other purposes, like intentionally creating a crazy path, then this might be something you would actually need to address in software. >> This little diagram, I'm showing the scenario where depth-first search can run into a problem when you have a depth limit cut off. If you've already committed to let's say a meandering path that's really tightly packed and run out of depth, so like you're counting up to the depth limit, you've hit the limit and then you start working backwards through the open set of available nodes. Provided that you're not returning any nodes in the closed set like back to their original state as part of this unwinding. If you're not doing that then that creates a wall. You can look at this picture here. I've got a starting point which is the green circle, a goal, the red circle, and a couple of obstacles, these walls that are in black and then I've got a cut off limit of 40. You can see the path is the solid black line. So initially I count out a tightly packed back and forth, so maybe just bad luck, this path ends up such that it doesn't reach the goal its first attempt going down the depth of its search, runs out of length so at that point, it starts to have to go back and consider other open nodes that might be available. At that point those open nodes that their node count is such that they've already accrued all of the meandering that led up to that point. So this is depicted in red. So I've picked the best of the nodes that would be open. So it actually should start I guess the cell right to the right of the X, that would be an open cell. And if you count down from there and go the most optimal it could possibly go towards the goal, you can see clearly that the total count, it is going to still hit the 40 limit and not have reached the goal. But if right from the start we counted over to the left from green, exactly as far as we need to go in Manhattan distance which I'd say 1, 2, 3, 4, 5, 6 over. If we'd initially gone six over and then down, I think it's eight and then write six, then clearly we would have reached our goal well within the 40 cut off limit. If you did want to limit your search in the context of static map you would probably be better off with not imposing a cut off limit for the path length but rather taking a subset of the map. So like if you had a grid lattice you could just take a subset of the grid and that would limit the maximum search depth but still allow the goal to be found if it is present. Next step, let's consider the possibility that we've removed from, say we exhaust possibilities for a particular path or part of it. So you might imagine that we could just return back to the pool of nodes that would become available again. What you would end up with is potentially revisiting that node over and over again in certain scenarios. And so this is, I'm trying to demonstrate how this could play out. Let's say that we've got similar to the previous slide. We've got a start position, the green circle, the goal in red, and let's say by bad luck we chose to go to the left when the goal was clearly to the right. We have this long meandering path again and it fills up the entire grid and we've run out of room. So if that all just stayed in like the metadata is just preserved, the only open node would be to the right of green and then we would take it and then find the goal and we would be fairly efficient. But let's say that instead we begin peeling back the tail end of the path, when it's exhausted we move back. Eventually when we clear the bottom row we would have alternate path directions. Now, this would at least in this case would require that instead of relying on the open set we would need to have some sort of metadata at each node that tracks what edges haven't been tried yet. So if we had that approach instead, we could do it. You backtrack, keeping tabs on every node that knows whether it's tried all of its available edges or not and then maybe right at first, just backing up that last row and is not too many possibilities. But let's say you get all the way back to the node, to the left of the green. And so I guess initially we tried going down but then maybe the next time we would try just going to the left. And think about all the different ways that you could draw a line through the grid until the end has nowhere to go. You could just quickly go to one of the corners and gets darker, you get painted into a corner. Some might touch every single of the available grid cells, some won't. You just think about all these different possibilities. I think would clearly be a combinatorial explosion. This is related to some problems like the self avoiding walk which involves moving the chess piece of the rook. How many different ways can it draw out a path on a chessboard without crossing where it's walked before? As the grid size gets bigger it explodes exponentially. So clearly if we're going to be implementing a depth-first search we don't want to be creating a scenario where we're trying all of these different paths over and over and there's going to be a lot of redundancy too at least on subsets of the path because if you backup enough you're going to start recreating the same structure over and over. I think you really would want to mark nodes as once you're done with them you don't want to ever visit them again for fear of creating these scenarios. And so that's one of the issues I have with the potential memory savings in a graph that I don't fully understand how you could free up for the graph scenario. Let's see. I guess I will mention one thing you could potentially remove from the available pool so that you get rid of a node but you don't want to use it again. You need maybe an additional way to mark something off. You still need some way to say that some node is closed. You could delete just the entirety of the graph node but then the graph would be gone. You could have separately your node records, I think potentially you could delete a node record but you would still need to track close set membership. So at minimum that's going to be a pointer per node. You can see the issue that we face if we're going to suggest there's possibility for memory savings. Here is a visualization of depth-first search play in the software. So you can see the leading edge of the path being formed. It does not spread out in that breadth way. By luck we didn't search the entire available space but this path is clearly not optimal. It's got a lot of zigzagging. So not really practical if you want an optimal path but it maybe has some interesting structure. So you might be wondering, well, maybe that could be useful in some way and maybe we want our agent to wander while still having an ultimate destination. So we want to ensure that our agent gets to someplace but we want them to intentionally demonstrate some silliness. I might be dating myself a bit, but perhaps you've seen the family circus cartoon. I'm not even sure if those are still around, the little kid. Whenever he's given a task to do efficiently, like take a mail to the mailbox, he goes meandering all over the place. Maybe you have some game play where you do want that maybe security guard going to a location and you want to just visit a lot of the area, you could also maybe be generating a maze and in fact, depth-first search is often used for generating mazes for procedural content generation. This could be something that is useful. >> So if we intentionally do want silly paths, we might want to make them a little more random than just having that organized structure of always trying up direction first, then right, down, left, and so on. So in this case, this is the same algorithm but modified with a little boolean toggle to tell it to shuffle the edges. So as edges are being expanded, they're actually all placed in a list, and then randomly edges are selected from that list, and then the resulting order that they were pulled out of the list is like a card shuffle. So each point at each depth or each iteration of the algorithm that is applied. In this case, the path is avoiding nodes that are in the open set. So it forces the generated path. Even though it's random, there's this wavefront of whatever the prior node was, creates with this four-way connectivity. It creates these two open set nodes that are right in the way that are blocking. So the only place that the path can form is away from there. It can eventually turn, but those open nodes are pushing along generally in a direction at least for a while. But if we allow open set editing, and again, this requires the modified stack so that we can pull things out of the middle. My implementation, I'm actually using a priority queue just because I had already used it for other algorithms. So that made it easy for me to just edit the priority to change the order. So in this case with the editing of the open set, you can see there's no longer that wavefront pushing the path along, it can fold back on itself with equal likelihood of going away from where it was before. And just statistically that results in a tighter path that's formed. And we can even play around a bit with this, the fact that we have this emergent waveform and maybe at a chance of open set revision. So only 50% of the time can the path fold back on itself. And here you see a blend of the two. And of course, you can play with that threshold in the code. It's hardcoded in my implementation rather than exposed, is like a public setting, but you can go in and play with it if you are interested. And we can also check out gateway connectivity grids. So that is a setting you can easily change in the code. But one thing interesting is a little self-intersection so the path will fold back on itself with eight-way connectivity since you can pass through the diagonals, that is where it generates this possibility of self-intersection. So if you did want to have eight-way connectivity and avoid that possibility, you'd have to mark it off. And of course, if you are using other types of grids, maybe waypoints, then you might have to do some more complicated geometric test as you're performing the search to avoid that. Again, if that's something that you need in your use of depth-first search. In terms of performance, it's complete. We will always have a solution if there is one on a finite map. There is, however, the issue of if you impose a maximum search depth, then you might have the problem of being cut off by what you've committed to before. In terms of optimality, of course, it's not optimal. But if you're going to be using depth-first search for path planning anyway, that's probably not your intent. In terms of time, it is O of m and that being the max new depth for a finite map, all your notes and edges will be the m value. And for space it is the branching factor times the maximum node depth will be the space. And that is if you're able to clean up the nodes that are no longer needed. Again, I don't have in my implementation. So the code that's provided, that implementation is the same as breadth-first search in terms of its space utilization. So some other ideas for the random depth-first search, which I think is really the main direction you would go for path planning in video games in practical use that you could imagine outside of just an academic look at depth-first search for comparison performance. I think the only practical use is for a really the random depth-first search. And if you're doing that, you might want to modify it to get different emergent characteristics. And one might be that you add bias for certain directions, like maybe more likely to go vertical more than horizontal for instance. And that would be pretty easy to do, just some random number generation. You might also pick a direction to go and you just keep going that direction until you hit an obstacle and then you pick another random direction once you've run out of room. That'd be easy to do on a grid lattice. Let's say you could try to make your paths even tighter together by not only allowing the open set editing, this require analyzing all the edges before committing. But you might play with the order that they are added to your stack. So you've got through at a particular node and a four-way connected grid lattice. So you've got these three new directions that you're generating, say like left, right and up. But let's say to the left, you happen to have an open set node where you could say that, well, that one's much more likely. So you'd have this distribution that you're generating and you make a decision, well, we're going to go there more likely than selecting any of the other nodes. And so that would tend to make the pads you're generating, be really sticky pull them, connected to one another. You probably don't want to do this all the time though, because you'll end up with something that looks very structured. There'll be a few random elements to it, like the elbows where the paths bend. But it might end up looking overly structured and so you probably want to play with that likelihood. So depth-first search is in many respects, a silly thing to consider for path planning. But in the context of video games, there may be some special applications where you can get some useful results like generating a wandering path that does actually end up going somewhere. What would be a good example? Generating mazes, and we will probably revisit when we talk about procedural content generation. We've looked at a few different ways that we can control the nature of randomly generated depth-first search.
