>> Hello. In this video, we're going to continue looking at procedural content generation. We're now going to consider the use of search based methods. So this is the idea of having a large space of possibilities, but instead of specifying exactly how we want to generate results from one of those possibilities, such as with rules based approaches or with generative grammars, instead we will focus on some means of evaluating parameters within a space. And just use a search to explore the space and evaluate what we end up at. And that is going to drive our procedural content generation. So what we are going to be able to achieve is we're not going to spend a lot of time authoring because we're going to avoid creating rules or grammars. Instead, all we'll need to do is define heuristics and fitness functions. This will allow us to score the quality of a move like as we explore the search space, we might have a heuristic to favor one choice over another, or perhaps the end result we can evaluate with a fitness function. This will give us a score of the quality of the result. So we don't need to understand as much about exactly how we will get there, we're just concerned about the end result and whether it is good or not. So if you have a means to perform that evaluation, and that means you have a means of comparing different possibilities within a very large space. So there are different approaches that can be taken, including heuristic search, hill climbing, simulated annealing, and genetic algorithms. So we might use depth first search in some situations for procedural content generation. This is probably most popular to use for generating mazes or similar types of structures, perhaps something like a city with different roads that connect in different ways. So you can use a depth first search to explore a discretized space such as a grid. And as the depth first search is applied to the space in different options that are considered, your algorithm can place walls and just continue on until some goal is reached if there is a goal, it might just be that you want to fill the space. So you can generate a path until you run out of room. Of course, you can build all those walls. If there are any unexplored paths, you can just branch from nodes that were never explored initially and just expand so you have all these different branches and sub branches that go off and completely cover the space. And so that gives you a nice maze like environment. You might apply certain types of constraints that might have some impact on the shape of the paths maybe that they don't get too close to where there's walls that have already been defined. So you can impose some separation through a heuristic. You might pre-populate the space with obstacles that will basically not be part of the frontier. And so the search algorithm is just not allowed to explore those areas. It could be reserved areas for gameplay purposes or some other reason. So there's a lot of different things you can do in terms of using depth first search. There's also some similar things you can do, getting away from depth first search specifically, but things you can do, such as figuring out connections between rooms, like by applying a minimum spanning tree algorithm to find a way to connect rooms. So this can be a useful way to generate content for your game. So if you are trying to generate procedural content just by using search to try to optimize some metric, well, the key here is you need that metric. So you have to be able to define fitness, which is the fitness of the solution is like the quality of the solution. And so that's really going to depend on the domain of what you're attempting to achieve. So you'll need to have a well-defined way to do so that might be by trying to analyze some aesthetic features in the content generated. It might have something to do with gameplay, like ensuring that there's a connected path for the player to get from point A to point B. So there's different things that you might need to do. Most search based optimization strategies use this concept of the energy of the system as being the metric. And so we need this way of calculating energy and change in energy related to where the search is going. So you have some discretized representation of the state-space or search space. And as this exploration occurs, the search algorithm is applied. The change in energy is positive if you're moving in a good direction, so it's a good move. And then it can be negative if it's a bad move. And so we need this way of computing and then being able to make these comparisons. So one simple approach is to use hill climbing again, provided that we have this means of measuring the fitness of the parameter space that we're exploring, knobs and dials in terms of how we can manipulate our content generation. We're specifically looking for the procedural content that maximizes the fitness. So if you can evaluate any different parameter space configuration for what generates your procedural content, if you can make that evaluation, then you can look in the neighborhood of any particular point. So that would be some combination of a continuous parameter space. You can evaluate a point and then you can see what direction is going by looking in the neighborhood, perhaps involving a derivative and that gives you a gradient. And then you can just follow this gradient. And depending on the nature of your parameter space, you might have a nice smooth gradient. And it's really easy to just follow the direction and whatever is the appropriate direction. If you're trying to maximize the fitness function, then from whatever initial point you evaluate, you just follow the gradient until you reach the peak. >> That is the ideal configuration that you should select. And a lot of problems unfortunately, don't have this nice monotonic gradient. A lot of cases you might have local minima, local maxima issues. And so this is where you might need to use some other strategy beyond just looking for a gradient to follow. So there are various approaches. I'm not going to get into a lot of detail, but it's covered in the intro AI class gets into a lot more detail. But you might use, for instance, some momentum based method. And so in this case, you have an analogy of a wheel or a ball that's rolling in the direction of the gradient and it can pick up speed. And if you reach a point that you're not sure if it's a local minima, local maxima situation, well, the ball will have picked up a lot of speed and therefore, it will search beyond the peak proportional to the speed that it's picked up. And of course, in terms of the algorithm implementation, you remember the best result you've seen. But you will go looking further beyond that point. And the idea is that you'll be carried past any small speed bumps, so to speak, in terms of local minima or local maxima. And so that can be helpful provided you have a higher level gradient that you can follow and you can skip over these little local disturbances. Another approach with hill climbing is that you can trial multiple initial points rather than start with just picking a single one and then looking for a gradient. You can instead try several. And hopefully, you can find, if not the best, you can find something that is close to the best. Similar to that is the approach of simulated annealing. So this is inspired by an annealing in metallurgy. So you can think of it as, say you have hot metal that you've formed, maybe from a liquid into a solid. And as the metal cools, connections will be formed between the molecules. But they might not necessarily be the optimum connections in terms of relieving stresses in the material. While it's still hot, connections will form and break and form and break. And so if the metal cools at the right rate, it will fall into a natural pattern of relieving stresses and molecules connecting in an optimal or close to optimal fashion. So inspired by that physical process. This can be implemented in an algorithm. So in terms of searching a parameter space, so you have this fitness function that you can evaluate, but your algorithm doesn't have this overview of a graph like what we see here in the visualization. All I can do is evaluate points and figure out from the points evaluated what the best at seen so far is. So the question is, well, what's the ideal points to explore? And so that exploration process can be driven by the annealing. So you're picking a neighbor based on some probability t, and the probability is based on how long the algorithm has been running. So the idea is that you start off very hot, and then you're cooling off. And so you're going to have a quality score for the neighbor, and you're going to be making decisions about whether or not to move in terms of the explored position according to this probability. So as it cools over time, you're going to be transitioning from exploration to exploitation. As it cools, you'll tend to stay in the same position or closer to the same position. When you're hot, when the algorithm is in the hot phase, it will be more likely to jump around in very large swings, large changes in the position that is being evaluated. So with simulated annealing just at a high level, the algorithm is going to work by started some randomly chosen initial state. Then you're going to work through some maximum number of steps. The temperature is going to be dependent on the number of steps. So again, it's going to start off hot and it's going to cool off over time. And we're going to select a new state or a random process looking at the neighbors of the state and whether or not we transition is going to be according to this probability. So when the temperature is high, the probability of switching states is going to be much more likely. So that's the exploration. And as it cools off, that's going to become less and less likely the further the randomly selected new state is from the current state. So that's going to mean that the most likely new states to evaluate are only going to be ones that are in the progressively smaller and smaller neighborhood. So we're going to go from quickly sampling like all over the parameter space down to progressively smaller and smaller. And so this will allow refinement. So the exploitation. So as it cools off, the only positions that are likely to evaluate as passing are going to only be in this ever shrinking neighborhood. So you'll be able to find the highest position in a local maxima situation, at least within that neighborhood. So this can work pretty well and also, as this process is continuing, you'll want to keep track of the very best sample that was ever seen. And then the final return will be the best that was ever seen. Just some case, through the random chance of jumping positions that you jump to a worst position to explore. So this can give good results. It's very dependent on the temperature function, and so that's going to be domain dependent. And you might even have to tune in to watch for whatever specific problem that you're working on. In the worst-case, if you don't have a good temperature function, is basically going to be a random search similar to the nonmonotonic hill climbing example we saw before, where you just have lots of just selecting lots of random points to explore. Another way that you can search parameter spaces is with genetic algorithms. So this is loosely inspired by the theory of evolution. So sometimes it's called evolutionary search. And it tends to be a popular approach just because I guess it sounds enticing to have AI that has evolving solutions. >> The basic way the algorithm works is that you have a population of configurations. So this is the exploring of the state space. And you check the quality of all of the solutions. And from that, you're determining which of the population to keep. And then of the population that you are keeping then you go through this natural life cycle approach. So there'll be some number of mutations in the population you can think of is like genetic mutations. And then there is a reproduction phase where some of the population will share their configuration. So this is something that has some characteristics of exploration and exploitation. So when you have mutations, this gives some probability of randomly replacing a member of the population with a neighbor. So you're placing some parameter within. Maybe you have a whole bunch of different parameters, and so one of them will be changed from whatever its current value is to another value in the state space. So this particular new value is probably not present in the population, at least for that one specific parameter. So you have this entirely new configuration for this particular member of the population. But you also have crossover. So this is where you have lots of configurations, lots of examples of the search space in terms of all of the parameters. So you have 10 different parameters, you might pick to swap a parameter with another in the population. So you end up with new configurations for both, but individually the parameters were present in the population before. And then, what happens is once you have all the mutations and crossovers, there's a reduction in the population. So the population can grow according to the mutations and crossovers. You can keep your previous examples then you evaluate all of them. And then you just pick the ones that have the best score and according to fitness. And you just keep repeating this process over and over. So again, the mutation is exploration. Because this is where you're picking up parameters that were likely not present in the population. And the crossover is exploitation. Among the population, you'll tend towards finding good isolated parameters. And then as many generations have this crossover performed, you'll find the right combination of those parameters. And so the idea is that you will get these increases. And if you don't have nice smooth gradient in terms of the fitness function over the many parameters, you still have a chance to jump past local minima, local maxima because of the mutation. So you might still jump from being stuck on one narrow range for a particular parameter or combination of parameters, you might eventually discover a good combination. So the pros of genetic algorithms is they reduce authorial burden. But more than with hill-climbing, but it's less than say, the generative grammars or the rule-based systems that we looked at. So also genetic algorithms tend to do a good job at finding a combination of parameters that gives close to the global optimum fitness. The downsides of genetic algorithms are that it takes a lot of experience to pick and to balance the mutation and crossover. The fitness function can be challenging to implement as well. And then lastly, there's this notion of genotype to phenotype mapping. So the genotype is the representation of the actual features in this parameter space form. And the phenotype is like, what's the actual content, at least in the context of procedural content generation. So the features might be, say, the placement of artifacts or some properties of the levels or terrain that are being generated. So how can you map those features to some parameter space that is defined in the genotype? And so that mapping is not always intuitive. So overall, with search-based procedural content generation, the positives for the search-based approaches that we looked at: It allows designers to specify high-level desires in terms of heuristics and fitness functions rather than low-level content authoring or rules authoring. Downsides; designing these evaluation functions can be very challenging. And you'll often find that many developers will prefer just to go ahead and use the generative grammars approach rather than trying to figure out the all the intricacies of developing these evaluation functions.
