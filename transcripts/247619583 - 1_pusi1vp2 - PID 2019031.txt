>> Next step, we will look at generating a NavMesh. There are a number of different ways to generate a NavMesh. We're first going to look at a naive greedy approach, but we'll also briefly discuss other possibilities that you're more likely to encounter in production implementation of a NavMesh. This can include libraries such as recast or NEOGEN, those being voxel-based approaches, and this helps especially with analyzing the environment to get to the point where you can create your polygons. So you actually start with the volume, then you move to the triangles. With this greedy algorithm that we're going to look at first, we're going to assume we're only worried about one level, so we don't need the voxel approach. We're just going straight to the polygons. The goal of the greedy implementation is we're trying to exhaustively create triangles through the scene, and these triangles have to be valid. So what we're going to be doing is iterating through all obstacle or boundary points of our world. We'll assume that the world is bounded by some polygon region. Probably this will just be like a rectangular boundary for simplicity, but we'll have all these different points for vertices in 2D. So we'll, at this part of the algorithm, we're going to ignore what points belong to what. We're going to add them all to a pool or a list, and we're going to select point a, and then we're going to iterate through again all points, making sure we don't pick a again but we're going to pick a point b that is not also a; and then we're going to iterate through and pick a point c that is neither a nor b. This defines a candidate triangle. It might not be a very good triangle, but that'll be our starting point. Now when we pick randomly of these points in the world, we are going to possibly get a triangle that is not counterclockwise or otherwise not valid, so we're going to screen our candidate triangle. We can check to see if it's degenerate in some way, so we could test the triangle for co-linearity or the overall area of the triangle being zero. If we encounter that we'll say, "Well, we don't want that triangle, so we'll skip over." Now, if it passed that test, then the other concern is if the triangle is counterclockwise, we want to, just for consistency for the various tests we're going to be doing. Many rely on this counterclockwise nature, so we also want to make sure that our triangle, the three points are listed counterclockwise. Otherwise, the ordering doesn't really matter in terms of if we have a list or an array in which it protects this first. That's not really important so long as they're counterclockwise. If the triangle fails, we're just going to skip it and we're going to iterate through the next possible pairing and try another one, and so on. And so if we find a valid triangle, then we might want to add that to a growing list of form triangles, but we can't just immediately do that. We want to make sure that this new candidate triangle that is counterclockwise and that is valid, all the points in a line with no area, so we want to test to see if it overlaps with any of the obstacles or if it overlaps with any of the triangles that have been formed so far in previous iterations. If neither of those two tests fail, then we can actually add the triangle and we'll continue this process exhaustively, trying all combinations of triangle vertices a, b, and c until we're complete. So that's, at a high level, what the algorithm is doing. So let's start looking more specifically at certain details and how we can actually implement this. So the first thing is rather than test the entire triangle, we're just going to consider each edge. Because if you think about it, if a triangle intersects, it might be caused largely by a particular selection of two of the three vertices being an edge, that would be the cause of why an intersection occurred. And there's going to be lots of triangles in that same family. You have a whole list of boundary points, so you pick a and b, and then c could be one of the whole list minus 2. If you've already identified that edge ab is the culprit, then that'll allow you to exclude a whole bunch of triangles all in one test. So we're going to switch to an edge-wise way of testing if triangles overlap, at least for part of the algorithm. Then further, there's a few little improvements we can make. So if the candidate triangle edge that we're considering, let's say ab, so we've picked a pairing of these obstacle vertices, and ab just happens to also be an edge in an obstacle. That'll be pretty common, that you would maybe pick an edge of an existing obstacles. So in that case, we will not bother testing against all the other obstacles because we already know it's part of an obstacle. This is assuming we don't have overlapping obstacles. So the assumption here is that we've already cleaned up or guaranteed that obstacles aren't overlapping. If we get past that test, if the edge is not part of a polygon edge, then we can test it by performing the checking if there's intersections with any of the obstacles in the scene, so we're going to iterate across all of the obstacles. And we also want to use short-circuiting. So as soon as there's ever a failure, we wanted to immediately quit, obviously not continue testing when it's pointless to do so. We can also improve our iteration process as described so far to iterate across the obstacle points three times. That suggests a loop nested three times. What we're basically doing is performing combinatorially N choose k, where k is fixed at 3, so N choose 3, so we can define our loops in a way that iterates through the N choose 3 possibilities. The first loop iterating through and stopping when it's three back, and then the next nested loop will start at index after i because we don't want to repeat ourselves and stop 2 back, and then the third loop will iterate through 1 after j, which is 1 after i. So this will allow us to generate N choose k triples without any repeats of the three, and so there's a little bit of performance gain there. Even if you do this optimized pairing or indexing into our list of boundary points, you're still going to have repeated edges. So another thing that you can do is you can cache the results by using something like a hashtable, maybe, say, a dictionary. You could look up to see if during a previous iteration you already stored that edge before so these repeats will occur not in the first to the second loop like the a and the b vertex. It's the fact that we have this third loop that you will restart generating b and c, and then also c back to a is another that will potentially have repeats as well because you wind your way counterclockwise around the triangle to form the three edges. So yeah, caching is something that optionally could be done to improve the performance. >> So next step we have to consider problems that can arise from just blindly performing this greedy algorithm. And so one possibility is what I refer to as adjacency blocking. And this is where you pick a triangle that can never be adjacent. If we're assuming that adjacency is only with a common edge with another polygon. So in this figure shown here, I've got a few different obstacles, of course the corners of the obstacles are boundary points. So we have this list of boundary points and let's just say by luck, you end up or your algorithm ends up selecting the triangle vertices A, D, and F. Well, in that case, this candidate triangle does not intersect with any obstacles. Provided that we're saying that triangle edges that just touch on the perimeter of our obstacle polygons, those are not classified as intersections. So only things that penetrate to the interior of our polygons is going to count. So if that's what we're enforcing in terms of our testing. This would be a valid triangle, ADF is valid. And you can see that maybe through other iterations, well, the triangle BEC would also be valid. And that's fine. We continue to form lots of triangles. Now, when it comes time later to use our triangles that we've created through this greedy algorithm, we want to look for adjacencies. Well, there's no shared edge between BEC and ADF. So what that would mean is since there's not a shared edge, there's not an adjacency and therefore there is not a path for the edge to move through. If you look at my mouse cursor, moving between these two squares, there's no way, at least in terms of path planning, for the path planning algorithm to be able to return a path that would go through the red dotted line area. So this wouldn't be an effective data structure for path planning. However, let's say we ended up with triangle ABF and then later formed say triangle BCF, CDF, and BEC. So all of those triangles the end result is past the tunnel that's made by the two squares passing between those, we'll see that there is an edge and it is common between BEF and BCF triangles. And so therefore we can build our graph structure for the path planning everything works out nicely. So we want to find a way to enforce this during the greedy building part of our algorithm. So we're going to have to add some extra test to this algorithm for it to work right. So this additional test is we're going to check that any candidate edge in our candidate triangle, we want to make sure that there are no obstacle vertices or points that lie on that edge. So we say we have edge AB and we have candidate point C. You would want to check that first that A, B, and C is not collinear. But if it is collinear, then we'll check to see if it lies between A and B. If C is between A and B, and collinear the three of them together, that is our between test. And so if that is true, that is between it's not a good edge to use because going back to the previous slide, you would create this exact scenario. So for instance, for candidate edge AD, point B is collinear in between A and D. And point C is collinear and between AD. So that is something that we can screen for and avoid. So we just skip over the edge and that'll resolve that problem. Another issue is related to floating point versus integers and the introduction to computational geometry. This is discussed as a concern where just the nature of floating point values and necessarily using Epsilon values because the floating point values aren't precise. There's a bit of error that's introduced. So you have to have this approximate equality that we leverage. The Epsilon value is a fudge factor. So you might set up this fudge factor for a particular test like between. But you have other tests you're doing too. So you're testing for degenerate triangles. You're testing for whether a polygon is counterclockwise. All of those tests have to agree flawlessly in the context of your algorithm. And that is really hard to guarantee with floating point values, you might tune the Epsilon to work in some test cases, but there's lots going on. You have different ranges of values that affect your exponent part of the floating point, which then affects the minimum different value of the floats and so on. Different tests that introduced different degrees of air, these geometry test that you're performing. And what can happen is that you get a disagreement, say you make a decision based on the use of the predicate between and then you degenerate triangle test. If that gets out of sorts, then you might, for instance, say that AC, this figure is exaggerated just so you can actually see some detail. Say ABC is really close to being collinear. So it's really close to having situation where B is between A and C. And you also have triangle ABC is really close to being degenerate and same with I don't have a vertex shown, we'll call it D. ADB is really close to being degenerate. Every single analysis of all of these has to agree perfectly. Or what happens is you don't end up with adjacent edges. And that means that you don't have a path in-between these three obstacles. If you use the integer mapping discussed in the first computational geometry lecture, this problem is dealt with. You create a much more stable situation for performing these tests, provided that the geometry tests are well-designed. Then you can deal with super skinny triangles as you encounter them. And speaking of long skinny triangles, that is a problem of the greedy algorithm. Even if you can deal with a find during this generation, at the greedy generation process, it might still have negative effects during gameplay. So if we were to say come up with a better algorithm, which honestly you would probably want to do for a production game and not just an academic exercise to learn about the process like the homework in the class. And you'd probably want an algorithm that makes triangles that are closer to equal edge length. And now I have this long skinny problem. And you also, in addition to maybe testing things like whether a point is inside a long skinny triangle during runtime because that'll have to be floating point, or generally will need to be just for the standard representation and processing that you'll find in a video game. You're also going to get poor path quality depending on where the point of the path is in the triangle, say the centroid of the triangle. You have these adjacent long skinny triangles. What you can end up with is a zigzagging back and forth. Because with a long skinny triangle, you are creating scenario where the centroids move a lot. So you end up with lots of zigzags. And you could address that like string pulling, but it'd be nice to start with a path that's already from the start fairly reasonable. And there's better algorithms, such as the ones based on Delaunay triangulation and many others that you can consider. Next issue to consider with this greedy algorithm as described so far is that it's very slow. I think it's big O of n to the power 4, where n is your number of obstacle vertices. So the n choose K is certainly a considerable impact on the performance of the three nested loops. Then you have all of these tests that need to be performed in the inner loop as well which involves analyzing all of the vertices. So that is probably the strongest evidence against using the greedy algorithm for creating a NavMesh. >> Another thing to consider is the testing against obstacles. It intercept the candidate edges that were forming. And we want to test whether this intersect with other obstacles. Well, we already mentioned that we don't want to have edges that just merely touch the perimeter of an obstacle polygon to count as an intersection. The whole point of the NavMesh is to maximize the coverage of the world. So we want to go right up to where the boundary conditions are. So we're going to be forming edges of our triangle that are right up against that. That should not count as an intersection. So it's challenging to deal with that. And the fact that we're picking vertices for our candidate triangles that are common to the obstacles themselves. So that creates a situation where we have edges that are going right through the worst possible place on the polygon, if it does in fact intersect. So it's going to go between two adjacent edges, say at one corner and then come out at another corner. And it's not necessarily that the two points are of the same obstacle. It can still happen just by luck from, say, another obstacle, and then it ends up being just perfectly aligned with two corners of an unrelated obstacle and it just cuts right through at that point. So this makes things tricky. So to exclude things that just touch the outside but still tests for the interior, that means you have to do really specialized test for intersection, especially if our obstacles are not convex. We might have concave obstacles. And we'll see that this is actually important later. So one thing is that I already mentioned that the obstacles can't intersect. And so there are geometry operations beyond the scope of this class that can perform, say, a union of multiple obstacles. And so that would allow you to go from, say, two overlapping obstacles down to one. There's other features that we might look into that would result in getting concave polygon shapes for our obstacles we'll touch on later. But basically, this test has to work with concave polygons unless we broke them up with another algorithm that would do a convex decomposition. So convex decomposition is not being done at least in this scenario described here. So we need a test that will work against concave polygons, at least that don't have holes or self intersections. And so it involves finding these candidate edges that do intersect with the obstacle at least at points. So we've identified it attaches a point. Now, in the figure here, we show one whereas clearly an intersection, the top one, and then the lower figure with a concave polygon, it's touching two vertices, but it's only touching the obstacle polygon itself. It's not intruding on the interior of that obstacle polygon. So we need a way to be able to test this. So we can identify the neighborhood of these intersections, say the edge where it intersects with point A. The neighborhood would be the edge to the right and to the left of that point if you're thinking in terms of a counterclockwise winding. So we can then test the opposite point, say point B that forms, it's a position vector. For a given directed edge, we can check to see if B is to the left of it or to the right of it. So for convex angles, we can check if the potentially intruding edge, if the opposite end point is to the left of both edges. If it is, then it clearly cuts into the interior. And then if it's concave, we can check, it turns out, you can check to see if it's not to the right of both edges. So if the edge that touches a concave vertex is to the right of both of the edges in that neighborhood, then it is leaving the polygon. Now, a concave polygon gone could be quite complicated and curl around itself in all ways. So it's just at that particular point it doesn't intrude, but it might intersect again later with the polygon. Of course, we'd have separate tests for the multiple candidate places that this could occur. So this just a high-level description of how the algorithm works. And you do need this special case tests. In the homework, this code is already provided to you, but it just demonstrates how the greedy algorithm approach. Well, initially it seems very simple. You start to discover more and more problems with the approach. So it's like he keeps sticking band-aids on it to make it work. And I think that's a pretty good description of it. Well, is there a better way? And there definitely are. It's like, I would say, beyond the scope of this class to be implementing this ourselves. So that there's lots of triangulation techniques out there. If we are to consider triangulation technique, a particular algorithm, we'd want one that can handle complex polygons with holes. And that's because if you think about what we're trying to represent, we have an overall world area, say a big rectangle. Then there's obstacles within it. And these obstacles carve-out holes of the boundaries of the world. So we might say initially the boundaries of the world define an interior region that is fully traversable by the agent. But then we put obstacles in and those curve away the traversable area and replace it with holes that are un-traversable. So that would be our starting point. This boundary with a bunch of holes for our obstacles or hallways and walls and buildings and so on. From that, you can perform a Delaunay triangulation. And so that's a common approach, but it needs to be what's called constrained or constrained edge Delaunay triangulation. And the constraint edges would be where the holes are. There are other algorithms out there as well that might be considered and have different features. Again, it's getting well beyond what we can fit into this class. But if we're going to the trouble to select something new, we probably also want something that's not going to give us a bunch of long and skinny triangles. So that is also a consideration. Delaunay does tend to give triangles that are closer to equilateral triangles. And the performance can be a lot better than what we've seen with greedy. Can be as good as big O n log n, where n is the number of the vertices. And also, there are libraries out there. One popular one is pol2tri. I've got a link listed here. This works well with clipper, which happens to be the libraries already used in the Game AI homework for the NavMesh. Clipper is used to perform the union of the obstacles. So we don't have to worry about overlapping obstacles and geometry expansion, which is a topic later in this discussion. Next step, I want to mention recast. So this is a library that is very popular for creating NavMeshes in video games. It involves creating a voxel rasterization of the world. So you've got all these voxels, and within each voxel, the geometry that can be quite complicated in a video game, do you think of. Triangle meshes and so on, that would affect the ability for an agent to move around, what surfaces to stand on. So this analysis can be performed in each of the voxels to determine whether the agent would even fit there. And this can happen at multiple layers. So you actually, the algorithm expands across all of the geometry provided from the game world. And then it makes determinations on just a little tiny cells of surface. Then it looks to neighboring cells to see if they can safely be stitched together. So you might have slopes or ramps and so on. And so the surfaces can be identified. And so the surfaces are then extracted. This creates layers of polygons which can be triangulated. We can create the NavMesh, but their layers, they might wrap around or fold on each other in a 2.5 dimension way. So you can think, for instance, having an agent that goes, say, on a catwalk or a bridge over another traversable area, or you can have platforms that the agent can jump between. So this works really well for game play. This is still a preprocessing. Typically, the process is performed. You bake the game level to generate the NavMesh. >> Basically, the voxels are a starting point that allow us to better identify navigable surfaces, especially with multiple levels that occur. You still have to perform the triangulation, but is just like an extra step before the triangulation part is performed. And so that ends up being critical for a lot of video games. After you have your triangles, there's still more to do. You need to determine the adjacency of all these triangles you formed. So certainly can go through triangles exhaustively, and then look at all the other triangles and see if you can find the adjacent edge. That is pretty expensive. You can speed this up quite a bit with a hash table where you hash your edges. So this is an interesting little software engineering problem here. How to pull this off with effective data structures? So you want for each triangle, store the edges and then you see if you can quickly look up the other triangles. But the occurrence of an edge in a given triangle, let's say it happens to be adjacent, say triangle abc has edge ab adjacent to another triangle. Well, it's not going to be ab because of the counterclockwise winding it's going to be bad for instance. And then how are you going to hash this as a data structure? So we need something that's order invariant doesn't matter if it's AB or BA and while the dot-product, based on the commutative property, it doesn't matter if you're if it's a or b dot a.b and we can take the dot product of these position vectors is just a vector from the origin. It doesn't need to be a relative position vector we can still perform the dot-product on it. So that is the base for the hashCode. And this allows us to store triangles hashed by a common edges, and then find these pairings, and so this is a way that we can quickly identify adjacencies for the purpose of forming our basically a path network but the graph structure that we need for our game. Well, it's not a path network until we had assigned a position, I guess to the nodes so yeah graph. Now another thing that can be performed optionally is triangle merging., Bbest because there may be larger polygons that could be part of our NavMesh, then who can have larger than a triangle? For instance, we might be able to put two triangles together. Go from a 2-3 sided polygons to 1-4 four-sided polygon. And it could still be convex which is our main requirement, and can still be adjacent to a mixture of other triangles and maybe other four-sided. We can keep expanding, we can keep merging, and emerging, and emerging exhaustively until like a greedy algorithm until we have figured out how deep we can go and that's not necessarily optimal. It's just, we've run out based on the initial decisions we make. We've run out of triangles, we can merge. But the key here is that we need to avoid merging polygons that ended up resulting in concave polygons and that's possible. You can imagine when the triangle, you can merge two and if there are long skinny triangles that are angled just the right way, it would not create a convex polygon. So we can leverage our same hash table of adjacency information we had before to iterate on the keys and figure out what pairings we want to attempt. We can do our candidate merge, which is a fairly straightforward operation just basically list operations. We have list of vertices and so we have to treat them circularly and figure out where the common edges and then stitch together the two list into a new vertex list to define the polygon. So if it is convex, of course, we continue with it, but now I need to update our hash table structure. And replace the two polygons with one, but also all of the other adjacencies that those original two polygons had need to all be updated to now have a new neighbor. So that is straightforward, but I would say error-prone. You have to be careful about properly updating the structure. So you just compete. You continue that process until we've got all of the triangles emerged. Next step you have all of this adjacency information possibly merge triangles that you have this more concise representation, of course, by the way, that helps with our search performance. We have fewer nodes and edges to worry about or a star search would work better. Next step we need to determine well where based on we know that each polygon is a node. But in terms of path representation, we actually need a position. We have to decide where's our position. We might think, "I'll just use the centroid and that will be where it is." But there's perhaps more to consider, but we need that position for defining an actual path. And then we also need the position because we need to support heuristic estimates. For A-star, we need like starting points as the algorithm advances to say, well what's the distance to the goal to steer where the A-star is going? We definitely need to make that determination of where are these nodes a specific point position for each? That is what we are going to look at next.
